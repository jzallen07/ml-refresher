{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: RAG Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0aacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple RAG (Retrieval-Augmented Generation) Demo\n",
    "================================================\n",
    "\n",
    "This demo covers:\n",
    "- Q36: What is RAG and why is it important for LLMs?\n",
    "- Q40: How do LLMs handle out-of-distribution data?\n",
    "\n",
    "RAG Pipeline: RETRIEVE \u2192 AUGMENT \u2192 GENERATE\n",
    "\n",
    "Interview Context:\n",
    "-----------------\n",
    "RAG solves key LLM limitations:\n",
    "1. Knowledge cutoff: Can access up-to-date information\n",
    "2. Hallucinations: Grounds responses in retrieved documents\n",
    "3. Verifiability: Can cite sources\n",
    "4. Domain knowledge: Can access specialized information\n",
    "\n",
    "This demo shows:\n",
    "1. Document chunking strategies (fixed-size vs sentence-based)\n",
    "2. Simple embedding creation (TF-IDF for demo purposes)\n",
    "3. Vector similarity search (cosine similarity)\n",
    "4. Query augmentation (adding retrieved context to prompt)\n",
    "5. OOD handling through retrieval\n",
    "6. Visualization of embedding space and retrieval scores\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c276e6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple, Dict\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db60259",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 1: KNOWLEDGE BASE\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec52c71",
   "metadata": {},
   "source": [
    "Create a small knowledge base about Machine Learning topics\n",
    "This simulates a vector database with documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_BASE = [\n",
    "    # Document 1: RAG Basics\n",
    "    \"\"\"Retrieval-Augmented Generation (RAG) is a technique that combines\n",
    "    large language models with external knowledge retrieval. It works by\n",
    "    first retrieving relevant documents from a knowledge base, then using\n",
    "    those documents as context for the LLM to generate more accurate responses.\n",
    "    This helps reduce hallucinations and provides up-to-date information.\"\"\",\n",
    "\n",
    "    # Document 2: Vector Embeddings\n",
    "    \"\"\"Vector embeddings are dense numerical representations of text that\n",
    "    capture semantic meaning. Documents and queries are converted to vectors\n",
    "    in a high-dimensional space where semantically similar texts have similar\n",
    "    vectors. This enables efficient similarity search using metrics like\n",
    "    cosine similarity.\"\"\",\n",
    "\n",
    "    # Document 3: Transformers\n",
    "    \"\"\"Transformer models use self-attention mechanisms to process sequences.\n",
    "    They consist of encoder and decoder blocks with multi-head attention layers.\n",
    "    The attention mechanism allows the model to focus on relevant parts of the\n",
    "    input sequence. BERT uses only encoders, GPT uses only decoders.\"\"\",\n",
    "\n",
    "    # Document 4: Fine-tuning\n",
    "    \"\"\"Fine-tuning adapts a pre-trained model to a specific task by continuing\n",
    "    training on task-specific data. Methods include full fine-tuning, LoRA\n",
    "    (Low-Rank Adaptation), and prompt tuning. Fine-tuning changes model weights\n",
    "    while RAG keeps weights frozen and retrieves information instead.\"\"\",\n",
    "\n",
    "    # Document 5: Vector Databases\n",
    "    \"\"\"Vector databases like Pinecone, Weaviate, and Chroma are optimized for\n",
    "    storing and searching high-dimensional vectors. They use algorithms like\n",
    "    HNSW (Hierarchical Navigable Small World) and IVF (Inverted File Index)\n",
    "    for approximate nearest neighbor search. This enables fast retrieval even\n",
    "    with millions of documents.\"\"\",\n",
    "\n",
    "    # Document 6: Attention Mechanism\n",
    "    \"\"\"The attention mechanism computes weighted combinations of values based\n",
    "    on query-key similarity. Self-attention allows each position to attend to\n",
    "    all positions in the sequence. Multi-head attention runs several attention\n",
    "    operations in parallel, allowing the model to focus on different aspects.\"\"\",\n",
    "\n",
    "    # Document 7: Context Window\n",
    "    \"\"\"Context window refers to the maximum sequence length a model can process.\n",
    "    For example, GPT-3 has a 4k token window, GPT-4 has up to 128k tokens.\n",
    "    Longer context windows allow processing more information but increase\n",
    "    computational cost quadratically due to self-attention.\"\"\",\n",
    "\n",
    "    # Document 8: Out-of-Distribution Data\n",
    "    \"\"\"Out-of-distribution (OOD) data refers to inputs significantly different\n",
    "    from training data. LLMs struggle with OOD data, often hallucinating or\n",
    "    providing incorrect answers. RAG helps by retrieving current information.\n",
    "    Good models should express uncertainty when facing OOD inputs.\"\"\",\n",
    "\n",
    "    # Document 9: Embeddings vs Tokens\n",
    "    \"\"\"Tokens are discrete units (words or subwords) while embeddings are\n",
    "    continuous vector representations. Tokenization splits text into tokens,\n",
    "    then each token is mapped to a learned embedding vector. These embeddings\n",
    "    capture semantic and syntactic properties of the tokens.\"\"\",\n",
    "\n",
    "    # Document 10: Semantic Search\n",
    "    \"\"\"Semantic search finds results based on meaning rather than keyword matching.\n",
    "    Unlike traditional search (BM25, TF-IDF) that matches exact terms, semantic\n",
    "    search uses embeddings to find conceptually similar content. This enables\n",
    "    finding \"car\" when searching for \"automobile\".\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21634c42",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 2: CHUNKING STRATEGIES\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentChunker:\n",
    "    \"\"\"\n",
    "    Different chunking strategies for documents.\n",
    "\n",
    "    Interview Concept:\n",
    "    ----------------\n",
    "    Chunk size affects retrieval quality:\n",
    "    - Too small: Loses context, retrieves irrelevant snippets\n",
    "    - Too large: Dilutes relevance, wastes context window\n",
    "    - Typical sizes: 200-512 tokens with 10-20% overlap\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def fixed_size_chunking(text: str, chunk_size: int = 100, overlap: int = 20) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into fixed-size character chunks with overlap.\n",
    "\n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            chunk_size: Number of characters per chunk\n",
    "            overlap: Number of overlapping characters between chunks\n",
    "\n",
    "        Interview Note:\n",
    "        --------------\n",
    "        Fixed-size chunking is simple but may split sentences awkwardly.\n",
    "        Overlap helps maintain context at boundaries.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end].strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            start += chunk_size - overlap\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    @staticmethod\n",
    "    def sentence_based_chunking(text: str, sentences_per_chunk: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into chunks of complete sentences.\n",
    "\n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            sentences_per_chunk: Number of sentences per chunk\n",
    "\n",
    "        Interview Note:\n",
    "        --------------\n",
    "        Sentence-based chunking preserves semantic boundaries.\n",
    "        Better for Q&A where complete thoughts matter.\n",
    "        \"\"\"\n",
    "        # Simple sentence splitting (naive - production would use spaCy/nltk)\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "        chunks = []\n",
    "        for i in range(0, len(sentences), sentences_per_chunk):\n",
    "            chunk_sentences = sentences[i:i + sentences_per_chunk]\n",
    "            chunk = '. '.join(chunk_sentences) + '.'\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d9a52",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 3: SIMPLE EMBEDDING MODEL\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7700e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEmbeddingModel:\n",
    "    \"\"\"\n",
    "    Simple embedding model using TF-IDF for demonstration.\n",
    "\n",
    "    Interview Context:\n",
    "    -----------------\n",
    "    In production, you'd use:\n",
    "    - OpenAI ada-002 (1536 dims)\n",
    "    - Sentence-BERT (768 dims)\n",
    "    - BGE/E5 models (open source)\n",
    "\n",
    "    TF-IDF works for demo but misses semantic similarity\n",
    "    (e.g., \"car\" and \"automobile\" won't be close)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_features: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize TF-IDF vectorizer.\n",
    "\n",
    "        Args:\n",
    "            max_features: Maximum number of features (vocabulary size)\n",
    "        \"\"\"\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2)  # Use unigrams and bigrams\n",
    "        )\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def fit(self, documents: List[str]):\n",
    "        \"\"\"\n",
    "        Fit the vectorizer on documents.\n",
    "\n",
    "        Interview Note:\n",
    "        --------------\n",
    "        This is like building a vocabulary. In production embedding models,\n",
    "        this is done during pre-training on massive corpora.\n",
    "        \"\"\"\n",
    "        self.vectorizer.fit(documents)\n",
    "        self.is_fitted = True\n",
    "        print(f\"\u2713 Embedding model fitted on {len(documents)} documents\")\n",
    "        print(f\"\u2713 Vocabulary size: {len(self.vectorizer.vocabulary_)}\")\n",
    "\n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert texts to embedding vectors.\n",
    "\n",
    "        Returns:\n",
    "            Array of shape (n_texts, embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted first!\")\n",
    "\n",
    "        embeddings = self.vectorizer.transform(texts).toarray()\n",
    "        return embeddings\n",
    "\n",
    "    def embed_single(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Embed a single text.\"\"\"\n",
    "        return self.embed([text])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda41bea",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 4: VECTOR SIMILARITY SEARCH\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc656edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDatabase:\n",
    "    \"\"\"\n",
    "    Simple vector database for similarity search.\n",
    "\n",
    "    Interview Context:\n",
    "    -----------------\n",
    "    This simulates vector databases like:\n",
    "    - Pinecone: Cloud-hosted, auto-scaling\n",
    "    - Weaviate: Open source, GraphQL API\n",
    "    - Chroma: Lightweight, embedded\n",
    "    - FAISS: Facebook's similarity search library\n",
    "\n",
    "    Key operations:\n",
    "    1. Index: Store document embeddings\n",
    "    2. Search: Find k nearest neighbors\n",
    "    3. Similarity metric: Usually cosine similarity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: SimpleEmbeddingModel):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "\n",
    "    def index_documents(self, documents: List[str]):\n",
    "        \"\"\"\n",
    "        Index documents by computing and storing their embeddings.\n",
    "\n",
    "        Interview Note:\n",
    "        --------------\n",
    "        In production:\n",
    "        - Documents are chunked first\n",
    "        - Metadata is stored (source, page, timestamp)\n",
    "        - Embeddings are stored in optimized data structures (HNSW graphs)\n",
    "        - Can handle millions/billions of vectors\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"STEP 1: INDEXING DOCUMENTS\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        self.documents = documents\n",
    "        self.embeddings = self.embedding_model.embed(documents)\n",
    "\n",
    "        print(f\"\u2713 Indexed {len(documents)} documents\")\n",
    "        print(f\"\u2713 Embedding dimension: {self.embeddings.shape[1]}\")\n",
    "        print(f\"\u2713 Total storage: {self.embeddings.nbytes / 1024:.2f} KB\")\n",
    "\n",
    "    def search(self, query: str, top_k: int = 3) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"\n",
    "        Search for most similar documents to query.\n",
    "\n",
    "        Args:\n",
    "            query: Search query text\n",
    "            top_k: Number of results to return\n",
    "\n",
    "        Returns:\n",
    "            List of (doc_index, similarity_score, document_text)\n",
    "\n",
    "        Interview Note:\n",
    "        --------------\n",
    "        Cosine similarity is most common:\n",
    "        - sim(A, B) = A\u00b7B / (||A|| ||B||)\n",
    "        - Range: [-1, 1], higher = more similar\n",
    "        - Efficient to compute\n",
    "        - Normalized (insensitive to vector magnitude)\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"STEP 2: RETRIEVAL - Searching for relevant documents\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Query: \\\"{query}\\\"\")\n",
    "\n",
    "        # Embed the query\n",
    "        query_embedding = self.embedding_model.embed_single(query)\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "        # Compute cosine similarity with all documents\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "\n",
    "        # Get top-k most similar documents\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        print(f\"\\nTop {top_k} most relevant documents:\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        for rank, idx in enumerate(top_indices, 1):\n",
    "            score = similarities[idx]\n",
    "            doc = self.documents[idx]\n",
    "            results.append((idx, score, doc))\n",
    "\n",
    "            print(f\"\\nRank {rank}: Document {idx}\")\n",
    "            print(f\"Similarity Score: {score:.4f}\")\n",
    "            print(f\"Preview: {doc[:150]}...\")\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a388fe",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 5: RAG PIPELINE\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbed6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Retrieve \u2192 Augment \u2192 Generate\n",
    "\n",
    "    Interview Explanation:\n",
    "    ---------------------\n",
    "    RAG = Retrieval-Augmented Generation\n",
    "\n",
    "    Traditional LLM: Query \u2192 LLM \u2192 Response\n",
    "    RAG Pipeline: Query \u2192 Retrieve Docs \u2192 Augment Prompt \u2192 LLM \u2192 Response\n",
    "\n",
    "    Benefits:\n",
    "    - Reduces hallucinations (grounds in retrieved facts)\n",
    "    - Enables up-to-date information (update DB, not model)\n",
    "    - Provides citations (can show sources)\n",
    "    - Domain expertise (add specialized documents)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_db: VectorDatabase):\n",
    "        self.vector_db = vector_db\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Step 1: RETRIEVE relevant documents.\n",
    "        \"\"\"\n",
    "        results = self.vector_db.search(query, top_k=top_k)\n",
    "        return [doc for _, _, doc in results]\n",
    "\n",
    "    def augment(self, query: str, documents: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Step 2: AUGMENT the query with retrieved context.\n",
    "\n",
    "        Interview Note:\n",
    "        --------------\n",
    "        Prompt engineering is crucial:\n",
    "        - Clear instruction to use only provided context\n",
    "        - Explicit instruction to say \"I don't know\" if info not in context\n",
    "        - Structured format (Context \u2192 Question \u2192 Answer)\n",
    "        \"\"\"\n",
    "        # Create augmented prompt with retrieved context\n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}: {doc}\"\n",
    "                               for i, doc in enumerate(documents)])\n",
    "\n",
    "        augmented_prompt = f\"\"\"You are a helpful AI assistant. Answer the question based ONLY on the provided context.\n",
    "If the context doesn't contain enough information to answer, say \"I don't have enough information to answer this.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        return augmented_prompt\n",
    "\n",
    "    def generate(self, augmented_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Step 3: GENERATE response (simulated).\n",
    "\n",
    "        Interview Note:\n",
    "        --------------\n",
    "        In production, this would call an LLM API:\n",
    "        - OpenAI GPT-4\n",
    "        - Anthropic Claude\n",
    "        - Local model (Llama, Mistral)\n",
    "\n",
    "        For this demo, we just show the augmented prompt that would be sent.\n",
    "        \"\"\"\n",
    "        return augmented_prompt\n",
    "\n",
    "    def query(self, user_query: str, top_k: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete RAG pipeline execution.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with retrieved_docs and augmented_prompt\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"COMPLETE RAG PIPELINE EXECUTION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"User Query: \\\"{user_query}\\\"\")\n",
    "\n",
    "        # Step 1: Retrieve\n",
    "        retrieved_docs = self.retrieve(user_query, top_k=top_k)\n",
    "\n",
    "        # Step 2: Augment\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"STEP 3: AUGMENTATION - Creating enhanced prompt\")\n",
    "        print(f\"{'='*70}\")\n",
    "        augmented_prompt = self.augment(user_query, retrieved_docs)\n",
    "        print(f\"\u2713 Added {len(retrieved_docs)} documents as context\")\n",
    "        print(f\"\u2713 Prompt length: {len(augmented_prompt)} characters\")\n",
    "\n",
    "        # Step 3: Generate (simulated)\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"STEP 4: GENERATION (simulated)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(\"In production, this augmented prompt would be sent to an LLM.\")\n",
    "        print(\"The LLM would generate an answer grounded in the retrieved context.\")\n",
    "\n",
    "        return {\n",
    "            'query': user_query,\n",
    "            'retrieved_docs': retrieved_docs,\n",
    "            'augmented_prompt': augmented_prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec47ff4",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 6: OUT-OF-DISTRIBUTION HANDLING\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fabf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_ood_handling():\n",
    "    \"\"\"\n",
    "    Demonstrate how RAG helps with out-of-distribution (OOD) queries.\n",
    "\n",
    "    Interview Context - Q40: OOD Data Handling\n",
    "    ------------------------------------------\n",
    "    LLMs struggle with:\n",
    "    1. Topics after training cutoff\n",
    "    2. Domain-specific knowledge\n",
    "    3. Novel terms/concepts\n",
    "    4. Different distributions\n",
    "\n",
    "    RAG helps by:\n",
    "    - Retrieving current information\n",
    "    - Providing domain expertise\n",
    "    - Reducing reliance on parametric knowledge\n",
    "    - Enabling graceful degradation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"OUT-OF-DISTRIBUTION (OOD) QUERY HANDLING\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Simulate different query distributions\n",
    "    queries = {\n",
    "        'in_distribution': \"What is RAG and how does it work?\",\n",
    "        'borderline': \"How do embeddings relate to semantic search?\",\n",
    "        'out_of_distribution': \"Explain quantum computing principles\"  # Not in our KB\n",
    "    }\n",
    "\n",
    "    print(\"\\nQuery Distribution Analysis:\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for query_type, query in queries.items():\n",
    "        print(f\"\\n{query_type.upper().replace('_', ' ')}:\")\n",
    "        print(f\"Query: {query}\")\n",
    "\n",
    "        # In production, you'd compute embedding distance to training distribution\n",
    "        # Here we just show the concept\n",
    "        if query_type == 'out_of_distribution':\n",
    "            print(\"\u274c Query is OOD (far from training distribution)\")\n",
    "            print(\"   \u2192 RAG will still retrieve best matches\")\n",
    "            print(\"   \u2192 System should express uncertainty\")\n",
    "            print(\"   \u2192 Alternative: Web search tool, or return 'I don't know'\")\n",
    "        else:\n",
    "            print(\"\u2713 Query is in/near training distribution\")\n",
    "            print(\"  \u2192 RAG will find relevant documents\")\n",
    "            print(\"  \u2192 High confidence response expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aa90c2",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 7: VISUALIZATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac067ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embedding_space(\n",
    "    vector_db: VectorDatabase,\n",
    "    query: str,\n",
    "    output_path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize document and query embeddings in 2D space.\n",
    "\n",
    "    Interview Visual Aid:\n",
    "    --------------------\n",
    "    Shows how:\n",
    "    - Documents cluster by topic\n",
    "    - Query embedding is close to relevant documents\n",
    "    - Similarity = geometric proximity in embedding space\n",
    "    \"\"\"\n",
    "    # Get all embeddings\n",
    "    all_embeddings = vector_db.embeddings\n",
    "    query_embedding = vector_db.embedding_model.embed_single(query).reshape(1, -1)\n",
    "\n",
    "    # Combine for PCA\n",
    "    combined = np.vstack([all_embeddings, query_embedding])\n",
    "\n",
    "    # Reduce to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(combined)\n",
    "\n",
    "    # Split back\n",
    "    docs_2d = embeddings_2d[:-1]\n",
    "    query_2d = embeddings_2d[-1]\n",
    "\n",
    "    # Compute similarities for coloring\n",
    "    query_embedding_orig = query_embedding.reshape(1, -1)\n",
    "    similarities = cosine_similarity(query_embedding_orig, all_embeddings)[0]\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot documents with color based on similarity\n",
    "    scatter = ax.scatter(\n",
    "        docs_2d[:, 0],\n",
    "        docs_2d[:, 1],\n",
    "        c=similarities,\n",
    "        cmap='RdYlGn',\n",
    "        s=200,\n",
    "        alpha=0.6,\n",
    "        edgecolors='black',\n",
    "        linewidth=1.5,\n",
    "        label='Documents'\n",
    "    )\n",
    "\n",
    "    # Add document labels\n",
    "    for i, (x, y) in enumerate(docs_2d):\n",
    "        ax.annotate(\n",
    "            f'Doc {i}',\n",
    "            (x, y),\n",
    "            fontsize=9,\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            fontweight='bold'\n",
    "        )\n",
    "\n",
    "    # Plot query\n",
    "    ax.scatter(\n",
    "        query_2d[0],\n",
    "        query_2d[1],\n",
    "        c='red',\n",
    "        s=500,\n",
    "        marker='*',\n",
    "        edgecolors='darkred',\n",
    "        linewidth=2,\n",
    "        label='Query',\n",
    "        zorder=5\n",
    "    )\n",
    "\n",
    "    ax.annotate(\n",
    "        'Query',\n",
    "        (query_2d[0], query_2d[1]),\n",
    "        xytext=(10, 10),\n",
    "        textcoords='offset points',\n",
    "        fontsize=12,\n",
    "        fontweight='bold',\n",
    "        color='darkred',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7)\n",
    "    )\n",
    "\n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Cosine Similarity to Query', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Styling\n",
    "    ax.set_xlabel('PCA Component 1', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('PCA Component 2', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(\n",
    "        'RAG Embedding Space Visualization\\n'\n",
    "        'Documents colored by similarity to query',\n",
    "        fontsize=14,\n",
    "        fontweight='bold',\n",
    "        pad=20\n",
    "    )\n",
    "    ax.legend(loc='upper right', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add explained variance\n",
    "    var_explained = pca.explained_variance_ratio_\n",
    "    ax.text(\n",
    "        0.02, 0.98,\n",
    "        f'PCA Variance Explained:\\n'\n",
    "        f'PC1: {var_explained[0]:.1%}\\n'\n",
    "        f'PC2: {var_explained[1]:.1%}',\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=10,\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n\u2713 Saved embedding space visualization to {output_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_retrieval_scores(\n",
    "    vector_db: VectorDatabase,\n",
    "    query: str,\n",
    "    output_path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize similarity scores between query and all documents.\n",
    "\n",
    "    Interview Visual Aid:\n",
    "    --------------------\n",
    "    Shows:\n",
    "    - Which documents are most relevant (highest scores)\n",
    "    - Score distribution (how selective is retrieval)\n",
    "    - Top-k cutoff visualization\n",
    "    \"\"\"\n",
    "    # Compute similarities\n",
    "    query_embedding = vector_db.embedding_model.embed_single(query).reshape(1, -1)\n",
    "    similarities = cosine_similarity(query_embedding, vector_db.embeddings)[0]\n",
    "\n",
    "    # Sort for visualization\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    sorted_similarities = similarities[sorted_indices]\n",
    "\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Left plot: Bar chart of all scores\n",
    "    colors = ['green' if i < 3 else 'lightblue' for i in range(len(similarities))]\n",
    "    bars = ax1.bar(\n",
    "        range(len(sorted_similarities)),\n",
    "        sorted_similarities,\n",
    "        color=colors,\n",
    "        edgecolor='black',\n",
    "        linewidth=1.5\n",
    "    )\n",
    "\n",
    "    # Highlight top-3\n",
    "    for i in range(min(3, len(bars))):\n",
    "        bars[i].set_label('Top-3 Retrieved' if i == 0 else '')\n",
    "\n",
    "    ax1.axhline(\n",
    "        y=sorted_similarities[2] if len(sorted_similarities) > 2 else 0,\n",
    "        color='red',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        label='Top-k Cutoff (k=3)'\n",
    "    )\n",
    "\n",
    "    ax1.set_xlabel('Document Index (sorted by score)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Cosine Similarity Score', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title(\n",
    "        'Document Relevance Scores\\n'\n",
    "        f'Query: \"{query[:40]}...\"',\n",
    "        fontsize=13,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    ax1.legend(loc='upper right', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.set_ylim([0, max(sorted_similarities) * 1.1])\n",
    "\n",
    "    # Right plot: Top documents with detailed scores\n",
    "    top_k = 5\n",
    "    top_indices = sorted_indices[:top_k]\n",
    "    top_scores = sorted_similarities[:top_k]\n",
    "\n",
    "    y_pos = np.arange(top_k)\n",
    "    bars2 = ax2.barh(y_pos, top_scores, color='green', alpha=0.6, edgecolor='black')\n",
    "\n",
    "    # Add score labels\n",
    "    for i, (idx, score) in enumerate(zip(top_indices, top_scores)):\n",
    "        ax2.text(\n",
    "            score + 0.01,\n",
    "            i,\n",
    "            f'{score:.4f}',\n",
    "            va='center',\n",
    "            fontweight='bold'\n",
    "        )\n",
    "\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels([f'Doc {idx}' for idx in top_indices])\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.set_xlabel('Cosine Similarity Score', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title(\n",
    "        f'Top-{top_k} Retrieved Documents\\n'\n",
    "        'These would be used as context',\n",
    "        fontsize=13,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    ax2.set_xlim([0, 1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\u2713 Saved retrieval scores visualization to {output_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_chunking_strategies():\n",
    "    \"\"\"\n",
    "    Compare different chunking strategies and their effects.\n",
    "\n",
    "    Interview Insight:\n",
    "    -----------------\n",
    "    Chunking affects retrieval quality:\n",
    "    - Fixed-size: May split concepts, simple to implement\n",
    "    - Sentence-based: Preserves meaning, variable sizes\n",
    "    - Semantic: Best quality, requires more processing\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"CHUNKING STRATEGY COMPARISON\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    sample_text = KNOWLEDGE_BASE[0]  # Use first document\n",
    "\n",
    "    chunker = DocumentChunker()\n",
    "\n",
    "    # Strategy 1: Fixed-size\n",
    "    fixed_chunks = chunker.fixed_size_chunking(sample_text, chunk_size=100, overlap=20)\n",
    "    print(f\"\\nSTRATEGY 1: Fixed-Size Chunking (100 chars, 20 overlap)\")\n",
    "    print(f\"Number of chunks: {len(fixed_chunks)}\")\n",
    "    print(f\"Average chunk size: {np.mean([len(c) for c in fixed_chunks]):.1f} chars\")\n",
    "    print(f\"Chunk size std dev: {np.std([len(c) for c in fixed_chunks]):.1f}\")\n",
    "    print(f\"First chunk preview: {fixed_chunks[0][:80]}...\")\n",
    "\n",
    "    # Strategy 2: Sentence-based\n",
    "    sentence_chunks = chunker.sentence_based_chunking(sample_text, sentences_per_chunk=2)\n",
    "    print(f\"\\nSTRATEGY 2: Sentence-Based Chunking (2 sentences/chunk)\")\n",
    "    print(f\"Number of chunks: {len(sentence_chunks)}\")\n",
    "    print(f\"Average chunk size: {np.mean([len(c) for c in sentence_chunks]):.1f} chars\")\n",
    "    print(f\"Chunk size std dev: {np.std([len(c) for c in sentence_chunks]):.1f}\")\n",
    "    print(f\"First chunk preview: {sentence_chunks[0][:80]}...\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"INTERVIEW INSIGHT:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"Fixed-size: Predictable, may split sentences (BAD for meaning)\")\n",
    "    print(\"Sentence-based: Preserves semantics (BETTER for Q&A)\")\n",
    "    print(\"Production: Use semantic chunking with embeddings or LangChain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1024ae",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "MAIN DEMO\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main demonstration of RAG pipeline.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"SIMPLE RAG (RETRIEVAL-AUGMENTED GENERATION) DEMONSTRATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nInterview Questions Covered:\")\n",
    "    print(\"- Q36: What is RAG and why is it important?\")\n",
    "    print(\"- Q40: How do LLMs handle out-of-distribution data?\")\n",
    "    print()\n",
    "    print(\"Pipeline: RETRIEVE \u2192 AUGMENT \u2192 GENERATE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Setup output directory\n",
    "    output_dir = Path(\"/Users/zack/dev/ml-refresher/data/interview_viz\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ========================================================================\n",
    "    # 1. COMPARE CHUNKING STRATEGIES\n",
    "    # ========================================================================\n",
    "    compare_chunking_strategies()\n",
    "\n",
    "    # ========================================================================\n",
    "    # 2. CREATE AND INDEX KNOWLEDGE BASE\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"BUILDING RAG SYSTEM\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Initialize embedding model\n",
    "    embedding_model = SimpleEmbeddingModel(max_features=100)\n",
    "    embedding_model.fit(KNOWLEDGE_BASE)\n",
    "\n",
    "    # Create vector database and index documents\n",
    "    vector_db = VectorDatabase(embedding_model)\n",
    "    vector_db.index_documents(KNOWLEDGE_BASE)\n",
    "\n",
    "    # ========================================================================\n",
    "    # 3. CREATE RAG PIPELINE\n",
    "    # ========================================================================\n",
    "    rag = RAGPipeline(vector_db)\n",
    "\n",
    "    # ========================================================================\n",
    "    # 4. TEST RAG WITH DIFFERENT QUERIES\n",
    "    # ========================================================================\n",
    "    queries = [\n",
    "        \"How does RAG help reduce hallucinations?\",\n",
    "        \"What are vector databases used for?\",\n",
    "        \"Explain the attention mechanism in transformers\"\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TESTING RAG PIPELINE WITH MULTIPLE QUERIES\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n\\n{'#'*70}\")\n",
    "        print(f\"QUERY {i}/{len(queries)}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "\n",
    "        result = rag.query(query, top_k=3)\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"AUGMENTED PROMPT (would be sent to LLM):\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(result['augmented_prompt'][:500] + \"...\")\n",
    "\n",
    "        # Visualizations for first query\n",
    "        if i == 1:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"CREATING VISUALIZATIONS\")\n",
    "            print(f\"{'='*70}\")\n",
    "\n",
    "            # Embedding space visualization\n",
    "            viz_path_1 = output_dir / \"11_rag_embedding_space.png\"\n",
    "            visualize_embedding_space(vector_db, query, str(viz_path_1))\n",
    "\n",
    "            # Retrieval scores visualization\n",
    "            viz_path_2 = output_dir / \"12_rag_retrieval_scores.png\"\n",
    "            visualize_retrieval_scores(vector_db, query, str(viz_path_2))\n",
    "\n",
    "    # ========================================================================\n",
    "    # 5. DEMONSTRATE OOD HANDLING\n",
    "    # ========================================================================\n",
    "    demonstrate_ood_handling()\n",
    "\n",
    "    # ========================================================================\n",
    "    # 6. SUMMARY\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RAG DEMO COMPLETE - KEY TAKEAWAYS\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(\"\"\"\n",
    "Interview Key Points:\n",
    "--------------------\n",
    "\n",
    "1. RAG PIPELINE (Q36):\n",
    "   \u2713 Retrieve: Find relevant docs using vector similarity\n",
    "   \u2713 Augment: Add docs to prompt as context\n",
    "   \u2713 Generate: LLM generates grounded response\n",
    "\n",
    "2. WHY RAG?\n",
    "   \u2713 Reduces hallucinations (grounds in facts)\n",
    "   \u2713 Enables up-to-date info (update DB, not model)\n",
    "   \u2713 Provides citations (can show sources)\n",
    "   \u2713 Domain expertise (add specialized docs)\n",
    "\n",
    "3. KEY COMPONENTS:\n",
    "   \u2713 Embedding model (TF-IDF, BERT, OpenAI)\n",
    "   \u2713 Vector database (Pinecone, Chroma, FAISS)\n",
    "   \u2713 Similarity search (cosine similarity)\n",
    "   \u2713 Prompt engineering (clear instructions)\n",
    "\n",
    "4. OOD HANDLING (Q40):\n",
    "   \u2713 RAG helps with OOD by retrieving current info\n",
    "   \u2713 Reduces reliance on parametric knowledge\n",
    "   \u2713 Should express uncertainty when no relevant docs\n",
    "   \u2713 Alternative: Web search tools, API calls\n",
    "\n",
    "5. CHUNKING STRATEGIES:\n",
    "   \u2713 Fixed-size: Simple, may split concepts\n",
    "   \u2713 Sentence-based: Preserves meaning\n",
    "   \u2713 Semantic: Best quality, more complex\n",
    "   \u2713 Typical: 200-512 tokens, 10-20% overlap\n",
    "\n",
    "6. RAG vs FINE-TUNING:\n",
    "   \u2713 RAG: Facts, citations, easy updates\n",
    "   \u2713 Fine-tuning: Behavior, style, task adaptation\n",
    "   \u2713 Often used together!\n",
    "\n",
    "7. PRODUCTION CONSIDERATIONS:\n",
    "   \u2713 Embedding quality matters most\n",
    "   \u2713 Hybrid retrieval (dense + sparse)\n",
    "   \u2713 Reranking for precision\n",
    "   \u2713 Query rewriting/expansion\n",
    "   \u2713 Metadata filtering\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"Visualizations saved to:\")\n",
    "    print(f\"  - {output_dir / '11_rag_embedding_space.png'}\")\n",
    "    print(f\"  - {output_dir / '12_rag_retrieval_scores.png'}\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6bfdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}