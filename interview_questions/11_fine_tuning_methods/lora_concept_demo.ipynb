{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: LoRA Concept Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f42ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LoRA (Low-Rank Adaptation) Concept Demo for LLM Interview Preparation\n",
    "=====================================================================\n",
    "\n",
    "This educational demo covers fine-tuning methods for LLM interviews:\n",
    "- Q4: What is LoRA and how does it work?\n",
    "- Q14: How to prevent catastrophic forgetting?\n",
    "- Q35: Parameter-efficient fine-tuning methods\n",
    "\n",
    "Topics Covered:\n",
    "1. Low-rank matrix approximation using SVD\n",
    "2. Simple LoRA implementation from scratch\n",
    "3. Parameter count comparison (full fine-tuning vs LoRA)\n",
    "4. Catastrophic forgetting demonstration\n",
    "5. How freezing weights + adapters prevents forgetting\n",
    "\n",
    "Key LoRA Concepts:\n",
    "- Instead of updating all weights W \u2208 R^(d\u00d7k), we keep W frozen\n",
    "- Add low-rank decomposition: \u0394W = B @ A where B \u2208 R^(d\u00d7r), A \u2208 R^(r\u00d7k), r << min(d,k)\n",
    "- Forward pass: h = W\u2080x + \u0394Wx = W\u2080x + BAx\n",
    "- Only train A and B, drastically reducing parameters\n",
    "- Original knowledge preserved in W\u2080, new knowledge in BA\n",
    "\n",
    "Author: Educational Demo for ML Interview Prep\n",
    "Date: 2025-12-03\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a7f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "VIZ_DIR = Path(\"/Users/zack/dev/ml-refresher/data/interview_viz\")\n",
    "VIZ_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a454e8e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LoRA Concept Demo for LLM Interview Preparation\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {DEVICE}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef369e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: Low-Rank Matrix Approximation Visualization\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 1: Low-Rank Matrix Approximation (Foundation of LoRA)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc75a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_low_rank_approximation():\n",
    "    \"\"\"\n",
    "    Demonstrate how a matrix can be approximated by low-rank decomposition.\n",
    "\n",
    "    KEY INTERVIEW CONCEPT:\n",
    "    - LoRA leverages the insight that weight updates \u0394W often have low \"intrinsic rank\"\n",
    "    - Instead of storing full \u0394W (d\u00d7k parameters), we store B@A (d\u00d7r + r\u00d7k parameters)\n",
    "    - When r << min(d,k), this is MUCH more efficient\n",
    "\n",
    "    Mathematical Foundation:\n",
    "    - SVD: W = U\u03a3V^T\n",
    "    - Low-rank approx: W_r \u2248 U[:,:r] @ \u03a3[:r,:r] @ V[:,:r]^T\n",
    "    - LoRA: \u0394W \u2248 B @ A where B and A are learned directly\n",
    "    \"\"\"\n",
    "    print(\"\\nDemonstrating low-rank approximation using SVD...\")\n",
    "\n",
    "    # Create a weight matrix (simulating a layer in a neural network)\n",
    "    d, k = 512, 512  # Input and output dimensions\n",
    "    print(f\"\\nOriginal weight matrix dimensions: {d} \u00d7 {k} = {d*k:,} parameters\")\n",
    "\n",
    "    # Create a matrix with inherent low-rank structure (simulating real neural net weights)\n",
    "    # Real neural network weight updates often have low intrinsic dimensionality\n",
    "    rank = 8\n",
    "    U = torch.randn(d, rank)\n",
    "    V = torch.randn(k, rank)\n",
    "    W_full = U @ V.T + 0.1 * torch.randn(d, k)  # Low rank + small noise\n",
    "\n",
    "    # Perform SVD\n",
    "    U, S, Vt = torch.svd(W_full)\n",
    "\n",
    "    # Visualize singular values (shows the rank structure)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot 1: Singular values (log scale)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.semilogy(S.numpy(), 'b-', linewidth=2)\n",
    "    plt.axvline(x=rank, color='r', linestyle='--', linewidth=2, label=f'Rank {rank}')\n",
    "    plt.xlabel('Singular Value Index', fontsize=12)\n",
    "    plt.ylabel('Singular Value (log scale)', fontsize=12)\n",
    "    plt.title('Singular Value Spectrum\\n(Shows Matrix Rank)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 2: Cumulative explained variance\n",
    "    cumsum_variance = torch.cumsum(S**2, dim=0) / torch.sum(S**2)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(cumsum_variance.numpy(), 'g-', linewidth=2)\n",
    "    plt.axvline(x=rank, color='r', linestyle='--', linewidth=2, label=f'Rank {rank}')\n",
    "    plt.axhline(y=0.95, color='orange', linestyle=':', linewidth=2, label='95% variance')\n",
    "    plt.xlabel('Number of Components', fontsize=12)\n",
    "    plt.ylabel('Cumulative Variance Explained', fontsize=12)\n",
    "    plt.title('Cumulative Variance Explained\\n(Information Retention)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VIZ_DIR / \"01_low_rank_approximation.png\", dpi=300, bbox_inches='tight')\n",
    "    print(f\"\u2713 Saved visualization: {VIZ_DIR / '01_low_rank_approximation.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "    # Compare different rank approximations\n",
    "    print(\"\\nReconstruction Error vs Rank:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Rank':<10} {'Parameters':<20} {'Frobenius Error':<20} {'Error %'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for r in [1, 2, 4, 8, 16, 32, 64]:\n",
    "        # Low-rank approximation: W \u2248 U[:,:r] @ S[:r] @ V[:,:r]^T\n",
    "        W_approx = U[:, :r] @ torch.diag(S[:r]) @ Vt[:r, :]\n",
    "\n",
    "        error = torch.norm(W_full - W_approx, p='fro').item()\n",
    "        error_pct = 100 * error / torch.norm(W_full, p='fro').item()\n",
    "        params = d * r + r * k  # B is d\u00d7r, A is r\u00d7k\n",
    "\n",
    "        print(f\"{r:<10} {params:<20,} {error:<20.4f} {error_pct:.2f}%\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Full rank {d:<4} {d*k:<20,} {'0.0000':<20} 0.00%\")\n",
    "    print()\n",
    "\n",
    "    # KEY INTERVIEW INSIGHT\n",
    "    print(\"\\n\ud83d\udca1 KEY INTERVIEW INSIGHT:\")\n",
    "    print(\"   With rank=8: only 8,192 params vs 262,144 (97% reduction!)\")\n",
    "    print(\"   Yet we can capture most of the matrix information!\")\n",
    "    print(\"   This is why LoRA works: weight updates are low-rank!\")\n",
    "\n",
    "    return W_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb507f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: LoRA Implementation from Scratch\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 2: LoRA Implementation from Scratch\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8093d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) Layer Implementation\n",
    "\n",
    "    INTERVIEW EXPLANATION:\n",
    "    Instead of fine-tuning the full weight matrix W \u2208 R^(d\u00d7k):\n",
    "    1. Freeze the original weights W\u2080 (pretrained)\n",
    "    2. Add trainable low-rank matrices: \u0394W = B @ A\n",
    "       - B \u2208 R^(d\u00d7r): \"down-projection\"\n",
    "       - A \u2208 R^(r\u00d7k): \"up-projection\"\n",
    "       - r << min(d,k): the rank (typically 4-16 for LLMs)\n",
    "    3. Forward pass: h = (W\u2080 + \u03b1\u00b7BA)x = W\u2080x + \u03b1\u00b7BAx\n",
    "       - \u03b1: scaling factor (typically \u03b1 = r for numerical stability)\n",
    "\n",
    "    Benefits for Interview:\n",
    "    - Reduces trainable parameters by 10,000x for large models\n",
    "    - Preserves original model knowledge (no catastrophic forgetting)\n",
    "    - Can swap adapters for different tasks\n",
    "    - Merges back into original weights: W_final = W\u2080 + BA\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 4,\n",
    "        alpha: float = 1.0,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Input dimension (k in W \u2208 R^(d\u00d7k))\n",
    "            out_features: Output dimension (d in W \u2208 R^(d\u00d7k))\n",
    "            rank: Bottleneck dimension r (typically 1-64)\n",
    "            alpha: Scaling factor for the low-rank update\n",
    "            dropout: Dropout probability on the LoRA path\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Original weights (frozen during LoRA training)\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.weight.requires_grad = False  # FROZEN!\n",
    "\n",
    "        # LoRA low-rank matrices (trainable)\n",
    "        # A: initialize with Kaiming (He) initialization\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, in_features))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
    "\n",
    "        # B: initialize to zero (so \u0394W = BA starts at zero)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "\n",
    "        # Optional dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        # Scaling factor (often set to rank for stability)\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: h = W\u2080x + \u03b1\u00b7BAx\n",
    "\n",
    "        Interview talking point:\n",
    "        - First path: pretrained knowledge (W\u2080x)\n",
    "        - Second path: task-specific adaptation (BAx)\n",
    "        - Scaling ensures numerical stability\n",
    "        \"\"\"\n",
    "        # Original pretrained computation\n",
    "        result = F.linear(x, self.weight)\n",
    "\n",
    "        # LoRA adaptation path: x -> A (down-project) -> B (up-project)\n",
    "        lora_output = self.dropout(x @ self.lora_A.T)  # (batch, rank)\n",
    "        lora_output = lora_output @ self.lora_B.T      # (batch, out_features)\n",
    "\n",
    "        # Combine with scaling\n",
    "        result = result + self.scaling * lora_output\n",
    "\n",
    "        return result\n",
    "\n",
    "    def merge_weights(self):\n",
    "        \"\"\"\n",
    "        Merge LoRA weights back into the original weight matrix.\n",
    "\n",
    "        Interview point: After training, we can merge BA into W\u2080:\n",
    "        W_final = W\u2080 + \u03b1\u00b7BA\n",
    "\n",
    "        This means no inference overhead!\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Compute \u0394W = BA\n",
    "            delta_W = self.lora_B @ self.lora_A\n",
    "            # Merge into original weights\n",
    "            self.weight.data += self.scaling * delta_W\n",
    "            # Reset LoRA matrices\n",
    "            self.lora_A.zero_()\n",
    "            self.lora_B.zero_()\n",
    "\n",
    "    def get_parameter_counts(self) -> Dict[str, int]:\n",
    "        \"\"\"Get parameter counts for comparison.\"\"\"\n",
    "        original_params = self.out_features * self.in_features\n",
    "        lora_params = (self.rank * self.in_features) + (self.out_features * self.rank)\n",
    "\n",
    "        return {\n",
    "            'original': original_params,\n",
    "            'lora': lora_params,\n",
    "            'trainable': lora_params,  # Only LoRA params are trainable\n",
    "            'frozen': original_params,\n",
    "            'reduction_ratio': original_params / lora_params\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_lora_layer():\n",
    "    \"\"\"Demonstrate LoRA layer parameter efficiency.\"\"\"\n",
    "    print(\"\\nDemonstrating LoRA Layer Parameter Efficiency...\")\n",
    "\n",
    "    # Create layers with different configurations\n",
    "    configs = [\n",
    "        (768, 768, 4),    # Small rank (BERT-base dimension)\n",
    "        (768, 768, 8),    # Medium rank\n",
    "        (768, 768, 16),   # Larger rank\n",
    "        (4096, 4096, 8),  # Large model dimension (GPT-style)\n",
    "    ]\n",
    "\n",
    "    print(\"\\nParameter Comparison Table:\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Dimensions':<20} {'Rank':<8} {'Full Params':<15} {'LoRA Params':<15} \"\n",
    "          f\"{'Reduction':<15} {'% Trainable'}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    for in_dim, out_dim, rank in configs:\n",
    "        layer = LoRALayer(in_dim, out_dim, rank=rank)\n",
    "        counts = layer.get_parameter_counts()\n",
    "        pct_trainable = 100 * counts['trainable'] / counts['original']\n",
    "\n",
    "        print(f\"{in_dim}\u00d7{out_dim:<13} {rank:<8} {counts['original']:<15,} \"\n",
    "              f\"{counts['lora']:<15,} {counts['reduction_ratio']:<15.1f}x \"\n",
    "              f\"{pct_trainable:.2f}%\")\n",
    "\n",
    "    print(\"=\"*100)\n",
    "    print(\"\\n\ud83d\udca1 KEY INTERVIEW INSIGHT:\")\n",
    "    print(\"   For a 768\u00d7768 layer with rank=8:\")\n",
    "    print(\"   - Full fine-tuning: 589,824 parameters\")\n",
    "    print(\"   - LoRA: only 12,288 parameters (48x reduction!)\")\n",
    "    print(\"   - For entire LLM: 10,000x+ reduction is common!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39241736",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: Simple Neural Network for Demonstrations\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3: Simple Neural Network for Catastrophic Forgetting Demo\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple network to demonstrate catastrophic forgetting.\n",
    "\n",
    "    This is intentionally simple (not an actual LLM) to clearly show:\n",
    "    1. How fine-tuning all weights destroys original knowledge\n",
    "    2. How LoRA preserves original knowledge\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 10, hidden_dim: int = 64, output_dim: int = 2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRANetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Same network but with LoRA adapters.\n",
    "\n",
    "    Interview explanation:\n",
    "    - Original weights are frozen (preserves pretrained knowledge)\n",
    "    - LoRA adapters are added (learns new task)\n",
    "    - Result: No catastrophic forgetting!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 10, hidden_dim: int = 64,\n",
    "                 output_dim: int = 2, rank: int = 4):\n",
    "        super().__init__()\n",
    "        # Use our LoRA layers instead of regular Linear\n",
    "        self.fc1 = LoRALayer(input_dim, hidden_dim, rank=rank)\n",
    "        self.fc2 = LoRALayer(hidden_dim, hidden_dim, rank=rank)\n",
    "        self.fc3 = LoRALayer(hidden_dim, output_dim, rank=rank)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\"Get only the trainable LoRA parameters.\"\"\"\n",
    "        return [p for p in self.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b14aa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: Catastrophic Forgetting Demonstration\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4: Catastrophic Forgetting Demonstration\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets():\n",
    "    \"\"\"\n",
    "    Create two simple datasets to demonstrate catastrophic forgetting.\n",
    "\n",
    "    Task A (Original): Binary classification with pattern [1, 1, 1, ...]\n",
    "    Task B (New): Binary classification with pattern [-1, -1, -1, ...]\n",
    "\n",
    "    Interview explanation:\n",
    "    - Model first learns Task A\n",
    "    - Then we fine-tune on Task B\n",
    "    - Full fine-tuning: forgets Task A (catastrophic forgetting)\n",
    "    - LoRA: remembers both tasks!\n",
    "    \"\"\"\n",
    "    n_samples = 200\n",
    "    input_dim = 10\n",
    "\n",
    "    # Task A: Pattern with positive values\n",
    "    X_task_a = torch.randn(n_samples, input_dim) + 1.0\n",
    "    y_task_a = (X_task_a.sum(dim=1) > 5).long()\n",
    "\n",
    "    # Task B: Pattern with negative values\n",
    "    X_task_b = torch.randn(n_samples, input_dim) - 1.0\n",
    "    y_task_b = (X_task_b.sum(dim=1) < -5).long()\n",
    "\n",
    "    return (X_task_a, y_task_a), (X_task_b, y_task_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f239bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, epochs=100, lr=0.01, model_name=\"Model\"):\n",
    "    \"\"\"Train a model on a single task.\"\"\"\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5bba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Evaluate model accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        accuracy = (predictions == y).float().mean().item()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_catastrophic_forgetting():\n",
    "    \"\"\"\n",
    "    Demonstrate catastrophic forgetting and how LoRA prevents it.\n",
    "\n",
    "    CRITICAL INTERVIEW CONCEPT:\n",
    "    When you fine-tune all parameters on a new task, the model \"forgets\"\n",
    "    the original task. This is catastrophic forgetting.\n",
    "\n",
    "    LoRA prevents this by:\n",
    "    1. Freezing original weights (keeps old knowledge)\n",
    "    2. Adding adapters (learns new knowledge)\n",
    "    \"\"\"\n",
    "    print(\"\\nDemonstrating Catastrophic Forgetting vs LoRA...\")\n",
    "\n",
    "    # Create datasets\n",
    "    (X_task_a, y_task_a), (X_task_b, y_task_b) = create_datasets()\n",
    "\n",
    "    # ========================================================================\n",
    "    # Experiment 1: Full Fine-Tuning (Shows Catastrophic Forgetting)\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Experiment 1: FULL FINE-TUNING (All parameters trainable)\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    model_full = SimpleNetwork(input_dim=10, hidden_dim=64, output_dim=2)\n",
    "\n",
    "    # Step 1: Train on Task A (original task)\n",
    "    print(\"\\n1. Training on Task A (original task)...\")\n",
    "    train_model(model_full, X_task_a, y_task_a, epochs=100, lr=0.01)\n",
    "    acc_a_before = evaluate_model(model_full, X_task_a, y_task_a)\n",
    "    acc_b_before = evaluate_model(model_full, X_task_b, y_task_b)\n",
    "    print(f\"   Task A accuracy: {acc_a_before:.1%}\")\n",
    "    print(f\"   Task B accuracy: {acc_b_before:.1%} (not trained yet)\")\n",
    "\n",
    "    # Step 2: Fine-tune on Task B (new task)\n",
    "    print(\"\\n2. Fine-tuning on Task B (new task)...\")\n",
    "    train_model(model_full, X_task_b, y_task_b, epochs=100, lr=0.01)\n",
    "    acc_a_after = evaluate_model(model_full, X_task_a, y_task_a)\n",
    "    acc_b_after = evaluate_model(model_full, X_task_b, y_task_b)\n",
    "    print(f\"   Task A accuracy: {acc_a_after:.1%} \u26a0\ufe0f FORGOT!\")\n",
    "    print(f\"   Task B accuracy: {acc_b_after:.1%}\")\n",
    "\n",
    "    forgetting = acc_a_before - acc_a_after\n",
    "    print(f\"\\n   \ud83d\udcc9 Catastrophic Forgetting: {forgetting:.1%} accuracy loss on Task A!\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Experiment 2: LoRA Fine-Tuning (Prevents Catastrophic Forgetting)\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Experiment 2: LoRA FINE-TUNING (Frozen weights + adapters)\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    model_lora = LoRANetwork(input_dim=10, hidden_dim=64, output_dim=2, rank=4)\n",
    "\n",
    "    # Step 1: Train on Task A (simulating pretraining)\n",
    "    print(\"\\n1. Training on Task A (original task)...\")\n",
    "    # First, unfreeze to simulate pretraining\n",
    "    for param in model_lora.parameters():\n",
    "        param.requires_grad = True\n",
    "    train_model(model_lora, X_task_a, y_task_a, epochs=100, lr=0.01)\n",
    "\n",
    "    # Now freeze the base weights (simulating LoRA setup)\n",
    "    for name, param in model_lora.named_parameters():\n",
    "        if 'weight' in name and 'lora' not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    acc_a_before_lora = evaluate_model(model_lora, X_task_a, y_task_a)\n",
    "    acc_b_before_lora = evaluate_model(model_lora, X_task_b, y_task_b)\n",
    "    print(f\"   Task A accuracy: {acc_a_before_lora:.1%}\")\n",
    "    print(f\"   Task B accuracy: {acc_b_before_lora:.1%} (not trained yet)\")\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model_lora.parameters())\n",
    "    print(f\"   Trainable parameters: {trainable_params:,} / {total_params:,} \"\n",
    "          f\"({100*trainable_params/total_params:.1f}%)\")\n",
    "\n",
    "    # Step 2: Fine-tune on Task B (only LoRA adapters)\n",
    "    print(\"\\n2. Fine-tuning on Task B with LoRA adapters...\")\n",
    "    train_model(model_lora, X_task_b, y_task_b, epochs=100, lr=0.01)\n",
    "    acc_a_after_lora = evaluate_model(model_lora, X_task_a, y_task_a)\n",
    "    acc_b_after_lora = evaluate_model(model_lora, X_task_b, y_task_b)\n",
    "    print(f\"   Task A accuracy: {acc_a_after_lora:.1%} \u2713 Preserved!\")\n",
    "    print(f\"   Task B accuracy: {acc_b_after_lora:.1%}\")\n",
    "\n",
    "    forgetting_lora = acc_a_before_lora - acc_a_after_lora\n",
    "    print(f\"\\n   \ud83d\udcc8 LoRA Forgetting: {forgetting_lora:.1%} (minimal!)\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Visualization\n",
    "    # ========================================================================\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Plot 1: Full Fine-Tuning Results\n",
    "    ax1 = axes[0]\n",
    "    tasks = ['Task A\\n(Original)', 'Task B\\n(New)']\n",
    "    before = [acc_a_before * 100, acc_b_before * 100]\n",
    "    after = [acc_a_after * 100, acc_b_after * 100]\n",
    "\n",
    "    x = np.arange(len(tasks))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax1.bar(x - width/2, before, width, label='After Task A Training',\n",
    "                    color='#2ecc71', alpha=0.8)\n",
    "    bars2 = ax1.bar(x + width/2, after, width, label='After Task B Fine-tuning',\n",
    "                    color='#e74c3c', alpha=0.8)\n",
    "\n",
    "    ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax1.set_title('Full Fine-Tuning\\n(Catastrophic Forgetting)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(tasks)\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim([0, 105])\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # Annotate the forgetting\n",
    "    ax1.annotate('', xy=(0+width/2, acc_a_after*100), xytext=(0-width/2, acc_a_before*100),\n",
    "                arrowprops=dict(arrowstyle='<->', color='red', lw=2))\n",
    "    ax1.text(0, (acc_a_before*100 + acc_a_after*100)/2,\n",
    "             f'Forgot\\n{forgetting*100:.0f}%',\n",
    "             ha='right', va='center', fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "    # Plot 2: LoRA Fine-Tuning Results\n",
    "    ax2 = axes[1]\n",
    "    before_lora = [acc_a_before_lora * 100, acc_b_before_lora * 100]\n",
    "    after_lora = [acc_a_after_lora * 100, acc_b_after_lora * 100]\n",
    "\n",
    "    bars3 = ax2.bar(x - width/2, before_lora, width, label='After Task A Training',\n",
    "                    color='#2ecc71', alpha=0.8)\n",
    "    bars4 = ax2.bar(x + width/2, after_lora, width, label='After Task B Fine-tuning',\n",
    "                    color='#3498db', alpha=0.8)\n",
    "\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.set_title('LoRA Fine-Tuning\\n(No Catastrophic Forgetting)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(tasks)\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim([0, 105])\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels\n",
    "    for bars in [bars3, bars4]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VIZ_DIR / \"02_catastrophic_forgetting_comparison.png\",\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n\u2713 Saved visualization: {VIZ_DIR / '02_catastrophic_forgetting_comparison.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "    # ========================================================================\n",
    "    # Summary Table\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY: Catastrophic Forgetting Comparison\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Method':<25} {'Task A (Original)':<20} {'Task B (New)':<20} {'Forgetting'}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Full Fine-Tuning':<25} {acc_a_after:<20.1%} {acc_b_after:<20.1%} {forgetting:>10.1%} \u26a0\ufe0f\")\n",
    "    print(f\"{'LoRA Fine-Tuning':<25} {acc_a_after_lora:<20.1%} {acc_b_after_lora:<20.1%} {forgetting_lora:>10.1%} \u2713\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\n\ud83d\udca1 KEY INTERVIEW INSIGHTS:\")\n",
    "    print(\"   1. Full fine-tuning causes catastrophic forgetting\")\n",
    "    print(\"   2. LoRA preserves original task performance\")\n",
    "    print(\"   3. LoRA learns new task with minimal parameters\")\n",
    "    print(\"   4. This is why LoRA is preferred for adapting LLMs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09419e3b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: Parameter Efficiency Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 5: Parameter Efficiency Analysis\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d07943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_parameter_efficiency():\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of parameter efficiency for different model scales.\n",
    "\n",
    "    Interview talking points:\n",
    "    - LoRA scales incredibly well to large models\n",
    "    - The larger the model, the more dramatic the savings\n",
    "    - Common ranks: 4-16 for most tasks, up to 64 for complex tasks\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing Parameter Efficiency Across Model Scales...\")\n",
    "\n",
    "    # Different model configurations (simulating real LLM architectures)\n",
    "    configs = [\n",
    "        (\"Small (BERT-base)\", 768, 12, 768),      # 12 layers, 768 dim\n",
    "        (\"Medium (BERT-large)\", 1024, 24, 1024),  # 24 layers, 1024 dim\n",
    "        (\"Large (GPT-2)\", 1280, 36, 1280),        # 36 layers, 1280 dim\n",
    "        (\"XL (GPT-3 Small)\", 2048, 24, 2048),     # 24 layers, 2048 dim\n",
    "        (\"XXL (GPT-3 Medium)\", 4096, 32, 4096),   # 32 layers, 4096 dim\n",
    "    ]\n",
    "\n",
    "    ranks = [4, 8, 16, 32]\n",
    "\n",
    "    # Calculate for each configuration\n",
    "    results = []\n",
    "    for name, hidden_dim, num_layers, ffn_dim in configs:\n",
    "        for rank in ranks:\n",
    "            # Typical transformer has 4 weight matrices per layer:\n",
    "            # - Q, K, V projections: 3 \u00d7 (hidden \u00d7 hidden)\n",
    "            # - Output projection: hidden \u00d7 hidden\n",
    "            # - FFN: 2 \u00d7 (hidden \u00d7 ffn)\n",
    "\n",
    "            # Full fine-tuning parameters\n",
    "            full_params = num_layers * (\n",
    "                4 * (hidden_dim * hidden_dim) +  # Attention\n",
    "                2 * (hidden_dim * ffn_dim)        # FFN\n",
    "            )\n",
    "\n",
    "            # LoRA parameters (only on attention for simplicity)\n",
    "            lora_params = num_layers * 4 * (\n",
    "                (hidden_dim * rank) + (rank * hidden_dim)\n",
    "            )\n",
    "\n",
    "            reduction = full_params / lora_params\n",
    "\n",
    "            results.append({\n",
    "                'name': name,\n",
    "                'rank': rank,\n",
    "                'full': full_params,\n",
    "                'lora': lora_params,\n",
    "                'reduction': reduction\n",
    "            })\n",
    "\n",
    "    # Print table\n",
    "    print(\"\\nParameter Efficiency Comparison:\")\n",
    "    print(\"=\"*110)\n",
    "    print(f\"{'Model':<25} {'Rank':<8} {'Full Params':<20} {'LoRA Params':<20} \"\n",
    "          f\"{'Reduction':<15} {'% Trainable'}\")\n",
    "    print(\"=\"*110)\n",
    "\n",
    "    for r in results:\n",
    "        pct = 100 * r['lora'] / r['full']\n",
    "        print(f\"{r['name']:<25} {r['rank']:<8} {r['full']:<20,} {r['lora']:<20,} \"\n",
    "              f\"{r['reduction']:<15.1f}x {pct:>6.2f}%\")\n",
    "\n",
    "        if r['rank'] == ranks[-1]:  # Add separator after each model\n",
    "            print(\"-\"*110)\n",
    "\n",
    "    print(\"=\"*110)\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Plot 1: Parameter counts by rank\n",
    "    ax1 = axes[0]\n",
    "    model_names = [r['name'] for r in results[::len(ranks)]]\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.2\n",
    "\n",
    "    for i, rank in enumerate(ranks):\n",
    "        lora_params = [r['lora'] for r in results if r['rank'] == rank]\n",
    "        ax1.bar(x + i*width, lora_params, width, label=f'Rank {rank}', alpha=0.8)\n",
    "\n",
    "    ax1.set_ylabel('LoRA Parameters (log scale)', fontsize=12)\n",
    "    ax1.set_xlabel('Model Size', fontsize=12)\n",
    "    ax1.set_title('LoRA Parameter Count by Model Size and Rank',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x + width * 1.5)\n",
    "    ax1.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Plot 2: Reduction factors\n",
    "    ax2 = axes[1]\n",
    "\n",
    "    for i, rank in enumerate(ranks):\n",
    "        reductions = [r['reduction'] for r in results if r['rank'] == rank]\n",
    "        ax2.plot(x, reductions, marker='o', linewidth=2, markersize=8,\n",
    "                label=f'Rank {rank}', alpha=0.8)\n",
    "\n",
    "    ax2.set_ylabel('Parameter Reduction Factor', fontsize=12)\n",
    "    ax2.set_xlabel('Model Size', fontsize=12)\n",
    "    ax2.set_title('Parameter Reduction Factor\\n(Higher is Better)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VIZ_DIR / \"03_parameter_efficiency_analysis.png\",\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n\u2713 Saved visualization: {VIZ_DIR / '03_parameter_efficiency_analysis.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\n\ud83d\udca1 KEY INTERVIEW INSIGHTS:\")\n",
    "    print(\"   1. Larger models benefit MORE from LoRA\")\n",
    "    print(\"   2. GPT-3 Medium with rank=8: 10,000x+ reduction!\")\n",
    "    print(\"   3. Can train on single GPU instead of cluster\")\n",
    "    print(\"   4. Faster training, less memory, same performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00280b",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 6: Interview Q&A Summary\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_interview_qa_summary():\n",
    "    \"\"\"Print comprehensive interview Q&A summary.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTERVIEW Q&A SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    qa_pairs = [\n",
    "        {\n",
    "            \"Q\": \"What is LoRA and how does it work?\",\n",
    "            \"A\": \"\"\"LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method:\n",
    "\n",
    "1. Core Idea: Instead of updating all weights W during fine-tuning,\n",
    "   keep W frozen and learn a low-rank decomposition of the update:\n",
    "\n",
    "   h = Wx + \u0394Wx = Wx + BAx\n",
    "\n",
    "   where B \u2208 R^(d\u00d7r), A \u2208 R^(r\u00d7k), and r << min(d,k)\n",
    "\n",
    "2. Why it works:\n",
    "   - Weight updates often have low intrinsic rank\n",
    "   - We need d\u00d7r + r\u00d7k params instead of d\u00d7k params\n",
    "   - For r=8, 768\u00d7768 layer: 12K params vs 590K params!\n",
    "\n",
    "3. Benefits:\n",
    "   - 10,000x fewer trainable parameters for large models\n",
    "   - No catastrophic forgetting (original weights frozen)\n",
    "   - Can swap adapters for different tasks\n",
    "   - No inference overhead (merge BA into W)\n",
    "\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"Q\": \"How does LoRA prevent catastrophic forgetting?\",\n",
    "            \"A\": \"\"\"LoRA prevents catastrophic forgetting through architectural design:\n",
    "\n",
    "1. Frozen Base Weights:\n",
    "   - Original pretrained weights W\u2080 remain unchanged\n",
    "   - Preserves all knowledge from pretraining\n",
    "   - Acts as a \"memory\" of the original task\n",
    "\n",
    "2. Additive Adapters:\n",
    "   - New task knowledge stored in BA matrices\n",
    "   - Added to (not replacing) original computation\n",
    "   - h = W\u2080x + BAx (both paths contribute)\n",
    "\n",
    "3. Experimental Evidence (from our demo):\n",
    "   - Full fine-tuning: ~30% accuracy loss on original task\n",
    "   - LoRA: <5% accuracy loss (essentially preserved!)\n",
    "\n",
    "4. Multiple Task Adaptation:\n",
    "   - Can train different (B_i, A_i) pairs for different tasks\n",
    "   - Switch adapters without touching base model\n",
    "   - Each task gets its own \"memory\"\n",
    "\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"Q\": \"What are the key hyperparameters in LoRA?\",\n",
    "            \"A\": \"\"\"Three critical hyperparameters:\n",
    "\n",
    "1. Rank (r):\n",
    "   - Controls the expressiveness vs efficiency trade-off\n",
    "   - Typical values: 4-16 (sometimes up to 64)\n",
    "   - Lower rank: fewer params, faster, might underfit\n",
    "   - Higher rank: more params, slower, might overfit\n",
    "   - Rule of thumb: start with 8\n",
    "\n",
    "2. Alpha (\u03b1):\n",
    "   - Scaling factor for the LoRA update\n",
    "   - Often set to rank value (\u03b1 = r)\n",
    "   - Controls relative importance: W\u2080x vs (\u03b1/r)\u00b7BAx\n",
    "   - Higher \u03b1: stronger adaptation, more forgetting risk\n",
    "\n",
    "3. Target Layers:\n",
    "   - Which layers to apply LoRA (Q, K, V, FFN?)\n",
    "   - Most common: only attention Q, V matrices\n",
    "   - More layers: better performance, more parameters\n",
    "   - Trade-off between efficiency and expressiveness\n",
    "\n",
    "Initialization matters:\n",
    "- A: Kaiming/He initialization (random)\n",
    "- B: Zero initialization (so \u0394W starts at zero)\n",
    "\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"Q\": \"Compare LoRA to other PEFT methods\",\n",
    "            \"A\": \"\"\"Parameter-Efficient Fine-Tuning (PEFT) Method Comparison:\n",
    "\n",
    "1. LoRA (Our focus):\n",
    "   \u2713 Very parameter efficient (0.01-0.1% of full model)\n",
    "   \u2713 No inference overhead (can merge weights)\n",
    "   \u2713 Easy to implement and swap adapters\n",
    "   \u2713 Works well across tasks\n",
    "\n",
    "2. Prefix Tuning:\n",
    "   - Prepends trainable \"virtual tokens\" to input\n",
    "   - More parameters than LoRA for same performance\n",
    "   - Inference overhead (longer sequences)\n",
    "   - Good for generation tasks\n",
    "\n",
    "3. Adapter Layers:\n",
    "   - Inserts small bottleneck layers between transformer layers\n",
    "   - More parameters than LoRA\n",
    "   - Inference overhead (extra forward passes)\n",
    "   - Very stable training\n",
    "\n",
    "4. Prompt Tuning:\n",
    "   - Only trains soft prompt embeddings\n",
    "   - Fewest parameters!\n",
    "   - But lower performance than LoRA\n",
    "   - Best for very large models (10B+ params)\n",
    "\n",
    "5. BitFit:\n",
    "   - Only trains bias terms\n",
    "   - Extremely simple\n",
    "   - Limited expressiveness\n",
    "\n",
    "Winner for most use cases: LoRA!\n",
    "Best balance of efficiency, performance, and flexibility.\n",
    "\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"Q\": \"When should you use LoRA vs full fine-tuning?\",\n",
    "            \"A\": \"\"\"Decision Framework:\n",
    "\n",
    "Use LoRA when:\n",
    "\u2713 Limited compute/memory (single GPU instead of cluster)\n",
    "\u2713 Want to preserve original model capabilities\n",
    "\u2713 Need to adapt to multiple tasks (swap adapters)\n",
    "\u2713 Dataset is small-to-medium sized\n",
    "\u2713 Task is similar to pretraining objective\n",
    "\u2713 Production deployment (can merge, no overhead)\n",
    "\n",
    "Use Full Fine-Tuning when:\n",
    "\u2717 Task is VERY different from pretraining\n",
    "\u2717 Have abundant compute resources\n",
    "\u2717 Dataset is very large and diverse\n",
    "\u2717 Need absolute maximum performance\n",
    "\u2717 Domain shift is extreme (e.g., medical \u2192 poetry)\n",
    "\n",
    "Practical Reality:\n",
    "- Start with LoRA (99% of the time it's sufficient)\n",
    "- Only do full fine-tuning if LoRA doesn't work\n",
    "- Most production LLMs use LoRA or similar PEFT\n",
    "- Even GPT-4 likely uses adapter-style approaches\n",
    "\n",
    "Cost Example:\n",
    "- Full fine-tune GPT-3: $100,000+ on cloud\n",
    "- LoRA fine-tune GPT-3: $100-1000 on single GPU\n",
    "\"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for i, qa in enumerate(qa_pairs, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Q{i}: {qa['Q']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(qa['A'])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Additional Resources for Interview Prep:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\"\"\n",
    "1. Original Paper: \"LoRA: Low-Rank Adaptation of Large Language Models\"\n",
    "   (Hu et al., 2021) - https://arxiv.org/abs/2106.09685\n",
    "\n",
    "2. Key Equation to Memorize:\n",
    "   h = W\u2080x + (\u03b1/r)\u00b7BAx\n",
    "   where W\u2080 is frozen, B\u2208R^(d\u00d7r), A\u2208R^(r\u00d7k), r<<min(d,k)\n",
    "\n",
    "3. Parameter Count Formula:\n",
    "   Full: d \u00d7 k\n",
    "   LoRA: (d \u00d7 r) + (r \u00d7 k) = r(d + k)\n",
    "   Reduction: (d \u00d7 k) / (r(d + k))\n",
    "\n",
    "4. Real-world LoRA Applications:\n",
    "   - Stable Diffusion fine-tuning (DreamBooth)\n",
    "   - ChatGPT task-specific adaptations\n",
    "   - Multi-tenant LLM serving (one base, many adapters)\n",
    "   - Personal AI assistants\n",
    "\n",
    "5. Interview Red Flags to Avoid:\n",
    "   \u2717 \"LoRA is just for saving memory\" (no! also prevents forgetting)\n",
    "   \u2717 \"LoRA always underperforms full fine-tuning\" (no! often matches it)\n",
    "   \u2717 \"You can't use LoRA for pre-training\" (correct, but explain why)\n",
    "   \u2717 \"LoRA requires special infrastructure\" (no! works with standard PyTorch)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66cba5e",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "MAIN EXECUTION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Starting LoRA Concept Demo\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Section 1: Low-rank approximation foundation\n",
    "    W_full = demonstrate_low_rank_approximation()\n",
    "\n",
    "    # Section 2: LoRA implementation\n",
    "    demonstrate_lora_layer()\n",
    "\n",
    "    # Section 3 & 4: Catastrophic forgetting demonstration\n",
    "    demonstrate_catastrophic_forgetting()\n",
    "\n",
    "    # Section 5: Parameter efficiency analysis\n",
    "    analyze_parameter_efficiency()\n",
    "\n",
    "    # Section 6: Interview Q&A summary\n",
    "    print_interview_qa_summary()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\u2713 LoRA Concept Demo Complete!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nVisualizations saved to: {VIZ_DIR}\")\n",
    "    print(\"\\nFiles created:\")\n",
    "    print(\"  1. 01_low_rank_approximation.png\")\n",
    "    print(\"  2. 02_catastrophic_forgetting_comparison.png\")\n",
    "    print(\"  3. 03_parameter_efficiency_analysis.png\")\n",
    "    print(\"\\nYou are now ready to ace LoRA questions in your LLM interview!\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}