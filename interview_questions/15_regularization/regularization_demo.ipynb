{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Regularization Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575cde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COMPREHENSIVE DROPOUT DEMONSTRATION FOR LLM INTERVIEWS\n",
    "======================================================\n",
    "\n",
    "Interview Question Q18: \"What is the role of dropout in training LLMs?\"\n",
    "\n",
    "KEY INTERVIEW POINTS:\n",
    "--------------------\n",
    "1. Dropout is a regularization technique that prevents overfitting\n",
    "2. During training: randomly zeros out activations with probability p\n",
    "3. During inference: all neurons are active (no dropout)\n",
    "4. Scaling: outputs are scaled by 1/(1-p) during training to maintain expected values\n",
    "5. In transformers: applied to attention scores, FFN outputs, and embeddings\n",
    "6. Typical rates: 0.1-0.3 for LLMs (lower than CNNs due to already high capacity)\n",
    "\n",
    "This demo provides:\n",
    "- Visualization of dropout behavior\n",
    "- Training comparison with/without dropout\n",
    "- Attention dropout implementation\n",
    "- Effect on different layers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d3c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1deb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44da0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR = \"/Users/zack/dev/ml-refresher/data/interview_viz\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DROPOUT DEMONSTRATION FOR LLM TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5273c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 1: DROPOUT BEHAVIOR VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 1: DROPOUT BEHAVIOR - TRAINING VS INFERENCE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "INTERVIEW INSIGHT:\n",
    "Dropout randomly zeros out neurons during training but is disabled during inference.\n",
    "This creates an ensemble effect where each mini-batch trains a different sub-network.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522af0c4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_dropout_behavior():\n",
    "    \"\"\"\n",
    "    Demonstrates how dropout masks activations differently between training and inference.\n",
    "\n",
    "    Interview Key Point: Dropout creates stochasticity during training but\n",
    "    deterministic behavior during inference.\n",
    "    \"\"\"\n",
    "    # Create a sample activation tensor (batch_size=1, features=100)\n",
    "    activations = torch.randn(1, 100) * 2 + 5  # Mean ~5, std ~2\n",
    "\n",
    "    # Different dropout rates to compare\n",
    "    dropout_rates = [0.0, 0.1, 0.3, 0.5]\n",
    "\n",
    "    fig, axes = plt.subplots(len(dropout_rates) + 1, 3, figsize=(15, 12))\n",
    "    fig.suptitle('Dropout Behavior: Training vs Inference', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Original activations\n",
    "    axes[0, 0].bar(range(100), activations[0].numpy(), color='blue', alpha=0.7)\n",
    "    axes[0, 0].set_title('Original Activations', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Activation Value')\n",
    "    axes[0, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[0, 0].set_ylim(-2, 12)\n",
    "    axes[0, 0].text(50, 10, f'Mean: {activations.mean():.2f}\\nStd: {activations.std():.2f}',\n",
    "                    ha='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    # Hide the other two plots in first row\n",
    "    axes[0, 1].axis('off')\n",
    "    axes[0, 2].axis('off')\n",
    "\n",
    "    for idx, p in enumerate(dropout_rates, start=1):\n",
    "        dropout_layer = nn.Dropout(p=p)\n",
    "\n",
    "        # TRAINING MODE - Apply dropout (stochastic)\n",
    "        dropout_layer.train()\n",
    "        train_output = dropout_layer(activations.clone())\n",
    "\n",
    "        # Show which neurons were dropped\n",
    "        mask = (train_output != 0).float()[0]\n",
    "\n",
    "        # INFERENCE MODE - No dropout (deterministic)\n",
    "        dropout_layer.eval()\n",
    "        eval_output = dropout_layer(activations.clone())\n",
    "\n",
    "        # Plot training mode\n",
    "        colors = ['red' if m == 0 else 'green' for m in mask]\n",
    "        axes[idx, 0].bar(range(100), train_output[0].numpy(), color=colors, alpha=0.7)\n",
    "        axes[idx, 0].set_title(f'Training Mode (p={p})', fontweight='bold')\n",
    "        axes[idx, 0].set_ylabel('Activation Value')\n",
    "        axes[idx, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "        axes[idx, 0].set_ylim(-2, 12)\n",
    "\n",
    "        # Calculate statistics\n",
    "        num_dropped = (mask == 0).sum().item()\n",
    "        active_mean = train_output[0][mask.bool()].mean() if mask.sum() > 0 else 0\n",
    "\n",
    "        axes[idx, 0].text(50, 10,\n",
    "                         f'Dropped: {num_dropped}/100 ({num_dropped}%)\\n'\n",
    "                         f'Active Mean: {active_mean:.2f}\\n'\n",
    "                         f'Scale Factor: {1/(1-p):.2f}',\n",
    "                         ha='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "        # Plot inference mode\n",
    "        axes[idx, 1].bar(range(100), eval_output[0].numpy(), color='blue', alpha=0.7)\n",
    "        axes[idx, 1].set_title(f'Inference Mode (p={p})', fontweight='bold')\n",
    "        axes[idx, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "        axes[idx, 1].set_ylim(-2, 12)\n",
    "        axes[idx, 1].text(50, 10,\n",
    "                         f'Mean: {eval_output.mean():.2f}\\n'\n",
    "                         f'All neurons active',\n",
    "                         ha='center', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "        # Plot dropout mask\n",
    "        axes[idx, 2].bar(range(100), mask.numpy(), color='black', alpha=0.7)\n",
    "        axes[idx, 2].set_title(f'Dropout Mask (p={p})', fontweight='bold')\n",
    "        axes[idx, 2].set_ylim(-0.1, 1.1)\n",
    "        axes[idx, 2].set_yticks([0, 1])\n",
    "        axes[idx, 2].set_yticklabels(['Dropped', 'Active'])\n",
    "        axes[idx, 2].text(50, 0.5, f'{int(mask.sum())}/100 active',\n",
    "                         ha='center', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "    # Set x-labels only on bottom row\n",
    "    for ax in axes[-1, :]:\n",
    "        ax.set_xlabel('Neuron Index')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/08_dropout_behavior.png\", dpi=300, bbox_inches='tight')\n",
    "    print(f\"\u2713 Saved: 08_dropout_behavior.png\")\n",
    "    print()\n",
    "    print(\"INTERVIEW EXPLANATION:\")\n",
    "    print(\"- Red bars: Neurons that were dropped (set to 0)\")\n",
    "    print(\"- Green bars: Active neurons (scaled up by 1/(1-p))\")\n",
    "    print(\"- Inference mode: All neurons active, no randomness\")\n",
    "    print(\"- Higher dropout rate = more neurons dropped = stronger regularization\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c2cdb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "visualize_dropout_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb395b8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 2: TRAINING COMPARISON - WITH VS WITHOUT DROPOUT\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: TRAINING COMPARISON - EFFECT ON OVERFITTING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "INTERVIEW INSIGHT:\n",
    "Dropout reduces overfitting by preventing co-adaptation of neurons.\n",
    "Networks learn more robust features that don't rely on specific neurons.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd36205",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple 3-layer network for demonstrating dropout effects.\n",
    "\n",
    "    Interview Note: This architecture mimics a simplified transformer FFN block.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, output_dim=2, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer + activation + dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second layer + activation + dropout\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Output layer (no dropout after final layer)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4b63f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=500, n_features=20, noise_level=0.5):\n",
    "    \"\"\"\n",
    "    Generate synthetic binary classification data.\n",
    "    Deliberately make it small to encourage overfitting without dropout.\n",
    "    \"\"\"\n",
    "    # Generate features\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "\n",
    "    # Create a complex decision boundary (non-linear)\n",
    "    # This mimics the complexity of language modeling tasks\n",
    "    weights = torch.randn(n_features)\n",
    "    logits = X @ weights + torch.sin(X[:, 0]) * 2 + X[:, 1] ** 2\n",
    "\n",
    "    # Add noise to make it harder\n",
    "    logits += torch.randn(n_samples) * noise_level\n",
    "\n",
    "    # Binary labels\n",
    "    y = (logits > logits.median()).long()\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5afb5e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_X, train_y, val_X, val_y, epochs=100):\n",
    "    \"\"\"\n",
    "    Train a model and track training/validation metrics.\n",
    "\n",
    "    Interview Note: We track both losses to demonstrate overfitting behavior.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_output = model(train_X)\n",
    "        train_loss = criterion(train_output, train_y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        train_pred = train_output.argmax(dim=1)\n",
    "        train_acc = (train_pred == train_y).float().mean()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(val_X)\n",
    "            val_loss = criterion(val_output, val_y)\n",
    "            val_pred = val_output.argmax(dim=1)\n",
    "            val_acc = (val_pred == val_y).float().mean()\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        train_accs.append(train_acc.item())\n",
    "        val_accs.append(val_acc.item())\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e57c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "print(\"Generating synthetic classification data...\")\n",
    "train_X, train_y = generate_synthetic_data(n_samples=200, noise_level=0.3)\n",
    "val_X, val_y = generate_synthetic_data(n_samples=100, noise_level=0.3)\n",
    "print(f\"Training samples: {len(train_X)}, Validation samples: {len(val_X)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different dropout rates\n",
    "dropout_rates = [0.0, 0.1, 0.3, 0.5]\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65874b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rate in dropout_rates:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training model with dropout rate = {rate}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    model = SimpleClassifier(dropout_rate=rate)\n",
    "    train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "        model, train_X, train_y, val_X, val_y, epochs=100\n",
    "    )\n",
    "\n",
    "    results[rate] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'final_gap': train_accs[-1] - val_accs[-1]  # Overfitting indicator\n",
    "    }\n",
    "\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"  Train Accuracy: {train_accs[-1]:.4f}\")\n",
    "    print(f\"  Val Accuracy:   {val_accs[-1]:.4f}\")\n",
    "    print(f\"  Accuracy Gap:   {results[rate]['final_gap']:.4f} (lower = less overfitting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Dropout Effect on Training Dynamics', fontsize=16, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'orange', 'green', 'blue']\n",
    "labels = [f'Dropout={rate}' for rate in dropout_rates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Training Loss\n",
    "ax = axes[0, 0]\n",
    "for idx, rate in enumerate(dropout_rates):\n",
    "    ax.plot(results[rate]['train_losses'], color=colors[idx], label=labels[idx], linewidth=2)\n",
    "ax.set_title('Training Loss', fontweight='bold', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fb8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Validation Loss\n",
    "ax = axes[0, 1]\n",
    "for idx, rate in enumerate(dropout_rates):\n",
    "    ax.plot(results[rate]['val_losses'], color=colors[idx], label=labels[idx], linewidth=2)\n",
    "ax.set_title('Validation Loss', fontweight='bold', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21605b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Training Accuracy\n",
    "ax = axes[1, 0]\n",
    "for idx, rate in enumerate(dropout_rates):\n",
    "    ax.plot(results[rate]['train_accs'], color=colors[idx], label=labels[idx], linewidth=2)\n",
    "ax.set_title('Training Accuracy', fontweight='bold', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081383c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Validation Accuracy\n",
    "ax = axes[1, 1]\n",
    "for idx, rate in enumerate(dropout_rates):\n",
    "    ax.plot(results[rate]['val_accs'], color=colors[idx], label=labels[idx], linewidth=2)\n",
    "ax.set_title('Validation Accuracy', fontweight='bold', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b002b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/09_dropout_training_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved: 09_dropout_training_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90104c1c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Print overfitting analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERFITTING ANALYSIS (Train-Val Accuracy Gap)\")\n",
    "print(\"=\"*80)\n",
    "for rate in dropout_rates:\n",
    "    gap = results[rate]['final_gap']\n",
    "    print(f\"Dropout {rate}: {gap:+.4f} {'\u2b06\ufe0f MORE OVERFITTING' if gap > 0.05 else '\u2713 LESS OVERFITTING'}\")\n",
    "print()\n",
    "print(\"INTERVIEW KEY POINT:\")\n",
    "print(\"Lower accuracy gap indicates better generalization.\")\n",
    "print(\"Dropout reduces overfitting but may slightly lower training accuracy.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05802b8c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3: ATTENTION DROPOUT IMPLEMENTATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: ATTENTION DROPOUT IN TRANSFORMERS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "INTERVIEW INSIGHT:\n",
    "In transformers, dropout is applied to:\n",
    "1. Attention weights (after softmax)\n",
    "2. Attention output (after value multiplication)\n",
    "3. Feed-forward network outputs\n",
    "4. Embeddings (input and positional)\n",
    "\n",
    "This prevents the model from relying too heavily on specific attention patterns.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee238693",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class AttentionWithDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified multi-head attention with dropout.\n",
    "\n",
    "    Interview Note: This shows where dropout is typically applied in transformer blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=64, n_heads=4, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.attn_dropout = nn.Dropout(p=dropout_rate)  # Applied to attention weights\n",
    "        self.output_dropout = nn.Dropout(p=dropout_rate)  # Applied to output\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "        - output: (batch_size, seq_len, d_model)\n",
    "        - attention_weights: (batch_size, n_heads, seq_len, seq_len) if return_attention=True\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Linear projections and reshape for multi-head attention\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        # scores: (batch_size, n_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "\n",
    "        # Softmax to get attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # *** CRITICAL: Dropout on attention weights ***\n",
    "        # This is where dropout is applied in attention mechanism\n",
    "        attn_weights_dropped = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Apply attention to values\n",
    "        # output: (batch_size, n_heads, seq_len, d_k)\n",
    "        output = torch.matmul(attn_weights_dropped, V)\n",
    "\n",
    "        # Concatenate heads\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        # *** CRITICAL: Dropout on output ***\n",
    "        output = self.output_dropout(output)\n",
    "\n",
    "        if return_attention:\n",
    "            return output, attn_weights, attn_weights_dropped\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4db4a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_attention_dropout():\n",
    "    \"\"\"\n",
    "    Visualize how dropout affects attention patterns.\n",
    "\n",
    "    Interview Key Point: Dropout in attention prevents over-reliance on specific tokens.\n",
    "    \"\"\"\n",
    "    # Create sample input (batch_size=1, seq_len=10, d_model=64)\n",
    "    seq_len = 10\n",
    "    x = torch.randn(1, seq_len, 64)\n",
    "\n",
    "    # Create attention modules with different dropout rates\n",
    "    dropout_rates = [0.0, 0.3, 0.5]\n",
    "\n",
    "    fig, axes = plt.subplots(len(dropout_rates), 4, figsize=(20, 12))\n",
    "    fig.suptitle('Attention Dropout Visualization', fontsize=16, fontweight='bold')\n",
    "\n",
    "    for idx, dropout_rate in enumerate(dropout_rates):\n",
    "        attn_module = AttentionWithDropout(d_model=64, n_heads=4, dropout_rate=dropout_rate)\n",
    "        attn_module.train()  # Enable dropout\n",
    "\n",
    "        # Get attention weights (both original and dropped)\n",
    "        _, attn_orig, attn_dropped = attn_module(x, return_attention=True)\n",
    "\n",
    "        # attn_orig: (1, 4, 10, 10) - 4 heads\n",
    "        # Let's visualize head 0\n",
    "        head_idx = 0\n",
    "        attn_orig_head = attn_orig[0, head_idx].detach().numpy()\n",
    "        attn_dropped_head = attn_dropped[0, head_idx].detach().numpy()\n",
    "\n",
    "        # Calculate dropout mask\n",
    "        dropout_mask = (attn_dropped_head != 0).astype(float)\n",
    "\n",
    "        # Plot original attention\n",
    "        im1 = axes[idx, 0].imshow(attn_orig_head, cmap='viridis', aspect='auto', vmin=0, vmax=0.3)\n",
    "        axes[idx, 0].set_title(f'Original Attention (dropout={dropout_rate})', fontweight='bold')\n",
    "        axes[idx, 0].set_xlabel('Key Position')\n",
    "        axes[idx, 0].set_ylabel('Query Position')\n",
    "        plt.colorbar(im1, ax=axes[idx, 0], fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Plot attention after dropout\n",
    "        im2 = axes[idx, 1].imshow(attn_dropped_head, cmap='viridis', aspect='auto', vmin=0, vmax=0.3)\n",
    "        axes[idx, 1].set_title(f'After Dropout (dropout={dropout_rate})', fontweight='bold')\n",
    "        axes[idx, 1].set_xlabel('Key Position')\n",
    "        axes[idx, 1].set_ylabel('Query Position')\n",
    "        plt.colorbar(im2, ax=axes[idx, 1], fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Plot dropout mask\n",
    "        im3 = axes[idx, 2].imshow(dropout_mask, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "        axes[idx, 2].set_title(f'Dropout Mask (dropout={dropout_rate})', fontweight='bold')\n",
    "        axes[idx, 2].set_xlabel('Key Position')\n",
    "        axes[idx, 2].set_ylabel('Query Position')\n",
    "        plt.colorbar(im3, ax=axes[idx, 2], fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Plot difference (what was removed)\n",
    "        difference = attn_orig_head - attn_dropped_head\n",
    "        im4 = axes[idx, 3].imshow(difference, cmap='Reds', aspect='auto', vmin=0, vmax=0.3)\n",
    "        axes[idx, 3].set_title(f'Dropped Values (dropout={dropout_rate})', fontweight='bold')\n",
    "        axes[idx, 3].set_xlabel('Key Position')\n",
    "        axes[idx, 3].set_ylabel('Query Position')\n",
    "        plt.colorbar(im4, ax=axes[idx, 3], fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Print statistics\n",
    "        num_zeros = (dropout_mask == 0).sum()\n",
    "        total = dropout_mask.size\n",
    "        print(f\"Dropout {dropout_rate}: {num_zeros}/{total} attention scores dropped \"\n",
    "              f\"({100*num_zeros/total:.1f}%)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/10_attention_dropout.png\", dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n\u2713 Saved: 10_attention_dropout.png\")\n",
    "    print()\n",
    "    print(\"INTERVIEW EXPLANATION:\")\n",
    "    print(\"- Original Attention: The attention pattern before dropout\")\n",
    "    print(\"- After Dropout: Some attention scores are zeroed out\")\n",
    "    print(\"- Dropout Mask: Shows which attention scores were kept (green) vs dropped (red)\")\n",
    "    print(\"- Dropped Values: Visualizes what information was removed\")\n",
    "    print()\n",
    "    print(\"WHY THIS MATTERS:\")\n",
    "    print(\"- Prevents model from always attending to the same tokens\")\n",
    "    print(\"- Forces learning of diverse attention patterns\")\n",
    "    print(\"- Creates ensemble effect across different dropout masks\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007cfdf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "visualize_attention_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0af101",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 4: LAYER-WISE DROPOUT DEMONSTRATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 4: LAYER-WISE DROPOUT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "INTERVIEW INSIGHT:\n",
    "Different layers can use different dropout rates. In LLMs:\n",
    "- Lower layers (closer to input): Often use higher dropout\n",
    "- Middle layers: Moderate dropout\n",
    "- Upper layers (closer to output): Sometimes lower or no dropout\n",
    "- This is task-dependent and often found through hyperparameter tuning\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a20010",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LayerwiseDropoutNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Network with different dropout rates at different layers.\n",
    "\n",
    "    Interview Note: Mimics how transformers might use varying dropout across layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, output_dim=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer 1 - High dropout (near input, more noise)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Layer 2 - Medium dropout\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "\n",
    "        # Layer 3 - Low dropout\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout3 = nn.Dropout(p=0.1)\n",
    "\n",
    "        # Output layer - No dropout\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, return_intermediates=False):\n",
    "        intermediates = {}\n",
    "\n",
    "        # Layer 1\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h1_dropped = self.dropout1(h1)\n",
    "        if return_intermediates:\n",
    "            intermediates['layer1'] = (h1, h1_dropped)\n",
    "\n",
    "        # Layer 2\n",
    "        h2 = F.relu(self.fc2(h1_dropped))\n",
    "        h2_dropped = self.dropout2(h2)\n",
    "        if return_intermediates:\n",
    "            intermediates['layer2'] = (h2, h2_dropped)\n",
    "\n",
    "        # Layer 3\n",
    "        h3 = F.relu(self.fc3(h2_dropped))\n",
    "        h3_dropped = self.dropout3(h3)\n",
    "        if return_intermediates:\n",
    "            intermediates['layer3'] = (h3, h3_dropped)\n",
    "\n",
    "        # Output\n",
    "        output = self.fc_out(h3_dropped)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return output, intermediates\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bfc94",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_layerwise_dropout():\n",
    "    \"\"\"Analyze how dropout affects activations at different layers.\"\"\"\n",
    "    model = LayerwiseDropoutNetwork()\n",
    "    model.train()  # Enable dropout\n",
    "\n",
    "    # Create sample input\n",
    "    x = torch.randn(1, 20)\n",
    "\n",
    "    # Forward pass with intermediate values\n",
    "    output, intermediates = model(x, return_intermediates=True)\n",
    "\n",
    "    print(\"Layer-wise Dropout Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    layer_names = ['layer1', 'layer2', 'layer3']\n",
    "    dropout_rates = [0.5, 0.3, 0.1]\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    fig.suptitle('Layer-wise Dropout Effects', fontsize=16, fontweight='bold')\n",
    "\n",
    "    for idx, (layer_name, dropout_rate) in enumerate(zip(layer_names, dropout_rates)):\n",
    "        h_before, h_after = intermediates[layer_name]\n",
    "\n",
    "        # Convert to numpy\n",
    "        before = h_before[0].detach().numpy()\n",
    "        after = h_after[0].detach().numpy()\n",
    "\n",
    "        # Calculate statistics\n",
    "        num_neurons = len(before)\n",
    "        num_dropped = (after == 0).sum()\n",
    "        percent_dropped = 100 * num_dropped / num_neurons\n",
    "\n",
    "        print(f\"\\n{layer_name.upper()} (dropout={dropout_rate}):\")\n",
    "        print(f\"  Neurons dropped: {num_dropped}/{num_neurons} ({percent_dropped:.1f}%)\")\n",
    "        print(f\"  Mean before: {before.mean():.4f}, Std before: {before.std():.4f}\")\n",
    "        print(f\"  Mean after:  {after.mean():.4f}, Std after:  {after.std():.4f}\")\n",
    "\n",
    "        # Plot 1: Activations before dropout\n",
    "        axes[idx, 0].bar(range(num_neurons), before, color='blue', alpha=0.7)\n",
    "        axes[idx, 0].set_title(f'{layer_name}: Before Dropout (p={dropout_rate})', fontweight='bold')\n",
    "        axes[idx, 0].set_ylabel('Activation')\n",
    "        axes[idx, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "        # Plot 2: Activations after dropout\n",
    "        colors = ['red' if a == 0 else 'green' for a in after]\n",
    "        axes[idx, 1].bar(range(num_neurons), after, color=colors, alpha=0.7)\n",
    "        axes[idx, 1].set_title(f'{layer_name}: After Dropout (p={dropout_rate})', fontweight='bold')\n",
    "        axes[idx, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "        # Plot 3: Dropout mask\n",
    "        mask = (after != 0).astype(float)\n",
    "        axes[idx, 2].bar(range(num_neurons), mask, color='black', alpha=0.7)\n",
    "        axes[idx, 2].set_title(f'{layer_name}: Dropout Mask', fontweight='bold')\n",
    "        axes[idx, 2].set_ylim(-0.1, 1.1)\n",
    "        axes[idx, 2].set_yticks([0, 1])\n",
    "        axes[idx, 2].set_yticklabels(['Dropped', 'Active'])\n",
    "\n",
    "    # Set x-labels\n",
    "    for ax in axes[-1, :]:\n",
    "        ax.set_xlabel('Neuron Index')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/11_layerwise_dropout.png\", dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n\u2713 Saved: 11_layerwise_dropout.png\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a1980",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "analyze_layerwise_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b5cd2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 5: DROPOUT SCALING DEMONSTRATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 5: DROPOUT SCALING BEHAVIOR\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "INTERVIEW INSIGHT:\n",
    "During training, PyTorch automatically scales activations by 1/(1-p) to maintain\n",
    "expected values. This is called \"inverted dropout\" and ensures that the scale\n",
    "of activations is consistent between training and inference.\n",
    "\n",
    "Mathematical intuition:\n",
    "- If dropout rate is 0.5, half the neurons are dropped on average\n",
    "- Remaining neurons are scaled by 1/(1-0.5) = 2\n",
    "- Expected value of output stays the same as without dropout\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8b0e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demonstrate_dropout_scaling():\n",
    "    \"\"\"Show how dropout scaling maintains expected values.\"\"\"\n",
    "\n",
    "    # Create a tensor with known mean\n",
    "    x = torch.ones(1000) * 10.0  # All values are 10.0\n",
    "    print(f\"Original tensor mean: {x.mean():.4f}\")\n",
    "    print()\n",
    "\n",
    "    dropout_rates = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    n_trials = 100\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for p in dropout_rates:\n",
    "        dropout = nn.Dropout(p=p)\n",
    "        dropout.train()  # Enable dropout\n",
    "\n",
    "        means = []\n",
    "        num_zeros_list = []\n",
    "\n",
    "        # Run multiple trials to get average behavior\n",
    "        for _ in range(n_trials):\n",
    "            output = dropout(x.clone())\n",
    "            means.append(output.mean().item())\n",
    "            num_zeros_list.append((output == 0).sum().item())\n",
    "\n",
    "        avg_mean = np.mean(means)\n",
    "        avg_zeros = np.mean(num_zeros_list)\n",
    "        theoretical_zeros = p * len(x)\n",
    "        scale_factor = 1 / (1 - p) if p < 1.0 else float('inf')\n",
    "\n",
    "        results.append({\n",
    "            'dropout_rate': p,\n",
    "            'avg_mean': avg_mean,\n",
    "            'avg_zeros': avg_zeros,\n",
    "            'theoretical_zeros': theoretical_zeros,\n",
    "            'scale_factor': scale_factor\n",
    "        })\n",
    "\n",
    "        print(f\"Dropout rate: {p:.1f}\")\n",
    "        print(f\"  Average output mean: {avg_mean:.4f} (should be ~10.0)\")\n",
    "        print(f\"  Average zeros: {avg_zeros:.1f} / {len(x)} ({100*avg_zeros/len(x):.1f}%)\")\n",
    "        print(f\"  Theoretical zeros: {theoretical_zeros:.1f} ({100*p:.1f}%)\")\n",
    "        print(f\"  Scale factor: {scale_factor:.4f}\")\n",
    "        print()\n",
    "\n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('Dropout Scaling Maintains Expected Values', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Mean values across dropout rates\n",
    "    dropout_vals = [r['dropout_rate'] for r in results]\n",
    "    means = [r['avg_mean'] for r in results]\n",
    "\n",
    "    axes[0].plot(dropout_vals, means, 'o-', linewidth=2, markersize=8, color='blue', label='Actual Mean')\n",
    "    axes[0].axhline(y=10.0, color='red', linestyle='--', linewidth=2, label='Expected Mean (10.0)')\n",
    "    axes[0].set_xlabel('Dropout Rate', fontweight='bold')\n",
    "    axes[0].set_ylabel('Output Mean', fontweight='bold')\n",
    "    axes[0].set_title('Mean Preservation with Dropout Scaling', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim([9, 11])\n",
    "\n",
    "    # Plot 2: Scale factors\n",
    "    scale_factors = [r['scale_factor'] for r in results if r['scale_factor'] != float('inf')]\n",
    "    dropout_vals_finite = [r['dropout_rate'] for r in results if r['scale_factor'] != float('inf')]\n",
    "\n",
    "    axes[1].plot(dropout_vals_finite, scale_factors, 'o-', linewidth=2, markersize=8, color='green')\n",
    "    axes[1].set_xlabel('Dropout Rate', fontweight='bold')\n",
    "    axes[1].set_ylabel('Scale Factor (1 / (1-p))', fontweight='bold')\n",
    "    axes[1].set_title('Dropout Scale Factor vs Dropout Rate', fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add annotations\n",
    "    for i, (p, sf) in enumerate(zip(dropout_vals_finite, scale_factors)):\n",
    "        if i % 2 == 0:  # Annotate every other point to avoid crowding\n",
    "            axes[1].annotate(f'{sf:.2f}', (p, sf), textcoords=\"offset points\",\n",
    "                           xytext=(0,10), ha='center', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/12_dropout_scaling.png\", dpi=300, bbox_inches='tight')\n",
    "    print(f\"\u2713 Saved: 12_dropout_scaling.png\")\n",
    "    print()\n",
    "    print(\"INTERVIEW KEY POINT:\")\n",
    "    print(\"Despite dropping neurons, the expected value remains constant due to scaling.\")\n",
    "    print(\"This is why dropout doesn't require changing learning rates or other hyperparameters.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad39f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "demonstrate_dropout_scaling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY FOR INTERVIEWS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERVIEW SUMMARY: KEY POINTS ABOUT DROPOUT IN LLMS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. WHAT IS DROPOUT?\n",
    "   - Regularization technique that randomly zeros out neurons during training\n",
    "   - Rate 'p' determines probability of dropping a neuron\n",
    "   - Disabled during inference (all neurons active)\n",
    "\n",
    "2. WHY USE DROPOUT IN LLMS?\n",
    "   - Prevents overfitting by reducing co-adaptation of neurons\n",
    "   - Creates ensemble effect (each batch trains a different sub-network)\n",
    "   - Forces network to learn robust, distributed representations\n",
    "   - Particularly important with large models and limited data\n",
    "\n",
    "3. WHERE IS DROPOUT APPLIED IN TRANSFORMERS?\n",
    "   - Attention weights (after softmax)\n",
    "   - Attention output projections\n",
    "   - Feed-forward network outputs\n",
    "   - Embedding layers (both token and positional)\n",
    "   - Some architectures use layer-specific dropout rates\n",
    "\n",
    "4. TYPICAL DROPOUT RATES FOR LLMS:\n",
    "   - 0.1 - 0.3 is common (lower than CNNs)\n",
    "   - Lower rates because transformers already have high capacity\n",
    "   - Too high dropout can hurt performance\n",
    "   - Often different rates for different components\n",
    "\n",
    "5. TECHNICAL DETAILS:\n",
    "   - Inverted dropout: scales by 1/(1-p) during training\n",
    "   - Maintains expected values between train/inference\n",
    "   - Implemented efficiently in modern frameworks\n",
    "   - No additional computation during inference\n",
    "\n",
    "6. TRADE-OFFS:\n",
    "   \u2713 Reduces overfitting\n",
    "   \u2713 Improves generalization\n",
    "   \u2713 Adds regularization without changing architecture\n",
    "   \u2717 Slower training (need more epochs)\n",
    "   \u2717 Can hurt performance if rate is too high\n",
    "   \u2717 Increases training variance\n",
    "\n",
    "7. ALTERNATIVES AND COMPLEMENTS:\n",
    "   - Layer normalization (used together with dropout)\n",
    "   - Weight decay / L2 regularization\n",
    "   - Data augmentation\n",
    "   - Early stopping\n",
    "   - DropConnect (drops connections instead of neurons)\n",
    "\n",
    "8. MODERN TRENDS:\n",
    "   - Some recent LLMs use less dropout or none at all\n",
    "   - Pre-training often uses less dropout than fine-tuning\n",
    "   - Task-dependent: more dropout for smaller datasets\n",
    "   - Architecture-dependent: some designs are naturally regularized\n",
    "\n",
    "Files generated:\n",
    "- 08_dropout_behavior.png: Training vs inference dropout behavior\n",
    "- 09_dropout_training_comparison.png: Effect on overfitting\n",
    "- 10_attention_dropout.png: Attention mechanism dropout visualization\n",
    "- 11_layerwise_dropout.png: Different dropout rates per layer\n",
    "- 12_dropout_scaling.png: How scaling maintains expected values\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968476ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Demo completed successfully!\")\n",
    "print(f\"All visualizations saved to: {OUTPUT_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}