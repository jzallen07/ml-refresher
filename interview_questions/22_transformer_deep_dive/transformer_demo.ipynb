{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03582c6",
   "metadata": {},
   "source": [
    "---\n",
    "title: Transformer Deep Dive Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc3589",
   "metadata": {},
   "source": [
    "# Transformer Architecture Deep Dive\n",
    "\n",
    "This demo visualizes the core concepts of the Transformer architecture\n",
    "that are commonly tested in ML/LLM interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fde02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1a2cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRANSFORMER ARCHITECTURE DEEP DIVE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75615fae",
   "metadata": {},
   "source": [
    "## 1. Scaled Dot-Product Attention\n",
    "\n",
    "The core attention formula:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        query: (batch, seq_len, d_k)\n",
    "        key: (batch, seq_len, d_k)\n",
    "        value: (batch, seq_len, d_v)\n",
    "        mask: optional mask for decoder self-attention\n",
    "\n",
    "    Returns:\n",
    "        attention_output: (batch, seq_len, d_v)\n",
    "        attention_weights: (batch, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    # Step 1: Compute attention scores (QK^T)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "    # Step 2: Scale by sqrt(d_k) to prevent vanishing gradients\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "\n",
    "    # Step 3: Apply mask if provided (for decoder)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Step 4: Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Step 5: Weighted sum of values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Demo: Simple attention computation\n",
    "print(\"\\n1. SCALED DOT-PRODUCT ATTENTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create sample Q, K, V\n",
    "batch_size, seq_len, d_k = 1, 4, 8\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Query shape: {Q.shape}\")\n",
    "print(f\"Key shape: {K.shape}\")\n",
    "print(f\"Value shape: {V.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"\\nAttention weights (each row sums to 1):\")\n",
    "print(weights[0].numpy().round(3))\n",
    "print(f\"Row sums: {weights[0].sum(dim=-1).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278df43",
   "metadata": {},
   "source": [
    "## 2. Why Scale by \u221ad_k?\n",
    "\n",
    "Without scaling, large d_k causes dot products to have large magnitude,\n",
    "pushing softmax into regions with extremely small gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6537a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. WHY SCALE BY \u221ad_k?\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Compare scaled vs unscaled attention\n",
    "d_k_large = 512\n",
    "Q_large = torch.randn(1, 4, d_k_large)\n",
    "K_large = torch.randn(1, 4, d_k_large)\n",
    "\n",
    "# Unscaled scores\n",
    "unscaled_scores = torch.matmul(Q_large, K_large.transpose(-2, -1))\n",
    "# Scaled scores\n",
    "scaled_scores = unscaled_scores / math.sqrt(d_k_large)\n",
    "\n",
    "print(f\"d_k = {d_k_large}\")\n",
    "print(f\"Unscaled score magnitude: {unscaled_scores.abs().mean():.2f}\")\n",
    "print(f\"Scaled score magnitude: {scaled_scores.abs().mean():.2f}\")\n",
    "print(f\"\\nUnscaled softmax (saturated - bad gradients):\")\n",
    "print(F.softmax(unscaled_scores[0], dim=-1).numpy().round(4))\n",
    "print(f\"\\nScaled softmax (distributed - good gradients):\")\n",
    "print(F.softmax(scaled_scores[0], dim=-1).numpy().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b68199",
   "metadata": {},
   "source": [
    "## 3. O(n\u00b2) Complexity Visualization\n",
    "\n",
    "Self-attention computes all pairwise relationships, leading to\n",
    "quadratic scaling with sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04fb43",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n3. O(n\u00b2) COMPLEXITY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "seq_lengths = [128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "computations = [n * n for n in seq_lengths]\n",
    "\n",
    "print(\"Sequence Length | Attention Computations | Memory (approx)\")\n",
    "print(\"-\" * 60)\n",
    "for n, c in zip(seq_lengths, computations):\n",
    "    # Assuming float32 (4 bytes) for attention matrix\n",
    "    memory_mb = (c * 4) / (1024 * 1024)\n",
    "    print(f\"{n:>14} | {c:>22,} | {memory_mb:>10.2f} MB\")\n",
    "\n",
    "print(\"\\nThis is why context windows have limits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4ea77",
   "metadata": {},
   "source": [
    "## 4. Positional Encodings\n",
    "\n",
    "Transformers are permutation-invariant, so positional encodings\n",
    "inject sequence order information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ab26d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sinusoidal_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Generate sinusoidal positional encodings.\n",
    "\n",
    "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    position = torch.arange(seq_len).unsqueeze(1)  # (seq_len, 1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "\n",
    "    return pe\n",
    "\n",
    "\n",
    "print(\"\\n4. POSITIONAL ENCODINGS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Generate positional encodings\n",
    "seq_len, d_model = 100, 64\n",
    "pe = sinusoidal_positional_encoding(seq_len, d_model)\n",
    "\n",
    "print(f\"Positional encoding shape: {pe.shape}\")\n",
    "print(f\"Position 0, first 8 dims: {pe[0, :8].numpy().round(3)}\")\n",
    "print(f\"Position 1, first 8 dims: {pe[1, :8].numpy().round(3)}\")\n",
    "\n",
    "# Show that relative positions can be computed\n",
    "print(f\"\\nRelative position property:\")\n",
    "print(\"PE(pos+k) can be expressed as a linear function of PE(pos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e93ff",
   "metadata": {},
   "source": [
    "## 5. Multi-Head Attention\n",
    "\n",
    "Multiple heads learn different types of relationships:\n",
    "syntactic, semantic, coreference, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993482c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention as described in 'Attention Is All You Need'.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Compute attention for all heads in parallel\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Concatenate heads and project\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "print(\"\\n5. MULTI-HEAD ATTENTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create multi-head attention\n",
    "d_model, num_heads = 64, 8\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Sample input\n",
    "x = torch.randn(1, 10, d_model)  # batch=1, seq_len=10, d_model=64\n",
    "output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  - {num_heads} heads, each attending over {attn_weights.shape[-1]} positions\")\n",
    "\n",
    "# Show head diversity\n",
    "print(f\"\\nHead diversity (attention entropy per head):\")\n",
    "for h in range(num_heads):\n",
    "    head_weights = attn_weights[0, h]\n",
    "    entropy = -(head_weights * torch.log(head_weights + 1e-9)).sum(dim=-1).mean()\n",
    "    print(f\"  Head {h}: entropy = {entropy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f91ca",
   "metadata": {},
   "source": [
    "## 6. Residual Connections & Layer Norm\n",
    "\n",
    "These enable training of very deep networks (100+ layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single Transformer encoder block with residuals and layer norm.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_out))  # Residual + LayerNorm\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))  # Residual + LayerNorm\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"\\n6. RESIDUAL CONNECTIONS & LAYER NORM\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a transformer block\n",
    "block = TransformerBlock(d_model=64, num_heads=8, d_ff=256)\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(1, 10, 64)\n",
    "output = block(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Show gradient flow with residuals\n",
    "print(f\"\\nWhy residuals matter:\")\n",
    "print(\"  - Without residuals: gradients vanish in deep networks\")\n",
    "print(\"  - With residuals: gradient = 1 + other_terms (always flows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f2061",
   "metadata": {},
   "source": [
    "## 7. Encoder-Decoder (Cross) Attention\n",
    "\n",
    "In seq2seq tasks, the decoder attends to encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d03653",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n7. ENCODER-DECODER ATTENTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Simulate encoder-decoder attention\n",
    "encoder_output = torch.randn(1, 20, 64)  # Source sequence (e.g., French)\n",
    "decoder_input = torch.randn(1, 15, 64)   # Target sequence (e.g., English)\n",
    "\n",
    "cross_attention = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "\n",
    "# Q from decoder, K/V from encoder\n",
    "output, attn_weights = cross_attention(\n",
    "    query=decoder_input,    # \"What am I generating?\"\n",
    "    key=encoder_output,     # \"What was the input?\"\n",
    "    value=encoder_output\n",
    ")\n",
    "\n",
    "print(f\"Encoder output shape: {encoder_output.shape} (source sequence)\")\n",
    "print(f\"Decoder input shape: {decoder_input.shape} (target sequence)\")\n",
    "print(f\"Cross-attention output: {output.shape}\")\n",
    "print(f\"Attention weights: {attn_weights.shape}\")\n",
    "print(f\"  - Each decoder position attends to all encoder positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889f4c8",
   "metadata": {},
   "source": [
    "## 8. Why Transformers Still Fail\n",
    "\n",
    "Understanding limitations is as important as understanding capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bbcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n8. WHY TRANSFORMERS STILL FAIL\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "limitations = [\n",
    "    (\"Context Limits\", \"Fixed window size (n tokens)\", \"Can't process arbitrarily long documents\"),\n",
    "    (\"Compute Cost\", \"O(n\u00b2) attention complexity\", \"Long contexts are expensive\"),\n",
    "    (\"Data Bias\", \"Trained on internet data\", \"Inherits and amplifies biases\"),\n",
    "    (\"Alignment\", \"Predicts next token\", \"Prediction \u2260 helpful/safe behavior\"),\n",
    "    (\"Hallucination\", \"No grounding in truth\", \"Confidently generates false information\"),\n",
    "    (\"Reasoning\", \"Pattern matching\", \"Struggles with novel logical problems\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Limitation':<20} {'Description':<35} {'Impact'}\")\n",
    "print(\"-\" * 90)\n",
    "for name, desc, impact in limitations:\n",
    "    print(f\"{name:<20} {desc:<35} {impact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97699e66",
   "metadata": {},
   "source": [
    "## Visualization: Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2224c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import os\n",
    "os.makedirs(\"data/interview_viz\", exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Attention heatmap\n",
    "ax = axes[0, 0]\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]\n",
    "seq_len = len(tokens)\n",
    "Q = torch.randn(1, seq_len, 32)\n",
    "K = torch.randn(1, seq_len, 32)\n",
    "V = torch.randn(1, seq_len, 32)\n",
    "_, attn = scaled_dot_product_attention(Q, K, V)\n",
    "im = ax.imshow(attn[0].numpy(), cmap='Blues')\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels(tokens, rotation=45)\n",
    "ax.set_yticklabels(tokens)\n",
    "ax.set_xlabel(\"Key (attending to)\")\n",
    "ax.set_ylabel(\"Query (from)\")\n",
    "ax.set_title(\"Self-Attention Weights\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# 2. Positional encoding patterns\n",
    "ax = axes[0, 1]\n",
    "pe = sinusoidal_positional_encoding(50, 64)\n",
    "im = ax.imshow(pe.numpy().T, cmap='RdBu', aspect='auto')\n",
    "ax.set_xlabel(\"Position\")\n",
    "ax.set_ylabel(\"Dimension\")\n",
    "ax.set_title(\"Sinusoidal Positional Encodings\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# 3. O(n\u00b2) complexity\n",
    "ax = axes[1, 0]\n",
    "seq_lengths = np.array([64, 128, 256, 512, 1024, 2048, 4096])\n",
    "computations = seq_lengths ** 2\n",
    "ax.plot(seq_lengths, computations / 1e6, 'b-o', linewidth=2)\n",
    "ax.fill_between(seq_lengths, 0, computations / 1e6, alpha=0.3)\n",
    "ax.set_xlabel(\"Sequence Length\")\n",
    "ax.set_ylabel(\"Attention Computations (millions)\")\n",
    "ax.set_title(\"O(n\u00b2) Complexity Scaling\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# 4. Multi-head attention diversity\n",
    "ax = axes[1, 1]\n",
    "mha = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "x = torch.randn(1, 8, 64)\n",
    "_, attn_weights = mha(x, x, x)\n",
    "# Show different patterns per head\n",
    "head_data = []\n",
    "for h in range(8):\n",
    "    head_data.append(attn_weights[0, h].detach().numpy().flatten())\n",
    "ax.boxplot(head_data, labels=[f\"H{i}\" for i in range(8)])\n",
    "ax.set_xlabel(\"Attention Head\")\n",
    "ax.set_ylabel(\"Attention Weight Distribution\")\n",
    "ax.set_title(\"Multi-Head Attention Diversity\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/interview_viz/transformer_deep_dive.png\", dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: data/interview_viz/transformer_deep_dive.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c446a5",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3698aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY INTERVIEW TAKEAWAYS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "takeaways = \"\"\"\n",
    "1. ATTENTION MECHANISM\n",
    "   - Replaces recurrence with parallel pairwise attention\n",
    "   - Formula: softmax(QK^T / \u221ad_k) \u00d7 V\n",
    "   - Scale factor prevents vanishing gradients\n",
    "\n",
    "2. O(n\u00b2) COMPLEXITY\n",
    "   - Every token attends to every other token\n",
    "   - Limits context window size\n",
    "   - Various optimizations exist (sparse, linear attention)\n",
    "\n",
    "3. POSITIONAL ENCODINGS\n",
    "   - Transformers are permutation-invariant\n",
    "   - Must inject order information externally\n",
    "   - Options: sinusoidal, learned, RoPE, ALiBi\n",
    "\n",
    "4. RESIDUALS + LAYER NORM\n",
    "   - Enable training very deep networks\n",
    "   - Gradient flows through skip connections\n",
    "   - Pre-norm vs post-norm affects stability\n",
    "\n",
    "5. MULTI-HEAD ATTENTION\n",
    "   - Different heads learn different patterns\n",
    "   - Diversity > parallelism\n",
    "   - Typically 8-128 heads in modern models\n",
    "\n",
    "6. LIMITATIONS MATTER\n",
    "   - Context limits, compute cost\n",
    "   - Data bias, alignment, hallucination\n",
    "   - Understanding failures shows depth\n",
    "\"\"\"\n",
    "print(takeaways)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMO COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}