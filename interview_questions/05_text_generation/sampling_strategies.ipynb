{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Sampling Strategies Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text Generation Sampling Strategies Demo\n",
    "=========================================\n",
    "\n",
    "This module demonstrates various text generation strategies used in Large Language Models.\n",
    "Perfect for interview preparation covering questions about:\n",
    "- Q5: Greedy decoding vs beam search\n",
    "- Q6: Temperature in LLM sampling\n",
    "- Q12: Top-k and top-p sampling\n",
    "\n",
    "Author: ML Interview Prep\n",
    "Date: 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3394d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650073ad",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "SETUP: Mock Vocabulary and Probability Distributions\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6767dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockVocabulary:\n",
    "    \"\"\"\n",
    "    A mock vocabulary for demonstration purposes.\n",
    "    In a real LLM, this would be the tokenizer's vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab = [\n",
    "            '<pad>', '<sos>', '<eos>', 'the', 'a', 'cat', 'dog', 'sat',\n",
    "            'on', 'mat', 'ran', 'jumped', 'quickly', 'slowly', 'and',\n",
    "            'or', 'but', 'very', 'quite', 'really', 'happy', 'sad',\n",
    "            'big', 'small', 'red', 'blue', 'green', 'yellow', 'house',\n",
    "            'tree', 'car', 'bike', 'book', 'pen', 'table', 'chair'\n",
    "        ]\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {idx: word for idx, word in enumerate(self.vocab)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def decode(self, idx: int) -> str:\n",
    "        return self.idx_to_word.get(idx, '<unk>')\n",
    "\n",
    "    def encode(self, word: str) -> int:\n",
    "        return self.word_to_idx.get(word, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a8d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_logits(vocab_size: int, seed: Optional[int] = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create mock logits (unnormalized log probabilities) that simulate\n",
    "    a language model's output at a single timestep.\n",
    "\n",
    "    In a real LLM, these would come from the final linear layer before softmax.\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        seed: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (vocab_size,) representing logits\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Create logits with some structure (not completely random)\n",
    "    # Higher values for common words, lower for rare words\n",
    "    logits = torch.randn(vocab_size) * 2.0\n",
    "\n",
    "    # Make some tokens more likely (simulate language model behavior)\n",
    "    # Indices 3-10 are common words in our vocab\n",
    "    logits[3:11] += 3.0  # Boost common words\n",
    "    logits[0:3] -= 5.0   # Suppress special tokens\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7924ff91",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "STRATEGY 1: GREEDY DECODING\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80320afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(logits: torch.Tensor, vocab: MockVocabulary) -> Tuple[int, str]:\n",
    "    \"\"\"\n",
    "    GREEDY DECODING: Always select the token with highest probability.\n",
    "\n",
    "    Algorithm:\n",
    "    1. Apply softmax to logits to get probabilities\n",
    "    2. Select argmax (token with highest probability)\n",
    "\n",
    "    Pros:\n",
    "    - Fast and deterministic\n",
    "    - Simple to implement\n",
    "    - Works well for factual/objective tasks\n",
    "\n",
    "    Cons:\n",
    "    - No exploration of alternative sequences\n",
    "    - Can lead to repetitive text\n",
    "    - May miss globally optimal sequences\n",
    "    - No diversity in generated text\n",
    "\n",
    "    Interview Talking Points:\n",
    "    - \"Greedy decoding is the simplest strategy but can be myopic\"\n",
    "    - \"It makes locally optimal choices without considering future consequences\"\n",
    "    - \"Used when you need deterministic, predictable outputs\"\n",
    "\n",
    "    Args:\n",
    "        logits: Unnormalized log probabilities from model\n",
    "        vocab: Vocabulary for decoding\n",
    "\n",
    "    Returns:\n",
    "        (token_id, token_string) tuple\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GREEDY DECODING\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get the token with maximum probability\n",
    "    token_id = torch.argmax(probs).item()\n",
    "    token_str = vocab.decode(token_id)\n",
    "\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    print(f\"Max probability: {probs[token_id]:.4f}\")\n",
    "    print(f\"Selected token ID: {token_id}\")\n",
    "    print(f\"Selected token: '{token_str}'\")\n",
    "\n",
    "    # Show top 5 alternatives\n",
    "    print(\"\\nTop 5 alternatives (not explored in greedy):\")\n",
    "    top_probs, top_indices = torch.topk(probs, k=5)\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        token = vocab.decode(idx.item())\n",
    "        selected = \"\u2713 SELECTED\" if i == 0 else \"\"\n",
    "        print(f\"  {i+1}. '{token}' (prob={prob:.4f}) {selected}\")\n",
    "\n",
    "    return token_id, token_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583cee3",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "STRATEGY 2: BEAM SEARCH\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa15a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Beam:\n",
    "    \"\"\"Represents a single beam (hypothesis) in beam search.\"\"\"\n",
    "    tokens: List[int]          # Sequence of token IDs\n",
    "    score: float               # Cumulative log probability\n",
    "    finished: bool = False     # Whether sequence ended with <eos>\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        \"\"\"For sorting beams by score.\"\"\"\n",
    "        return self.score < other.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0de86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(\n",
    "    logits_sequence: List[torch.Tensor],\n",
    "    vocab: MockVocabulary,\n",
    "    beam_width: int = 3,\n",
    "    length_penalty: float = 1.0\n",
    ") -> List[Beam]:\n",
    "    \"\"\"\n",
    "    BEAM SEARCH: Keep track of top-k most likely sequences at each step.\n",
    "\n",
    "    Algorithm:\n",
    "    1. Initialize with <sos> token\n",
    "    2. At each step:\n",
    "       - For each beam, get top-k next tokens\n",
    "       - Create k new candidate sequences per beam\n",
    "       - Keep only top beam_width candidates overall\n",
    "    3. Return beams when all finish or max length reached\n",
    "\n",
    "    Key Concepts:\n",
    "    - Beam width: Number of hypotheses to maintain\n",
    "    - Length penalty: Prevents bias toward shorter sequences\n",
    "    - Score normalization: Divide by length^penalty\n",
    "\n",
    "    Pros:\n",
    "    - Better than greedy: explores multiple paths\n",
    "    - Can find higher probability sequences\n",
    "    - Configurable exploration vs exploitation\n",
    "\n",
    "    Cons:\n",
    "    - More computationally expensive (k times greedy)\n",
    "    - Still may miss globally optimal sequence\n",
    "    - Can lead to generic/safe outputs\n",
    "    - Tends to prefer shorter sequences without length penalty\n",
    "\n",
    "    Interview Talking Points:\n",
    "    - \"Beam search is a middle ground between greedy and exhaustive search\"\n",
    "    - \"Beam width is a key hyperparameter: larger = better quality but slower\"\n",
    "    - \"Length penalty prevents bias toward short sequences\"\n",
    "    - \"Used in machine translation, summarization, etc.\"\n",
    "\n",
    "    Args:\n",
    "        logits_sequence: List of logit tensors (one per timestep)\n",
    "        vocab: Vocabulary for decoding\n",
    "        beam_width: Number of beams to maintain (k)\n",
    "        length_penalty: Alpha for length normalization (score / len^alpha)\n",
    "\n",
    "    Returns:\n",
    "        List of top beams sorted by score\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"BEAM SEARCH (beam_width={beam_width}, length_penalty={length_penalty})\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Initialize: Start with a single beam containing <sos> token\n",
    "    sos_token = vocab.encode('<sos>')\n",
    "    eos_token = vocab.encode('<eos>')\n",
    "\n",
    "    beams = [Beam(tokens=[sos_token], score=0.0)]\n",
    "\n",
    "    print(f\"\\nInitial beam: [<sos>] (token_id={sos_token})\")\n",
    "    print(f\"EOS token ID: {eos_token}\")\n",
    "\n",
    "    # Process each timestep\n",
    "    for step, logits in enumerate(logits_sequence):\n",
    "        print(f\"\\n--- Step {step + 1} ---\")\n",
    "\n",
    "        # Convert logits to log probabilities (for numerical stability)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Generate candidates from all active beams\n",
    "        candidates = []\n",
    "\n",
    "        for beam_idx, beam in enumerate(beams):\n",
    "            if beam.finished:\n",
    "                # Keep finished beams as-is\n",
    "                candidates.append(beam)\n",
    "                continue\n",
    "\n",
    "            # Get top-k tokens for this beam\n",
    "            top_log_probs, top_indices = torch.topk(log_probs, k=beam_width)\n",
    "\n",
    "            # Create new candidate beams\n",
    "            for log_prob, token_id in zip(top_log_probs, top_indices):\n",
    "                new_tokens = beam.tokens + [token_id.item()]\n",
    "                new_score = beam.score + log_prob.item()\n",
    "\n",
    "                # Check if this token is <eos>\n",
    "                finished = (token_id.item() == eos_token)\n",
    "\n",
    "                new_beam = Beam(\n",
    "                    tokens=new_tokens,\n",
    "                    score=new_score,\n",
    "                    finished=finished\n",
    "                )\n",
    "                candidates.append(new_beam)\n",
    "\n",
    "                if beam_idx == 0:  # Show details for first beam\n",
    "                    token_str = vocab.decode(token_id.item())\n",
    "                    print(f\"  Beam {beam_idx} + '{token_str}': score={new_score:.3f}\")\n",
    "\n",
    "        # Apply length penalty and select top beam_width candidates\n",
    "        # Length penalty: normalized_score = score / (length ** length_penalty)\n",
    "        scored_candidates = []\n",
    "        for candidate in candidates:\n",
    "            length = len(candidate.tokens)\n",
    "            normalized_score = candidate.score / (length ** length_penalty)\n",
    "            scored_candidates.append((normalized_score, candidate))\n",
    "\n",
    "        # Sort by normalized score (higher is better)\n",
    "        scored_candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Keep top beam_width\n",
    "        beams = [candidate for _, candidate in scored_candidates[:beam_width]]\n",
    "\n",
    "        print(f\"\\nTop {beam_width} beams after step {step + 1}:\")\n",
    "        for i, beam in enumerate(beams):\n",
    "            tokens_str = [vocab.decode(t) for t in beam.tokens]\n",
    "            status = \"FINISHED\" if beam.finished else \"active\"\n",
    "            print(f\"  {i+1}. {tokens_str} (score={beam.score:.3f}, {status})\")\n",
    "\n",
    "        # Early stopping if all beams are finished\n",
    "        if all(beam.finished for beam in beams):\n",
    "            print(\"\\nAll beams finished! Stopping early.\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FINAL BEAMS (sorted by score):\")\n",
    "    print('='*70)\n",
    "    for i, beam in enumerate(beams):\n",
    "        tokens_str = [vocab.decode(t) for t in beam.tokens]\n",
    "        length = len(beam.tokens)\n",
    "        normalized_score = beam.score / (length ** length_penalty)\n",
    "        print(f\"\\nBeam {i+1}:\")\n",
    "        print(f\"  Tokens: {tokens_str}\")\n",
    "        print(f\"  Score: {beam.score:.4f}\")\n",
    "        print(f\"  Length: {length}\")\n",
    "        print(f\"  Normalized score: {normalized_score:.4f}\")\n",
    "\n",
    "    return beams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cffe19",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "STRATEGY 3: TEMPERATURE SCALING\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5242b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sampling(\n",
    "    logits: torch.Tensor,\n",
    "    vocab: MockVocabulary,\n",
    "    temperature: float = 1.0,\n",
    "    num_samples: int = 5\n",
    ") -> List[Tuple[int, str, float]]:\n",
    "    \"\"\"\n",
    "    TEMPERATURE SCALING: Control randomness/creativity of sampling.\n",
    "\n",
    "    Algorithm:\n",
    "    1. Divide logits by temperature: scaled_logits = logits / T\n",
    "    2. Apply softmax to get probabilities\n",
    "    3. Sample from the resulting distribution\n",
    "\n",
    "    Temperature Effects:\n",
    "    - T = 1.0: Use original distribution (standard sampling)\n",
    "    - T \u2192 0: Approaches greedy (deterministic, peaked distribution)\n",
    "    - T > 1: More uniform distribution (more random, creative)\n",
    "    - T < 1: More peaked distribution (more focused, conservative)\n",
    "\n",
    "    Mathematical Intuition:\n",
    "    - Softmax: p_i = exp(logit_i) / \u03a3 exp(logit_j)\n",
    "    - With temp: p_i = exp(logit_i/T) / \u03a3 exp(logit_j/T)\n",
    "    - Lower T makes large logits dominate exponentially more\n",
    "    - Higher T makes all logits more similar after exp()\n",
    "\n",
    "    Use Cases:\n",
    "    - T=0.7: Factual tasks (Q&A, math, coding)\n",
    "    - T=1.0: Balanced tasks (general conversation)\n",
    "    - T=1.2-1.5: Creative tasks (story writing, brainstorming)\n",
    "\n",
    "    Interview Talking Points:\n",
    "    - \"Temperature controls the exploration-exploitation tradeoff\"\n",
    "    - \"Low temp = more deterministic, high temp = more diverse\"\n",
    "    - \"It's like adjusting confidence: low temp = very confident picks\"\n",
    "    - \"Different tasks need different temperatures\"\n",
    "\n",
    "    Args:\n",
    "        logits: Unnormalized log probabilities\n",
    "        vocab: Vocabulary for decoding\n",
    "        temperature: Scaling factor (T > 0)\n",
    "        num_samples: Number of samples to draw\n",
    "\n",
    "    Returns:\n",
    "        List of (token_id, token_str, probability) tuples\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TEMPERATURE SAMPLING (T={temperature})\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Scale logits by temperature\n",
    "    scaled_logits = logits / temperature\n",
    "\n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "    print(f\"Original logits range: [{logits.min():.2f}, {logits.max():.2f}]\")\n",
    "    print(f\"Scaled logits range: [{scaled_logits.min():.2f}, {scaled_logits.max():.2f}]\")\n",
    "    print(f\"Probability entropy: {-(probs * torch.log(probs + 1e-10)).sum():.4f}\")\n",
    "\n",
    "    # Sample multiple times to show distribution\n",
    "    samples = []\n",
    "    print(f\"\\nDrawing {num_samples} samples:\")\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Sample from categorical distribution\n",
    "        token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        token_str = vocab.decode(token_id)\n",
    "        prob = probs[token_id].item()\n",
    "\n",
    "        samples.append((token_id, token_str, prob))\n",
    "        print(f\"  Sample {i+1}: '{token_str}' (prob={prob:.4f})\")\n",
    "\n",
    "    # Show distribution statistics\n",
    "    print(\"\\nTop 10 most likely tokens:\")\n",
    "    top_probs, top_indices = torch.topk(probs, k=10)\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        token = vocab.decode(idx.item())\n",
    "        print(f\"  {i+1}. '{token}' (prob={prob:.4f})\")\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d437818",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "STRATEGY 4: TOP-K SAMPLING\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a92e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(\n",
    "    logits: torch.Tensor,\n",
    "    vocab: MockVocabulary,\n",
    "    k: int = 10,\n",
    "    temperature: float = 1.0,\n",
    "    num_samples: int = 5\n",
    ") -> List[Tuple[int, str, float]]:\n",
    "    \"\"\"\n",
    "    TOP-K SAMPLING: Sample from only the k most likely tokens.\n",
    "\n",
    "    Algorithm:\n",
    "    1. Apply temperature scaling (optional)\n",
    "    2. Find top-k tokens by probability\n",
    "    3. Set all other tokens' probabilities to 0\n",
    "    4. Renormalize the top-k probabilities\n",
    "    5. Sample from this restricted distribution\n",
    "\n",
    "    Key Concepts:\n",
    "    - k is a hyperparameter: larger k = more diversity\n",
    "    - Truncates the \"long tail\" of unlikely tokens\n",
    "    - Prevents sampling of very low probability (nonsensical) tokens\n",
    "\n",
    "    Pros:\n",
    "    - Prevents unlikely/nonsensical tokens\n",
    "    - More diverse than greedy or beam search\n",
    "    - Simple and effective\n",
    "    - Consistent quality across different contexts\n",
    "\n",
    "    Cons:\n",
    "    - Fixed k may not fit all contexts\n",
    "    - When distribution is flat, top-k may be too restrictive\n",
    "    - When distribution is peaked, top-k may include bad tokens\n",
    "\n",
    "    Interview Talking Points:\n",
    "    - \"Top-k prevents the model from saying ridiculous things\"\n",
    "    - \"It's like giving the model a curated menu of reasonable options\"\n",
    "    - \"Fixed k is both strength and weakness - doesn't adapt to context\"\n",
    "    - \"Introduced in the paper 'Hierarchical Neural Story Generation' (2018)\"\n",
    "\n",
    "    Args:\n",
    "        logits: Unnormalized log probabilities\n",
    "        vocab: Vocabulary for decoding\n",
    "        k: Number of top tokens to consider\n",
    "        temperature: Temperature scaling factor\n",
    "        num_samples: Number of samples to draw\n",
    "\n",
    "    Returns:\n",
    "        List of (token_id, token_str, probability) tuples\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TOP-K SAMPLING (k={k}, T={temperature})\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    scaled_logits = logits / temperature\n",
    "\n",
    "    # Get probabilities\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "    # Get top-k probabilities and indices\n",
    "    top_k_probs, top_k_indices = torch.topk(probs, k=k)\n",
    "\n",
    "    print(f\"Original vocab size: {len(probs)}\")\n",
    "    print(f\"Restricted to top-{k} tokens\")\n",
    "    print(f\"Top-{k} probability mass: {top_k_probs.sum():.4f}\")\n",
    "    print(f\"Excluded probability mass: {1 - top_k_probs.sum():.4f}\")\n",
    "\n",
    "    # Create a new probability distribution with only top-k\n",
    "    # Set all other probs to 0 and renormalize\n",
    "    restricted_probs = torch.zeros_like(probs)\n",
    "    restricted_probs[top_k_indices] = top_k_probs\n",
    "\n",
    "    # Renormalize (should already sum to ~1, but for numerical stability)\n",
    "    restricted_probs = restricted_probs / restricted_probs.sum()\n",
    "\n",
    "    # Sample multiple times\n",
    "    samples = []\n",
    "    print(f\"\\nDrawing {num_samples} samples:\")\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        token_id = torch.multinomial(restricted_probs, num_samples=1).item()\n",
    "        token_str = vocab.decode(token_id)\n",
    "        prob = restricted_probs[token_id].item()\n",
    "\n",
    "        samples.append((token_id, token_str, prob))\n",
    "        print(f\"  Sample {i+1}: '{token_str}' (prob={prob:.4f})\")\n",
    "\n",
    "    # Show the top-k tokens\n",
    "    print(f\"\\nTop-{k} candidate tokens:\")\n",
    "    for i, (prob, idx) in enumerate(zip(top_k_probs, top_k_indices)):\n",
    "        token = vocab.decode(idx.item())\n",
    "        print(f\"  {i+1}. '{token}' (prob={prob:.4f})\")\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a8763",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "STRATEGY 5: TOP-P (NUCLEUS) SAMPLING\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b747af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(\n",
    "    logits: torch.Tensor,\n",
    "    vocab: MockVocabulary,\n",
    "    p: float = 0.9,\n",
    "    temperature: float = 1.0,\n",
    "    num_samples: int = 5\n",
    ") -> List[Tuple[int, str, float]]:\n",
    "    \"\"\"\n",
    "    TOP-P (NUCLEUS) SAMPLING: Sample from smallest set of tokens whose\n",
    "    cumulative probability exceeds p.\n",
    "\n",
    "    Algorithm:\n",
    "    1. Apply temperature scaling (optional)\n",
    "    2. Sort tokens by probability (descending)\n",
    "    3. Compute cumulative probability\n",
    "    4. Find smallest set where cumulative prob >= p\n",
    "    5. Sample from this \"nucleus\" set\n",
    "\n",
    "    Key Concepts:\n",
    "    - Adaptive: nucleus size varies with distribution shape\n",
    "    - p is typically 0.9-0.95 (90-95% probability mass)\n",
    "    - Also called \"nucleus sampling\"\n",
    "\n",
    "    Advantages over Top-K:\n",
    "    - Adapts to context: fewer tokens when confident, more when uncertain\n",
    "    - When distribution is peaked: nucleus is small (focused)\n",
    "    - When distribution is flat: nucleus is large (diverse)\n",
    "\n",
    "    Pros:\n",
    "    - Adapts to model confidence\n",
    "    - More context-aware than top-k\n",
    "    - Maintains quality while allowing creativity\n",
    "    - Currently preferred method in many LLMs\n",
    "\n",
    "    Cons:\n",
    "    - Slightly more complex than top-k\n",
    "    - Can still include unlikely tokens in flat distributions\n",
    "\n",
    "    Interview Talking Points:\n",
    "    - \"Top-p adapts to model confidence - that's its key advantage\"\n",
    "    - \"It's like having a dynamic top-k based on certainty\"\n",
    "    - \"Introduced in 'The Curious Case of Neural Text Degeneration' (2019)\"\n",
    "    - \"Most modern LLMs (GPT-3/4, Claude) use top-p by default\"\n",
    "    - \"Can be combined with temperature for fine-grained control\"\n",
    "\n",
    "    Args:\n",
    "        logits: Unnormalized log probabilities\n",
    "        vocab: Vocabulary for decoding\n",
    "        p: Cumulative probability threshold (0 < p <= 1)\n",
    "        temperature: Temperature scaling factor\n",
    "        num_samples: Number of samples to draw\n",
    "\n",
    "    Returns:\n",
    "        List of (token_id, token_str, probability) tuples\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TOP-P (NUCLEUS) SAMPLING (p={p}, T={temperature})\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    scaled_logits = logits / temperature\n",
    "\n",
    "    # Get probabilities\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "    # Sort probabilities in descending order\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "\n",
    "    # Compute cumulative probabilities\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "    # Find the cutoff index where cumulative prob exceeds p\n",
    "    # Remove tokens with cumulative probability above p\n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "\n",
    "    # Shift right to keep at least one token and the first token above threshold\n",
    "    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "    sorted_indices_to_remove[0] = False\n",
    "\n",
    "    # Create mask for original indices\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "\n",
    "    # Create restricted probability distribution\n",
    "    restricted_probs = probs.clone()\n",
    "    restricted_probs[indices_to_remove] = 0.0\n",
    "\n",
    "    # Count nucleus size\n",
    "    nucleus_size = (restricted_probs > 0).sum().item()\n",
    "\n",
    "    print(f\"Original vocab size: {len(probs)}\")\n",
    "    print(f\"Nucleus size: {nucleus_size} tokens\")\n",
    "    print(f\"Nucleus probability mass: {restricted_probs.sum():.4f}\")\n",
    "    print(f\"Excluded probability mass: {1 - restricted_probs.sum():.4f}\")\n",
    "    print(f\"Nucleus size is {nucleus_size/len(probs)*100:.1f}% of vocabulary\")\n",
    "\n",
    "    # Renormalize\n",
    "    restricted_probs = restricted_probs / restricted_probs.sum()\n",
    "\n",
    "    # Sample multiple times\n",
    "    samples = []\n",
    "    print(f\"\\nDrawing {num_samples} samples:\")\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        token_id = torch.multinomial(restricted_probs, num_samples=1).item()\n",
    "        token_str = vocab.decode(token_id)\n",
    "        prob = restricted_probs[token_id].item()\n",
    "\n",
    "        samples.append((token_id, token_str, prob))\n",
    "        print(f\"  Sample {i+1}: '{token_str}' (prob={prob:.4f})\")\n",
    "\n",
    "    # Show the nucleus tokens (up to 15)\n",
    "    nucleus_indices = (restricted_probs > 0).nonzero(as_tuple=True)[0]\n",
    "    nucleus_probs = restricted_probs[nucleus_indices]\n",
    "\n",
    "    # Sort nucleus by probability\n",
    "    sorted_nucleus_probs, sorted_nucleus_idx = torch.sort(nucleus_probs, descending=True)\n",
    "    sorted_nucleus_indices = nucleus_indices[sorted_nucleus_idx]\n",
    "\n",
    "    print(f\"\\nTop tokens in nucleus (up to 15):\")\n",
    "    for i, (prob, idx) in enumerate(zip(sorted_nucleus_probs[:15], sorted_nucleus_indices[:15])):\n",
    "        token = vocab.decode(idx.item())\n",
    "        cumulative = sorted_nucleus_probs[:i+1].sum().item()\n",
    "        print(f\"  {i+1}. '{token}' (prob={prob:.4f}, cumulative={cumulative:.4f})\")\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f87a1f1",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "VISUALIZATION FUNCTIONS\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_temperature_effects(\n",
    "    logits: torch.Tensor,\n",
    "    vocab: MockVocabulary,\n",
    "    temperatures: List[float],\n",
    "    save_path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize how temperature affects probability distributions.\n",
    "\n",
    "    Creates a plot showing:\n",
    "    - Original logits\n",
    "    - Probability distributions at different temperatures\n",
    "    - Entropy at each temperature\n",
    "\n",
    "    Args:\n",
    "        logits: Original logits\n",
    "        vocab: Vocabulary for labeling\n",
    "        temperatures: List of temperatures to visualize\n",
    "        save_path: Path to save the figure\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VISUALIZING TEMPERATURE EFFECTS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Select top-k tokens to visualize (for clarity)\n",
    "    k = 15\n",
    "    probs_T1 = F.softmax(logits, dim=-1)\n",
    "    top_probs, top_indices = torch.topk(probs_T1, k=k)\n",
    "\n",
    "    # Get token labels\n",
    "    token_labels = [vocab.decode(idx.item()) for idx in top_indices]\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Temperature Effects on Probability Distribution', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot for each temperature\n",
    "    entropies = []\n",
    "\n",
    "    for idx, temp in enumerate(temperatures):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "        # Compute probabilities at this temperature\n",
    "        scaled_logits = logits / temp\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "        # Extract probabilities for top-k tokens\n",
    "        top_probs_temp = probs[top_indices]\n",
    "\n",
    "        # Compute entropy: -\u03a3 p(x) log p(x)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum().item()\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        # Create bar plot\n",
    "        bars = ax.bar(range(k), top_probs_temp.numpy(), alpha=0.7, color='steelblue')\n",
    "        ax.set_xlabel('Token', fontsize=12)\n",
    "        ax.set_ylabel('Probability', fontsize=12)\n",
    "        ax.set_title(f'Temperature = {temp} (Entropy = {entropy:.3f})', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(range(k))\n",
    "        ax.set_xticklabels(token_labels, rotation=45, ha='right')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        # Highlight top token\n",
    "        bars[0].set_color('crimson')\n",
    "        bars[0].set_alpha(1.0)\n",
    "\n",
    "        # Add probability labels on bars\n",
    "        for i, (bar, prob) in enumerate(zip(bars, top_probs_temp)):\n",
    "            if i < 5:  # Label top 5\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{prob:.3f}',\n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n\u2713 Saved visualization to: {save_path}\")\n",
    "\n",
    "    # Print entropy analysis\n",
    "    print(\"\\nEntropy Analysis:\")\n",
    "    print(\"(Higher entropy = more uniform/random distribution)\")\n",
    "    for temp, entropy in zip(temperatures, entropies):\n",
    "        print(f\"  T={temp}: Entropy={entropy:.4f}\")\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91290387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sampling_comparison(\n",
    "    logits: torch.Tensor,\n",
    "    vocab: MockVocabulary,\n",
    "    save_path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare probability distributions for different sampling strategies.\n",
    "\n",
    "    Creates a multi-panel plot comparing:\n",
    "    - Original distribution\n",
    "    - Top-k filtered distribution\n",
    "    - Top-p (nucleus) filtered distribution\n",
    "    - Temperature-scaled distribution\n",
    "\n",
    "    Args:\n",
    "        logits: Original logits\n",
    "        vocab: Vocabulary for labeling\n",
    "        save_path: Path to save the figure\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VISUALIZING SAMPLING STRATEGY COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Parameters\n",
    "    k = 10\n",
    "    p = 0.9\n",
    "    temperature = 1.2\n",
    "\n",
    "    # Original probabilities\n",
    "    probs_original = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get top tokens for consistent x-axis\n",
    "    top_probs, top_indices = torch.topk(probs_original, k=20)\n",
    "    token_labels = [vocab.decode(idx.item()) for idx in top_indices]\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Comparison of Sampling Strategies', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Original distribution\n",
    "    ax = axes[0, 0]\n",
    "    probs_display = probs_original[top_indices].numpy()\n",
    "    bars = ax.bar(range(len(probs_display)), probs_display, alpha=0.7, color='steelblue')\n",
    "    bars[0].set_color('crimson')\n",
    "    ax.set_title('Original Distribution (Softmax)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Token', fontsize=12)\n",
    "    ax.set_ylabel('Probability', fontsize=12)\n",
    "    ax.set_xticks(range(len(token_labels)))\n",
    "    ax.set_xticklabels(token_labels, rotation=45, ha='right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # 2. Top-k sampling\n",
    "    ax = axes[0, 1]\n",
    "    probs_topk = probs_original.clone()\n",
    "    top_k_probs, top_k_indices = torch.topk(probs_original, k=k)\n",
    "    mask = torch.zeros_like(probs_topk, dtype=torch.bool)\n",
    "    mask[top_k_indices] = True\n",
    "    probs_topk[~mask] = 0\n",
    "    probs_topk = probs_topk / probs_topk.sum()  # Renormalize\n",
    "\n",
    "    probs_display = probs_topk[top_indices].numpy()\n",
    "    bars = ax.bar(range(len(probs_display)), probs_display, alpha=0.7, color='forestgreen')\n",
    "    for i in range(min(k, len(bars))):\n",
    "        bars[i].set_alpha(1.0)\n",
    "    ax.set_title(f'Top-k Sampling (k={k})', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Token', fontsize=12)\n",
    "    ax.set_ylabel('Probability', fontsize=12)\n",
    "    ax.set_xticks(range(len(token_labels)))\n",
    "    ax.set_xticklabels(token_labels, rotation=45, ha='right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.axvline(x=k-0.5, color='red', linestyle='--', linewidth=2, label=f'Top-{k} cutoff')\n",
    "    ax.legend()\n",
    "\n",
    "    # 3. Top-p (nucleus) sampling\n",
    "    ax = axes[1, 0]\n",
    "    sorted_probs, sorted_indices = torch.sort(probs_original, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "    sorted_indices_to_remove[0] = False\n",
    "\n",
    "    probs_topp = probs_original.clone()\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    probs_topp[indices_to_remove] = 0\n",
    "    nucleus_size = (probs_topp > 0).sum().item()\n",
    "    probs_topp = probs_topp / probs_topp.sum()  # Renormalize\n",
    "\n",
    "    probs_display = probs_topp[top_indices].numpy()\n",
    "    bars = ax.bar(range(len(probs_display)), probs_display, alpha=0.7, color='darkorange')\n",
    "    for i, bar in enumerate(bars):\n",
    "        if probs_display[i] > 0:\n",
    "            bar.set_alpha(1.0)\n",
    "    ax.set_title(f'Top-p Sampling (p={p}, nucleus_size={nucleus_size})', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Token', fontsize=12)\n",
    "    ax.set_ylabel('Probability', fontsize=12)\n",
    "    ax.set_xticks(range(len(token_labels)))\n",
    "    ax.set_xticklabels(token_labels, rotation=45, ha='right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # 4. Temperature scaling\n",
    "    ax = axes[1, 1]\n",
    "    scaled_logits = logits / temperature\n",
    "    probs_temp = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "    probs_display = probs_temp[top_indices].numpy()\n",
    "    bars = ax.bar(range(len(probs_display)), probs_display, alpha=0.7, color='mediumpurple')\n",
    "    bars[0].set_color('crimson')\n",
    "    ax.set_title(f'Temperature Scaling (T={temperature})', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Token', fontsize=12)\n",
    "    ax.set_ylabel('Probability', fontsize=12)\n",
    "    ax.set_xticks(range(len(token_labels)))\n",
    "    ax.set_xticklabels(token_labels, rotation=45, ha='right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n\u2713 Saved visualization to: {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_beam_search_tree(save_path: str):\n",
    "    \"\"\"\n",
    "    Create a conceptual visualization of how beam search explores the search space.\n",
    "\n",
    "    Shows:\n",
    "    - Search tree with multiple beams\n",
    "    - Pruned paths vs kept paths\n",
    "    - Comparison with greedy search\n",
    "\n",
    "    Args:\n",
    "        save_path: Path to save the figure\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VISUALIZING BEAM SEARCH TREE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    fig.suptitle('Greedy Decoding vs Beam Search', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Greedy search visualization\n",
    "    ax = axes[0]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Greedy Decoding (Single Path)', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Draw greedy path\n",
    "    positions = [(1, 5), (3, 6), (5, 7), (7, 8), (9, 7)]\n",
    "    for i in range(len(positions) - 1):\n",
    "        x1, y1 = positions[i]\n",
    "        x2, y2 = positions[i + 1]\n",
    "        ax.plot([x1, x2], [y1, y2], 'b-', linewidth=3, alpha=0.8)\n",
    "        ax.plot(x1, y1, 'ro', markersize=15)\n",
    "    ax.plot(positions[-1][0], positions[-1][1], 'ro', markersize=15)\n",
    "\n",
    "    # Add labels\n",
    "    ax.text(1, 5, 'START', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "    ax.text(9, 7, 'END', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "    # Show unexplored alternatives\n",
    "    unexplored = [(3, 4), (3, 5), (5, 5), (5, 6), (7, 6), (7, 7)]\n",
    "    for x, y in unexplored:\n",
    "        ax.plot(x, y, 'o', color='lightgray', markersize=10, alpha=0.5)\n",
    "\n",
    "    ax.text(5, 1, 'Only explores highest\\nprobability path', ha='center', fontsize=11,\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    # Beam search visualization\n",
    "    ax = axes[1]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Beam Search (beam_width=3)', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Draw multiple beams\n",
    "    beam_paths = [\n",
    "        [(1, 5), (3, 6), (5, 7), (7, 8), (9, 7)],   # Beam 1\n",
    "        [(1, 5), (3, 5), (5, 6), (7, 7), (9, 6)],   # Beam 2\n",
    "        [(1, 5), (3, 4), (5, 5), (7, 6), (9, 5)],   # Beam 3\n",
    "    ]\n",
    "    colors = ['crimson', 'forestgreen', 'darkorange']\n",
    "\n",
    "    for path, color in zip(beam_paths, colors):\n",
    "        for i in range(len(path) - 1):\n",
    "            x1, y1 = path[i]\n",
    "            x2, y2 = path[i + 1]\n",
    "            ax.plot([x1, x2], [y1, y2], '-', color=color, linewidth=2.5, alpha=0.7)\n",
    "            ax.plot(x1, y1, 'o', color=color, markersize=12)\n",
    "        ax.plot(path[-1][0], path[-1][1], 'o', color=color, markersize=12)\n",
    "\n",
    "    # Add labels\n",
    "    ax.text(1, 5, 'START', ha='center', va='center', fontsize=9, fontweight='bold', color='white')\n",
    "    ax.text(9, 7, 'Path 1', ha='center', va='center', fontsize=8, color='white')\n",
    "    ax.text(9, 6, 'Path 2', ha='center', va='center', fontsize=8, color='white')\n",
    "    ax.text(9, 5, 'Path 3', ha='center', va='center', fontsize=8, color='white')\n",
    "\n",
    "    ax.text(5, 1, 'Explores top-k paths\\nin parallel', ha='center', fontsize=11,\n",
    "           bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n\u2713 Saved visualization to: {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea696da",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "MAIN DEMONSTRATION\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d0243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main demonstration function that runs all sampling strategies and creates\n",
    "    visualizations for interview preparation.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"TEXT GENERATION SAMPLING STRATEGIES DEMO\")\n",
    "    print(\"LLM Interview Preparation\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = \"/Users/zack/dev/ml-refresher/data/interview_viz/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"\\nOutput directory: {output_dir}\")\n",
    "\n",
    "    # Initialize mock vocabulary\n",
    "    vocab = MockVocabulary()\n",
    "    print(f\"\\nVocabulary size: {len(vocab)}\")\n",
    "    print(f\"Sample tokens: {vocab.vocab[:10]}\")\n",
    "\n",
    "    # Create mock logits for a single timestep\n",
    "    logits = create_mock_logits(len(vocab), seed=42)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART 1: SINGLE TIMESTEP DEMONSTRATIONS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. Greedy Decoding\n",
    "    greedy_token_id, greedy_token = greedy_decode(logits, vocab)\n",
    "\n",
    "    # 2. Temperature Sampling (multiple temperatures)\n",
    "    temps = [0.5, 1.0, 1.5]\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARING DIFFERENT TEMPERATURES\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    for temp in temps:\n",
    "        temperature_sampling(logits, vocab, temperature=temp, num_samples=3)\n",
    "\n",
    "    # 3. Top-k Sampling\n",
    "    top_k_sampling(logits, vocab, k=10, temperature=1.0, num_samples=5)\n",
    "\n",
    "    # 4. Top-p Sampling\n",
    "    top_p_sampling(logits, vocab, p=0.9, temperature=1.0, num_samples=5)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART 2: MULTI-STEP BEAM SEARCH DEMONSTRATION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Create a sequence of logits for beam search (simulate 4 timesteps)\n",
    "    logits_sequence = [\n",
    "        create_mock_logits(len(vocab), seed=42 + i) for i in range(4)\n",
    "    ]\n",
    "\n",
    "    # 5. Beam Search\n",
    "    beams = beam_search(logits_sequence, vocab, beam_width=3, length_penalty=0.6)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART 3: VISUALIZATIONS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Visualization 1: Temperature effects\n",
    "    visualize_temperature_effects(\n",
    "        logits, vocab,\n",
    "        temperatures=[0.5, 0.8, 1.0, 1.5],\n",
    "        save_path=os.path.join(output_dir, \"temperature_effects.png\")\n",
    "    )\n",
    "\n",
    "    # Visualization 2: Strategy comparison\n",
    "    visualize_sampling_comparison(\n",
    "        logits, vocab,\n",
    "        save_path=os.path.join(output_dir, \"sampling_comparison.png\")\n",
    "    )\n",
    "\n",
    "    # Visualization 3: Beam search tree\n",
    "    visualize_beam_search_tree(\n",
    "        save_path=os.path.join(output_dir, \"beam_search_tree.png\")\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INTERVIEW KEY TAKEAWAYS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\"\"\n",
    "1. GREEDY DECODING:\n",
    "   - Simplest but myopic\n",
    "   - Always picks highest probability token\n",
    "   - Good for: deterministic tasks (math, code)\n",
    "   - Bad for: creative generation\n",
    "\n",
    "2. BEAM SEARCH:\n",
    "   - Keeps top-k sequences at each step\n",
    "   - Better than greedy for finding high-quality sequences\n",
    "   - Requires length penalty to avoid short sequences\n",
    "   - Good for: translation, summarization\n",
    "   - Computational cost: O(k \u00d7 vocab_size) per step\n",
    "\n",
    "3. TEMPERATURE:\n",
    "   - Controls randomness/creativity\n",
    "   - T\u21920: deterministic (like greedy)\n",
    "   - T>1: more random/creative\n",
    "   - T<1: more focused/conservative\n",
    "   - Always used with other sampling methods\n",
    "\n",
    "4. TOP-K SAMPLING:\n",
    "   - Fixed cutoff: only sample from top-k tokens\n",
    "   - Prevents unlikely/nonsensical tokens\n",
    "   - Cons: k doesn't adapt to context\n",
    "   - Good for: general text generation\n",
    "\n",
    "5. TOP-P (NUCLEUS) SAMPLING:\n",
    "   - Adaptive cutoff based on cumulative probability\n",
    "   - Nucleus grows/shrinks with model confidence\n",
    "   - More flexible than top-k\n",
    "   - Current best practice in most LLMs\n",
    "   - Often combined with temperature\n",
    "\n",
    "PRACTICAL TIPS:\n",
    "- Combine strategies: temperature + top-p is common\n",
    "- Different tasks need different settings\n",
    "- Factual tasks: low temp, greedy/beam search\n",
    "- Creative tasks: higher temp, top-p\n",
    "- GPT-3 default: temperature=1.0, top-p=1.0 (no truncation)\n",
    "- Claude default: similar to top-p with temperature\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DEMO COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nAll visualizations saved to: {output_dir}\")\n",
    "    print(\"\\nGenerated files:\")\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.endswith('.png'):\n",
    "            print(f\"  - {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}