{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Embeddings Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe68334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comprehensive Embeddings Demo for LLM Interviews\n",
    "=================================================\n",
    "\n",
    "This demo covers all essential embedding concepts for technical interviews:\n",
    "1. nn.Embedding creation and usage\n",
    "2. Equivalence between embedding lookup and one-hot matrix multiplication\n",
    "3. Different initialization methods\n",
    "4. Cosine similarity computation\n",
    "5. Embedding space visualization\n",
    "6. Weight tying (input/output embeddings)\n",
    "7. How embeddings are learned through backpropagation\n",
    "\n",
    "Author: Interview Prep\n",
    "Date: 2025-12-03\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c7281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d91ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR = Path(\"/Users/zack/dev/ml-refresher/data/interview_viz\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36bbc6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EMBEDDINGS DEMO FOR LLM INTERVIEWS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: Basic Embedding Creation and Usage\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 1: Basic Embedding Creation and Usage\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small vocabulary for demonstration\n",
    "vocab = [\"<PAD>\", \"<UNK>\", \"hello\", \"world\", \"machine\", \"learning\", \"neural\", \"network\"]\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 4  # Small dimension for easy visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word to index mapping\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nVocabulary: {vocab}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ea453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b8b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nEmbedding layer created:\")\n",
    "print(f\"  - Weight matrix shape: {embedding_layer.weight.shape}\")\n",
    "print(f\"  - Total parameters: {embedding_layer.weight.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45796233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up embeddings for some words\n",
    "word_indices = torch.LongTensor([word2idx[\"hello\"], word2idx[\"world\"], word2idx[\"learning\"]])\n",
    "word_embeddings = embedding_layer(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8ed2f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"\\nEmbedding lookup example:\")\n",
    "print(f\"  - Input indices: {word_indices.tolist()} -> {[idx2word[i] for i in word_indices.tolist()]}\")\n",
    "print(f\"  - Output shape: {word_embeddings.shape}\")\n",
    "print(f\"  - Embedding for 'hello':\\n{word_embeddings[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: Embedding Lookup = One-Hot @ Weight Matrix\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 2: Embedding Lookup \u2261 One-Hot @ Weight Matrix\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53119958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that embedding lookup is equivalent to one-hot encoding @ weight matrix\n",
    "word_idx = word2idx[\"machine\"]\n",
    "print(f\"\\nDemonstrating equivalence for word: '{idx2word[word_idx]}' (index {word_idx})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640bbdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Direct embedding lookup\n",
    "embedding_direct = embedding_layer(torch.LongTensor([word_idx]))\n",
    "print(f\"\\nMethod 1 - Direct embedding lookup:\")\n",
    "print(f\"  Shape: {embedding_direct.shape}\")\n",
    "print(f\"  Values: {embedding_direct.squeeze()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f15441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: One-hot encoding @ weight matrix\n",
    "one_hot = F.one_hot(torch.LongTensor([word_idx]), num_classes=vocab_size).float()\n",
    "embedding_onehot = one_hot @ embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83872ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nMethod 2 - One-hot @ Weight matrix:\")\n",
    "print(f\"  One-hot shape: {one_hot.shape}\")\n",
    "print(f\"  One-hot vector: {one_hot.squeeze().tolist()}\")\n",
    "print(f\"  Weight matrix shape: {embedding_layer.weight.shape}\")\n",
    "print(f\"  Result shape: {embedding_onehot.shape}\")\n",
    "print(f\"  Values: {embedding_onehot.squeeze()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b11baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify they are identical\n",
    "are_equal = torch.allclose(embedding_direct, embedding_onehot, atol=1e-6)\n",
    "print(f\"\\nAre the two methods equivalent? {are_equal}\")\n",
    "print(f\"Max difference: {torch.max(torch.abs(embedding_direct - embedding_onehot)).item():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb856ac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Explain why embedding lookup is more efficient\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"EFFICIENCY COMPARISON:\")\n",
    "print(\"-\"*80)\n",
    "print(\"Why use embedding lookup instead of one-hot multiplication?\")\n",
    "print(f\"  1. Memory: One-hot requires {vocab_size} floats, embedding lookup uses 1 integer\")\n",
    "print(f\"  2. Computation: One-hot does {vocab_size} \u00d7 {embedding_dim} = {vocab_size * embedding_dim} multiplications\")\n",
    "print(f\"     Embedding lookup does 0 multiplications (direct indexing)\")\n",
    "print(f\"  3. For vocab_size=50k, embedding_dim=768: One-hot needs 38.4M multiplications vs 0!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2310176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: Different Initialization Methods\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3: Different Initialization Methods\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default initialization (uniform)\n",
    "emb_default = nn.Embedding(vocab_size, embedding_dim)\n",
    "print(f\"\\n1. Default Initialization (Uniform):\")\n",
    "print(f\"   Distribution: U(-sqrt(k), sqrt(k)) where k = 1/embedding_dim\")\n",
    "print(f\"   Weight matrix stats:\")\n",
    "print(f\"     Mean: {emb_default.weight.mean().item():.4f}\")\n",
    "print(f\"     Std:  {emb_default.weight.std().item():.4f}\")\n",
    "print(f\"     Min:  {emb_default.weight.min().item():.4f}\")\n",
    "print(f\"     Max:  {emb_default.weight.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa91bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier/Glorot initialization\n",
    "emb_xavier = nn.Embedding(vocab_size, embedding_dim)\n",
    "nn.init.xavier_uniform_(emb_xavier.weight)\n",
    "print(f\"\\n2. Xavier/Glorot Uniform Initialization:\")\n",
    "print(f\"   Distribution: U(-a, a) where a = sqrt(6/(fan_in + fan_out))\")\n",
    "print(f\"   Weight matrix stats:\")\n",
    "print(f\"     Mean: {emb_xavier.weight.mean().item():.4f}\")\n",
    "print(f\"     Std:  {emb_xavier.weight.std().item():.4f}\")\n",
    "print(f\"     Min:  {emb_xavier.weight.min().item():.4f}\")\n",
    "print(f\"     Max:  {emb_xavier.weight.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal initialization\n",
    "emb_normal = nn.Embedding(vocab_size, embedding_dim)\n",
    "nn.init.normal_(emb_normal.weight, mean=0.0, std=0.02)\n",
    "print(f\"\\n3. Normal Initialization (BERT-style):\")\n",
    "print(f\"   Distribution: N(0, 0.02\u00b2)\")\n",
    "print(f\"   Weight matrix stats:\")\n",
    "print(f\"     Mean: {emb_normal.weight.mean().item():.4f}\")\n",
    "print(f\"     Std:  {emb_normal.weight.std().item():.4f}\")\n",
    "print(f\"     Min:  {emb_normal.weight.min().item():.4f}\")\n",
    "print(f\"     Max:  {emb_normal.weight.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79387ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero initialization (for special tokens like padding)\n",
    "emb_zero = nn.Embedding(vocab_size, embedding_dim)\n",
    "nn.init.zeros_(emb_zero.weight)\n",
    "emb_zero.weight.data[1:] = emb_normal.weight.data[1:]  # Keep only first row as zeros\n",
    "print(f\"\\n4. Zero Initialization (for special tokens):\")\n",
    "print(f\"   First row (PAD token): {emb_zero.weight[0].tolist()}\")\n",
    "print(f\"   Second row (UNK token): {emb_zero.weight[1][:4].tolist()}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87513b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize initialization distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle(\"Embedding Initialization Methods\", fontsize=16, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c173e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_methods = [\n",
    "    (emb_default, \"Default (Uniform)\", axes[0, 0]),\n",
    "    (emb_xavier, \"Xavier Uniform\", axes[0, 1]),\n",
    "    (emb_normal, \"Normal (BERT-style)\", axes[1, 0]),\n",
    "    (emb_zero, \"Mixed (Zero + Normal)\", axes[1, 1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for emb, title, ax in init_methods:\n",
    "    weights_flat = emb.weight.detach().numpy().flatten()\n",
    "    ax.hist(weights_flat, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(f\"{title}\\nMean: {weights_flat.mean():.3f}, Std: {weights_flat.std():.3f}\")\n",
    "    ax.set_xlabel(\"Weight Value\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fa72e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"embedding_initialization_methods.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved initialization visualization to {OUTPUT_DIR / 'embedding_initialization_methods.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5478aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: Cosine Similarity Between Embeddings\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4: Cosine Similarity Between Embeddings\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings with higher dimension for better demonstration\n",
    "embedding_dim_large = 16\n",
    "emb_large = nn.Embedding(vocab_size, embedding_dim_large)\n",
    "nn.init.normal_(emb_large.weight, mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually adjust some embeddings to create semantic relationships\n",
    "with torch.no_grad():\n",
    "    # Make \"neural\" and \"network\" similar\n",
    "    base_vec = torch.randn(embedding_dim_large) * 0.02\n",
    "    emb_large.weight[word2idx[\"neural\"]] = base_vec + torch.randn(embedding_dim_large) * 0.005\n",
    "    emb_large.weight[word2idx[\"network\"]] = base_vec + torch.randn(embedding_dim_large) * 0.005\n",
    "\n",
    "    # Make \"machine\" and \"learning\" similar\n",
    "    base_vec2 = torch.randn(embedding_dim_large) * 0.02\n",
    "    emb_large.weight[word2idx[\"machine\"]] = base_vec2 + torch.randn(embedding_dim_large) * 0.005\n",
    "    emb_large.weight[word2idx[\"learning\"]] = base_vec2 + torch.randn(embedding_dim_large) * 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042245f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"\\nComputing cosine similarities between word embeddings:\")\n",
    "print(f\"Embedding dimension: {embedding_dim_large}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d8e72",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise similarities\n",
    "word_pairs = [\n",
    "    (\"neural\", \"network\"),\n",
    "    (\"machine\", \"learning\"),\n",
    "    (\"neural\", \"machine\"),\n",
    "    (\"hello\", \"world\"),\n",
    "    (\"hello\", \"learning\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc975aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'Word 1':<12} {'Word 2':<12} {'Cosine Similarity':<20} {'Interpretation'}\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word1, word2 in word_pairs:\n",
    "    emb1 = emb_large.weight[word2idx[word1]]\n",
    "    emb2 = emb_large.weight[word2idx[word2]]\n",
    "    sim = cosine_similarity(emb1, emb2)\n",
    "\n",
    "    if sim > 0.7:\n",
    "        interpretation = \"Very similar\"\n",
    "    elif sim > 0.3:\n",
    "        interpretation = \"Somewhat similar\"\n",
    "    elif sim > 0:\n",
    "        interpretation = \"Slightly similar\"\n",
    "    else:\n",
    "        interpretation = \"Different\"\n",
    "\n",
    "    print(f\"{word1:<12} {word2:<12} {sim:>18.4f}  {interpretation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1aa519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute full similarity matrix\n",
    "similarity_matrix = torch.zeros(vocab_size, vocab_size)\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        similarity_matrix[i, j] = cosine_similarity(\n",
    "            emb_large.weight[i],\n",
    "            emb_large.weight[j]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902013fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(similarity_matrix.numpy(), cmap='RdYlBu', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(vocab_size))\n",
    "ax.set_yticks(range(vocab_size))\n",
    "ax.set_xticklabels(vocab, rotation=45, ha='right')\n",
    "ax.set_yticklabels(vocab)\n",
    "ax.set_title(\"Cosine Similarity Matrix of Word Embeddings\", fontsize=14, fontweight='bold', pad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Cosine Similarity\", rotation=270, labelpad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcca334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text annotations\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        text = ax.text(j, i, f\"{similarity_matrix[i, j].item():.2f}\",\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b0d40",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"embedding_similarity_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved similarity matrix to {OUTPUT_DIR / 'embedding_similarity_matrix.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: 2D Visualization of Embedding Space\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 5: 2D Visualization of Embedding Space\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d58014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: PCA projection\n",
    "print(\"\\nMethod 1: PCA Projection\")\n",
    "embeddings_np = emb_large.weight.detach().numpy()\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94771b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  Original dimension: {embedding_dim_large}\")\n",
    "print(f\"  Projected dimension: 2\")\n",
    "print(f\"  Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"  Total variance explained: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd262799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Manual projection (take first 2 dimensions)\n",
    "print(\"\\nMethod 2: Manual Projection (first 2 dimensions)\")\n",
    "embeddings_manual = embeddings_np[:, :2]\n",
    "print(f\"  Simply taking first 2 dimensions of {embedding_dim_large}D space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b2a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973150b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization\n",
    "ax1.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=200, c='steelblue', alpha=0.6, edgecolors='black')\n",
    "for i, word in enumerate(vocab):\n",
    "    ax1.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                fontsize=11, fontweight='bold', ha='center', va='bottom',\n",
    "                xytext=(0, 5), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55234a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)\", fontsize=12)\n",
    "ax1.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)\", fontsize=12)\n",
    "ax1.set_title(\"PCA Projection of Embedding Space\", fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax1.axvline(x=0, color='k', linestyle='-', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fb3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual projection visualization\n",
    "ax2.scatter(embeddings_manual[:, 0], embeddings_manual[:, 1], s=200, c='coral', alpha=0.6, edgecolors='black')\n",
    "for i, word in enumerate(vocab):\n",
    "    ax2.annotate(word, (embeddings_manual[i, 0], embeddings_manual[i, 1]),\n",
    "                fontsize=11, fontweight='bold', ha='center', va='bottom',\n",
    "                xytext=(0, 5), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb492c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.set_xlabel(\"Dimension 1\", fontsize=12)\n",
    "ax2.set_ylabel(\"Dimension 2\", fontsize=12)\n",
    "ax2.set_title(\"Manual Projection (First 2 Dimensions)\", fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='k', linestyle='-', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701306e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"embedding_space_visualization.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved embedding space visualization to {OUTPUT_DIR / 'embedding_space_visualization.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92025a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional 3D visualization with trajectory showing semantic relationships\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10de5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA for 3D\n",
    "pca_3d = PCA(n_components=3)\n",
    "embeddings_3d = pca_3d.fit_transform(embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bddfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot points\n",
    "scatter = ax.scatter(embeddings_3d[:, 0], embeddings_3d[:, 1], embeddings_3d[:, 2],\n",
    "                     s=200, c=range(vocab_size), cmap='viridis', alpha=0.6, edgecolors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels\n",
    "for i, word in enumerate(vocab):\n",
    "    ax.text(embeddings_3d[i, 0], embeddings_3d[i, 1], embeddings_3d[i, 2],\n",
    "            word, fontsize=10, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw lines between related words\n",
    "related_pairs = [\n",
    "    (\"neural\", \"network\", 'red'),\n",
    "    (\"machine\", \"learning\", 'blue'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4978aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word1, word2, color in related_pairs:\n",
    "    idx1, idx2 = word2idx[word1], word2idx[word2]\n",
    "    ax.plot([embeddings_3d[idx1, 0], embeddings_3d[idx2, 0]],\n",
    "            [embeddings_3d[idx1, 1], embeddings_3d[idx2, 1]],\n",
    "            [embeddings_3d[idx1, 2], embeddings_3d[idx2, 2]],\n",
    "            color=color, linewidth=2, linestyle='--', alpha=0.7, label=f\"{word1}-{word2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xlabel(f\"PC1 ({pca_3d.explained_variance_ratio_[0]:.1%})\", fontsize=11)\n",
    "ax.set_ylabel(f\"PC2 ({pca_3d.explained_variance_ratio_[1]:.1%})\", fontsize=11)\n",
    "ax.set_zlabel(f\"PC3 ({pca_3d.explained_variance_ratio_[2]:.1%})\", fontsize=11)\n",
    "ax.set_title(\"3D PCA Projection of Embedding Space\", fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca6fc9d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"embedding_space_3d.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\u2713 Saved 3D embedding space to {OUTPUT_DIR / 'embedding_space_3d.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae5e61",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: Weight Tying (Input/Output Embeddings)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 6: Weight Tying (Input/Output Embeddings)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed84f90",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"Simple language model demonstrating weight tying.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tie_weights=True):\n",
    "        super().__init__()\n",
    "        self.tie_weights = tie_weights\n",
    "\n",
    "        # Input embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Simple hidden layer\n",
    "        self.hidden = nn.Linear(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Output projection back to embedding dimension\n",
    "        self.output_proj = nn.Linear(hidden_dim, embedding_dim)\n",
    "\n",
    "        # Output layer (vocabulary projection)\n",
    "        if tie_weights:\n",
    "            # Share weights between input embeddings and output layer\n",
    "            self.output = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "            self.output.weight = self.embedding.weight  # Weight tying!\n",
    "        else:\n",
    "            # Separate weights\n",
    "            self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        emb = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        hidden = F.relu(self.hidden(emb))  # (batch_size, seq_len, hidden_dim)\n",
    "        output_emb = self.output_proj(hidden)  # (batch_size, seq_len, embedding_dim)\n",
    "        logits = self.output(output_emb)  # (batch_size, seq_len, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58174a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models with and without weight tying\n",
    "vocab_size_demo = 1000\n",
    "embedding_dim_demo = 256\n",
    "hidden_dim_demo = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17445071",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tied = SimpleLanguageModel(vocab_size_demo, embedding_dim_demo, hidden_dim_demo, tie_weights=True)\n",
    "model_separate = SimpleLanguageModel(vocab_size_demo, embedding_dim_demo, hidden_dim_demo, tie_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d8909",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nWeight Tying Comparison:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Model with TIED weights:\")\n",
    "print(f\"  Total parameters: {model_tied.count_parameters():,}\")\n",
    "print(f\"  Input embedding is SAME as output layer weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af316e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nModel with SEPARATE weights:\")\n",
    "print(f\"  Total parameters: {model_separate.count_parameters():,}\")\n",
    "print(f\"  Input embedding is DIFFERENT from output layer weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_saved = model_separate.count_parameters() - model_tied.count_parameters()\n",
    "reduction_pct = (params_saved / model_separate.count_parameters()) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nParameter reduction: {params_saved:,} ({reduction_pct:.1f}%)\")\n",
    "print(f\"Saved parameters = vocab_size \u00d7 embedding_dim = {vocab_size_demo} \u00d7 {embedding_dim_demo} = {vocab_size_demo * embedding_dim_demo:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403abcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify weight tying\n",
    "with torch.no_grad():\n",
    "    test_input = torch.LongTensor([[0, 1, 2]])\n",
    "\n",
    "    # Check if weights are actually shared\n",
    "    emb_weight_id = id(model_tied.embedding.weight)\n",
    "    out_weight_id = id(model_tied.output.weight)\n",
    "\n",
    "    print(f\"\\nVerifying weight sharing:\")\n",
    "    print(f\"  Input embedding weight tensor ID: {emb_weight_id}\")\n",
    "    print(f\"  Output layer weight tensor ID: {out_weight_id}\")\n",
    "    print(f\"  Are they the same object? {emb_weight_id == out_weight_id}\")\n",
    "\n",
    "    # Modify embedding and verify output changes\n",
    "    original_weight = model_tied.embedding.weight[0].clone()\n",
    "    model_tied.embedding.weight[0] = torch.ones_like(model_tied.embedding.weight[0])\n",
    "\n",
    "    print(f\"\\nAfter modifying embedding weight[0]:\")\n",
    "    print(f\"  Embedding weight[0]: {model_tied.embedding.weight[0][:5]}...\")\n",
    "    print(f\"  Output weight[0]: {model_tied.output.weight[0][:5]}...\")\n",
    "    print(f\"  Are they identical? {torch.allclose(model_tied.embedding.weight[0], model_tied.output.weight[0])}\")\n",
    "\n",
    "    # Restore\n",
    "    model_tied.embedding.weight[0] = original_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize architecture\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cdfa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with tied weights\n",
    "ax1.text(0.5, 0.9, \"Input Tokens\", ha='center', va='center', fontsize=12,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "ax1.arrow(0.5, 0.85, 0, -0.1, head_width=0.05, head_length=0.03, fc='black', ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ed667",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1.text(0.5, 0.7, f\"Embedding\\n({vocab_size_demo} \u00d7 {embedding_dim_demo})\", ha='center', va='center', fontsize=11,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "ax1.arrow(0.5, 0.65, 0, -0.1, head_width=0.05, head_length=0.03, fc='black', ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd75b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1.text(0.5, 0.5, f\"Hidden Layer\\n({embedding_dim_demo} \u2192 {hidden_dim_demo})\", ha='center', va='center', fontsize=11,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "ax1.arrow(0.5, 0.45, 0, -0.1, head_width=0.05, head_length=0.03, fc='black', ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52843151",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1.text(0.5, 0.3, f\"Output Projection\\n({hidden_dim_demo} \u2192 {embedding_dim_demo})\", ha='center', va='center', fontsize=11,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "ax1.arrow(0.5, 0.25, 0, -0.1, head_width=0.05, head_length=0.03, fc='black', ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290972c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1.text(0.5, 0.1, f\"Output Layer (TIED)\\n({embedding_dim_demo} \u00d7 {vocab_size_demo})\", ha='center', va='center', fontsize=11,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightcoral', edgecolor='black', linewidth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw tie connection\n",
    "ax1.annotate('', xy=(0.7, 0.7), xytext=(0.7, 0.1),\n",
    "            arrowprops=dict(arrowstyle='<->', color='red', lw=3, linestyle='--'))\n",
    "ax1.text(0.85, 0.4, 'SHARED\\nWEIGHTS', ha='center', va='center', fontsize=10,\n",
    "         fontweight='bold', color='red', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title(\"Weight Tying (Fewer Parameters)\", fontsize=13, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ecd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with separate weights\n",
    "ax2.text(0.5, 0.9, \"Input Tokens\", ha='center', va='center', fontsize=12,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "ax2.arrow(0.5, 0.85, 0, -0.1, head_width=0.05, head_length=0.03, fc='black', ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.text(0.5, 0.7, f\"Embedding\\n({vocab_size_demo} \u00d7 {embedding_dim_demo})\", ha='center', va='center', fontsize=11,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "ax2.arrow(0.5, 0.65, 0, -0.1, head_width=0.05, head_length=0.03, fc='black', ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.text(0.5, 0.5, f\"Hidden Layer\\n({embedding_dim_demo} \u2192 {hidden_dim_demo})\", ha='center', va='center', fontsize=11,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "ax2.arrow(0.5, 0.45, 0, -0.1, head_width=0.05, head_length=0.03, fc='black', ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.text(0.5, 0.3, f\"Output Projection\\n({hidden_dim_demo} \u2192 {embedding_dim_demo})\", ha='center', va='center', fontsize=11,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "ax2.arrow(0.5, 0.25, 0, -0.1, head_width=0.05, head_length=0.03, fc='black', ec='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad03b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.text(0.5, 0.1, f\"Output Layer (SEPARATE)\\n({embedding_dim_demo} \u00d7 {vocab_size_demo})\", ha='center', va='center', fontsize=11,\n",
    "         bbox=dict(boxstyle='round', facecolor='plum', edgecolor='black', linewidth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.axis('off')\n",
    "ax2.set_title(\"Separate Weights (More Parameters)\", fontsize=13, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddacb29",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"weight_tying_architecture.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved weight tying visualization to {OUTPUT_DIR / 'weight_tying_architecture.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: How Embeddings Are Learned (Gradient Updates)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 7: How Embeddings Are Learned Through Backpropagation\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple embedding layer\n",
    "vocab_size_train = 5\n",
    "embedding_dim_train = 3\n",
    "emb_train = nn.Embedding(vocab_size_train, embedding_dim_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with small values for visibility\n",
    "with torch.no_grad():\n",
    "    emb_train.weight.fill_(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35627285",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSimple training example:\")\n",
    "print(f\"  Vocabulary size: {vocab_size_train}\")\n",
    "print(f\"  Embedding dimension: {embedding_dim_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379441cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nInitial embedding weights:\")\n",
    "print(emb_train.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple task: predict next word\n",
    "# Sequence: [0, 1] -> target: 2\n",
    "input_seq = torch.LongTensor([0, 1])\n",
    "target = torch.LongTensor([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31151bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear layer for prediction\n",
    "linear = nn.Linear(embedding_dim_train, vocab_size_train)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(list(emb_train.parameters()) + list(linear.parameters()), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6486e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTraining setup:\")\n",
    "print(f\"  Input sequence: {input_seq.tolist()}\")\n",
    "print(f\"  Target: {target.tolist()}\")\n",
    "print(f\"  Loss function: CrossEntropyLoss\")\n",
    "print(f\"  Optimizer: SGD with lr=0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store history for visualization\n",
    "history = {\n",
    "    'weights': [emb_train.weight.data.clone()],\n",
    "    'losses': [],\n",
    "    'gradients': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ffd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "print(f\"\\n{'Epoch':<8} {'Loss':<12} {'Gradient Norm':<20} {'Weight Change'}\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c11d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    embeddings = emb_train(input_seq)  # (2, 3)\n",
    "    # Simple aggregation: mean pooling\n",
    "    pooled = embeddings.mean(dim=0, keepdim=True)  # (1, 3)\n",
    "    logits = linear(pooled)  # (1, vocab_size)\n",
    "    loss = criterion(logits, target)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Store gradient before update\n",
    "    grad_norm = emb_train.weight.grad.norm().item()\n",
    "    history['gradients'].append(grad_norm)\n",
    "    history['losses'].append(loss.item())\n",
    "\n",
    "    # Calculate weight change\n",
    "    old_weight = emb_train.weight.data.clone()\n",
    "\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track changes\n",
    "    weight_change = (emb_train.weight.data - old_weight).norm().item()\n",
    "    history['weights'].append(emb_train.weight.data.clone())\n",
    "\n",
    "    print(f\"{epoch:<8} {loss.item():<12.6f} {grad_norm:<20.6f} {weight_change:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFinal embedding weights:\")\n",
    "print(emb_train.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38fc49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nKey observations:\")\n",
    "print(f\"  1. Only embeddings for tokens in input_seq [0, 1] receive gradients\")\n",
    "print(f\"  2. Embedding for token 2 (target) is NOT updated directly\")\n",
    "print(f\"  3. Embeddings change to minimize prediction error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which embeddings changed\n",
    "initial_weights = history['weights'][0]\n",
    "final_weights = history['weights'][-1]\n",
    "weight_changes = (final_weights - initial_weights).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab1f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nPer-token weight changes (L1 norm):\")\n",
    "for i in range(vocab_size_train):\n",
    "    change = weight_changes[i].sum().item()\n",
    "    status = \"\u2713 UPDATED\" if change > 0.01 else \"\u25cb unchanged\"\n",
    "    print(f\"  Token {i}: {change:.6f}  {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bafdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning dynamics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb29c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curve\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['losses'], marker='o', linewidth=2, markersize=6)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=11)\n",
    "ax.set_ylabel(\"Loss\", fontsize=11)\n",
    "ax.set_title(\"Training Loss Over Time\", fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient norm\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['gradients'], marker='s', linewidth=2, markersize=6, color='orange')\n",
    "ax.set_xlabel(\"Epoch\", fontsize=11)\n",
    "ax.set_ylabel(\"Gradient Norm\", fontsize=11)\n",
    "ax.set_title(\"Gradient Magnitude Over Time\", fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602716ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight evolution for each token\n",
    "ax = axes[1, 0]\n",
    "for token_id in range(vocab_size_train):\n",
    "    # Track first dimension of embedding for simplicity\n",
    "    weight_trajectory = [w[token_id, 0].item() for w in history['weights']]\n",
    "    linestyle = '-' if token_id in input_seq else '--'\n",
    "    alpha = 1.0 if token_id in input_seq else 0.4\n",
    "    label = f\"Token {token_id}\" + (\" (in input)\" if token_id in input_seq else \" (not in input)\")\n",
    "    ax.plot(weight_trajectory, marker='o', linewidth=2, linestyle=linestyle,\n",
    "            alpha=alpha, label=label, markersize=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xlabel(\"Epoch\", fontsize=11)\n",
    "ax.set_ylabel(\"Weight Value (dim 0)\", fontsize=11)\n",
    "ax.set_title(\"Embedding Weight Evolution (First Dimension)\", fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cafbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final weight heatmap\n",
    "ax = axes[1, 1]\n",
    "im = ax.imshow(final_weights.numpy(), cmap='coolwarm', aspect='auto')\n",
    "ax.set_xlabel(\"Embedding Dimension\", fontsize=11)\n",
    "ax.set_ylabel(\"Token ID\", fontsize=11)\n",
    "ax.set_title(\"Final Embedding Weights\", fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(range(embedding_dim_train))\n",
    "ax.set_yticks(range(vocab_size_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate which tokens were in input\n",
    "for token_id in range(vocab_size_train):\n",
    "    label = \"\u2190 in input\" if token_id in input_seq else \"\"\n",
    "    if label:\n",
    "        ax.text(embedding_dim_train + 0.1, token_id, label,\n",
    "                fontsize=9, va='center', fontweight='bold', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed34f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Weight Value\", rotation=270, labelpad=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513975b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"embedding_learning_dynamics.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved learning dynamics to {OUTPUT_DIR / 'embedding_learning_dynamics.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed gradient flow explanation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED GRADIENT FLOW EXPLANATION\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de60c680",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "During backpropagation:\n",
    "\n",
    "1. Forward Pass:\n",
    "   - Input tokens [0, 1] \u2192 Embedding lookup \u2192 Get embeddings E[0] and E[1]\n",
    "   - Mean pooling \u2192 Average embeddings\n",
    "   - Linear layer \u2192 Logits\n",
    "   - CrossEntropy loss with target token 2\n",
    "\n",
    "2. Backward Pass (Gradient Flow):\n",
    "\n",
    "   \u2202Loss/\u2202Logits \u2192 \u2202Logits/\u2202Linear_weights \u2192 \u2202Linear_output/\u2202Pooled\n",
    "   \u2192 \u2202Pooled/\u2202Embeddings \u2192 \u2202Embeddings/\u2202Embedding_weights\n",
    "\n",
    "   Key insight: Only E[0] and E[1] receive gradients!\n",
    "   - E[0].grad = \u2202Loss/\u2202E[0] \u2260 0  (was in input)\n",
    "   - E[1].grad = \u2202Loss/\u2202E[1] \u2260 0  (was in input)\n",
    "   - E[2].grad = 0  (target, but not in input)\n",
    "   - E[3].grad = 0  (not used)\n",
    "   - E[4].grad = 0  (not used)\n",
    "\n",
    "3. Weight Update:\n",
    "   - E[i] \u2190 E[i] - learning_rate \u00d7 \u2202Loss/\u2202E[i]\n",
    "   - Only E[0] and E[1] are updated\n",
    "   - This is why embedding layers are SPARSE updates\n",
    "\n",
    "4. Why This Matters:\n",
    "   - Embeddings learn from the contexts they appear in\n",
    "   - Rare words update less frequently (fewer gradient updates)\n",
    "   - Common words get more gradient updates\n",
    "   - This is a form of implicit regularization\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BONUS: Common Interview Questions\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMMON INTERVIEW QUESTIONS & ANSWERS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47953bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = [\n",
    "    (\"Q1: Why use embeddings instead of one-hot encoding?\",\n",
    "     \"\"\"A: Three main reasons:\n",
    "     1. EFFICIENCY: One-hot is sparse and requires vocab_size \u00d7 embedding_dim multiplications.\n",
    "        Embedding lookup is just indexing (O(1) operation).\n",
    "     2. MEMORY: One-hot stores vocab_size floats per token, embedding stores 1 integer.\n",
    "     3. LEARNING: Embeddings can capture semantic relationships. One-hot vectors are\n",
    "        orthogonal and don't capture any relationships.\"\"\"),\n",
    "\n",
    "    (\"Q2: What is weight tying and why use it?\",\n",
    "     \"\"\"A: Weight tying shares the embedding matrix between input embeddings and output\n",
    "     projection layer. Benefits:\n",
    "     1. Reduces parameters by vocab_size \u00d7 embedding_dim\n",
    "     2. For large vocab (50k) and embedding dim (768): saves 38M parameters!\n",
    "     3. Acts as regularization - forces consistency between input and output spaces\n",
    "     4. Used in GPT, BERT, and most modern transformers\"\"\"),\n",
    "\n",
    "    (\"Q3: How are embeddings initialized?\",\n",
    "     \"\"\"A: Common methods:\n",
    "     1. Random Uniform: U(-1/\u221ad, 1/\u221ad) - PyTorch default\n",
    "     2. Xavier/Glorot: U(-\u221a(6/(fan_in+fan_out)), \u221a(6/(fan_in+fan_out)))\n",
    "     3. Normal: N(0, 0.02\u00b2) - Used in BERT\n",
    "     4. Zero for special tokens (like padding)\n",
    "     Choice affects training stability and convergence speed.\"\"\"),\n",
    "\n",
    "    (\"Q4: Why do only some embeddings update during training?\",\n",
    "     \"\"\"A: Sparse gradients! Only embeddings that appear in the current batch receive\n",
    "     gradients through backpropagation. This means:\n",
    "     1. Rare words update slowly (fewer gradient updates)\n",
    "     2. Common words update frequently\n",
    "     3. This is actually beneficial - rare words need fewer updates\n",
    "     4. Can lead to cold-start problem for very rare words\"\"\"),\n",
    "\n",
    "    (\"Q5: What's the relationship between embedding dimension and model performance?\",\n",
    "     \"\"\"A: Trade-offs:\n",
    "     - Higher dimension: More expressiveness, but more parameters and slower training\n",
    "     - Lower dimension: Faster, but may not capture complex relationships\n",
    "     - Typical values: 128-256 (small models), 512-1024 (medium), 1024-4096 (large)\n",
    "     - Rule of thumb: embedding_dim \u2248 \u2074\u221avocabulary_size (but varies widely)\"\"\"),\n",
    "\n",
    "    (\"Q6: Can you explain the mathematical equivalence: Embedding[i] = OneHot[i] @ W?\",\n",
    "     f\"\"\"A: Mathematically identical but computationally different:\n",
    "\n",
    "     OneHot approach:\n",
    "       v = [0,0,1,0,0,...,0]  (vocab_size elements, mostly zeros)\n",
    "       result = v @ W         (vocab_size \u00d7 embedding_dim multiplication)\n",
    "\n",
    "     Embedding approach:\n",
    "       result = W[i,:]        (direct indexing, no multiplication)\n",
    "\n",
    "     Same result, but embedding is O(1) vs O(vocab_size \u00d7 embedding_dim)\n",
    "     For vocab=50k, embedding_dim=768: That's 38.4M operations saved per lookup!\"\"\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcaf094",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for i, (question, answer) in enumerate(qa_pairs, 1):\n",
    "    print(f\"\\n{question}\")\n",
    "    print(answer)\n",
    "    if i < len(qa_pairs):\n",
    "        print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY - KEY TAKEAWAYS FOR INTERVIEWS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298783f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "\u2713 Embeddings convert discrete tokens to continuous vectors\n",
    "\u2713 Embedding lookup = One-hot @ Weight matrix (but way more efficient)\n",
    "\u2713 Only embeddings of tokens in the batch receive gradient updates (sparse updates)\n",
    "\u2713 Weight tying reduces parameters dramatically (vocab_size \u00d7 embedding_dim savings)\n",
    "\u2713 Common initialization: Uniform, Xavier, Normal (BERT uses N(0, 0.02\u00b2))\n",
    "\u2713 Cosine similarity measures semantic similarity between embeddings\n",
    "\u2713 Higher dimension = more expressiveness but more parameters\n",
    "\u2713 Embeddings learn semantic relationships through backpropagation\n",
    "\u2713 Rare words update less frequently than common words\n",
    "\u2713 Embedding dimension is a key hyperparameter affecting model capacity\n",
    "\n",
    "Files saved to: {}/\n",
    "  \u2713 embedding_initialization_methods.png\n",
    "  \u2713 embedding_similarity_matrix.png\n",
    "  \u2713 embedding_space_visualization.png\n",
    "  \u2713 embedding_space_3d.png\n",
    "  \u2713 weight_tying_architecture.png\n",
    "  \u2713 embedding_learning_dynamics.png\n",
    "\"\"\".format(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bdcd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEMO COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}