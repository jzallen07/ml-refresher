{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Loss Functions Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0dbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loss Functions and Math for LLM Interviews - Educational Demo\n",
    "\n",
    "This comprehensive demo covers key mathematical concepts for LLM interviews:\n",
    "- Q25: Cross-Entropy Loss\n",
    "- Q29: Perplexity\n",
    "- Q30: KL Divergence\n",
    "- Q31: ReLU and activation functions\n",
    "- Bonus: Chain Rule and Backpropagation\n",
    "\n",
    "Each section implements concepts from scratch, then compares with PyTorch,\n",
    "and includes visualizations for deep understanding.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd906e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup visualization directory\n",
    "VIZ_DIR = Path(\"/Users/zack/dev/ml-refresher/data/interview_viz\")\n",
    "VIZ_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae1eaac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOSS FUNCTIONS AND MATH FOR LLM INTERVIEWS - COMPREHENSIVE DEMO\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e801761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: CROSS-ENTROPY LOSS (Q25)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 1: CROSS-ENTROPY LOSS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW CONTEXT:\n",
    "Cross-entropy is THE fundamental loss function for language models.\n",
    "It measures the difference between predicted probability distribution and\n",
    "the true distribution (one-hot encoded target).\n",
    "\n",
    "Mathematical Formula:\n",
    "    L = -\u2211 y_true * log(y_pred)\n",
    "\n",
    "For a single token prediction:\n",
    "    L = -log(p_target)\n",
    "\n",
    "where p_target is the predicted probability of the correct token.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a86399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_from_scratch(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implement cross-entropy loss from scratch.\n",
    "\n",
    "    Args:\n",
    "        logits: Raw model outputs (batch_size, num_classes)\n",
    "        targets: True class indices (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "        Average cross-entropy loss\n",
    "\n",
    "    Steps:\n",
    "        1. Convert logits to probabilities using softmax\n",
    "        2. Extract probability of correct class\n",
    "        3. Take negative log\n",
    "        4. Average over batch\n",
    "    \"\"\"\n",
    "    print(\"\\nCross-Entropy Calculation Steps:\")\n",
    "    print(f\"1. Input logits shape: {logits.shape}\")\n",
    "    print(f\"   Logits (raw scores):\\n{logits}\")\n",
    "\n",
    "    # Step 1: Apply softmax to convert logits to probabilities\n",
    "    # Softmax: exp(x_i) / sum(exp(x_j))\n",
    "    # We subtract max for numerical stability\n",
    "    max_logits = torch.max(logits, dim=1, keepdim=True)[0]\n",
    "    exp_logits = torch.exp(logits - max_logits)\n",
    "    probs = exp_logits / torch.sum(exp_logits, dim=1, keepdim=True)\n",
    "\n",
    "    print(f\"\\n2. After softmax (probabilities):\\n{probs}\")\n",
    "    print(f\"   Probabilities sum to 1: {torch.allclose(probs.sum(dim=1), torch.ones(probs.shape[0]))}\")\n",
    "\n",
    "    # Step 2: Extract probability of correct class for each sample\n",
    "    batch_size = logits.shape[0]\n",
    "    target_probs = probs[range(batch_size), targets]\n",
    "\n",
    "    print(f\"\\n3. Target classes: {targets}\")\n",
    "    print(f\"   Probabilities of correct classes: {target_probs}\")\n",
    "\n",
    "    # Step 3: Take negative log\n",
    "    neg_log_probs = -torch.log(target_probs)\n",
    "\n",
    "    print(f\"\\n4. Negative log probabilities: {neg_log_probs}\")\n",
    "\n",
    "    # Step 4: Average over batch\n",
    "    loss = torch.mean(neg_log_probs)\n",
    "\n",
    "    print(f\"\\n5. Average loss: {loss.item():.4f}\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predicting next token in language modeling\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Example: Language Model Token Prediction\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary: [\"the\", \"cat\", \"sat\", \"mat\"]\n",
    "vocab = [\"the\", \"cat\", \"sat\", \"mat\"]\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model predicts distribution over vocabulary for 3 positions\n",
    "# Batch size = 3 (3 different positions)\n",
    "logits = torch.tensor([\n",
    "    [2.0, 1.0, 0.5, 0.2],  # Position 1: model predicts \"the\" strongly\n",
    "    [0.3, 3.0, 0.4, 0.1],  # Position 2: model predicts \"cat\" strongly\n",
    "    [0.2, 0.1, 2.5, 1.0],  # Position 3: model predicts \"sat\" strongly\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8991a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True next tokens: [\"the\", \"cat\", \"sat\"] -> indices [0, 1, 2]\n",
    "targets = torch.tensor([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f612d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nVocabulary: {vocab}\")\n",
    "print(f\"Predictions for 3 positions in sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate from scratch\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FROM SCRATCH IMPLEMENTATION\")\n",
    "print(\"=\"*40)\n",
    "loss_scratch = cross_entropy_from_scratch(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with PyTorch\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"PYTORCH IMPLEMENTATION\")\n",
    "print(\"=\"*40)\n",
    "loss_pytorch = F.cross_entropy(logits, targets)\n",
    "print(f\"\\nPyTorch cross_entropy: {loss_pytorch.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907de8d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"\\nDifference: {abs(loss_scratch - loss_pytorch).item():.10f}\")\n",
    "print(\"\u2713 Implementations match!\" if torch.allclose(loss_scratch, loss_pytorch) else \"\u2717 Mismatch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-entropy behavior\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Visualizing Cross-Entropy Loss Behavior\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102896e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left plot: Loss vs predicted probability\n",
    "probs_range = np.linspace(0.01, 1.0, 100)\n",
    "ce_loss = -np.log(probs_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes[0].plot(probs_range, ce_loss, linewidth=2, color='blue')\n",
    "axes[0].set_xlabel('Predicted Probability of Correct Class', fontsize=12)\n",
    "axes[0].set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "axes[0].set_title('Cross-Entropy Loss vs Predicted Probability', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(x=1.0, color='r', linestyle='--', alpha=0.3, label='Perfect prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3874c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add annotations\n",
    "axes[0].annotate('High penalty for\\nlow confidence', xy=(0.1, -np.log(0.1)),\n",
    "                xytext=(0.3, 3), fontsize=10,\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=1.5))\n",
    "axes[0].annotate('Low penalty for\\nhigh confidence', xy=(0.9, -np.log(0.9)),\n",
    "                xytext=(0.7, 1), fontsize=10,\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243430c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right plot: Example predictions\n",
    "example_probs = [0.9, 0.7, 0.5, 0.3, 0.1]\n",
    "example_losses = [-np.log(p) for p in example_probs]\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(example_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d533df",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = axes[1].bar(range(len(example_probs)), example_losses, color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_xlabel('Predicted Probability', fontsize=12)\n",
    "axes[1].set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "axes[1].set_title('Loss for Different Confidence Levels', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(range(len(example_probs)))\n",
    "axes[1].set_xticklabels([f'{p:.1f}' for p in example_probs])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae673c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add value labels on bars\n",
    "for i, (bar, loss, prob) in enumerate(zip(bars, example_losses, example_probs)):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{loss:.2f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffe4aa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"01_cross_entropy_behavior.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\u2713 Saved visualization: {VIZ_DIR / '01_cross_entropy_behavior.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd63200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: PERPLEXITY (Q29)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 2: PERPLEXITY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2989773",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW CONTEXT:\n",
    "Perplexity is the primary evaluation metric for language models.\n",
    "It measures how \"surprised\" the model is by the test data.\n",
    "\n",
    "Mathematical Definition:\n",
    "    Perplexity = exp(average cross-entropy loss)\n",
    "    PPL = exp(L) where L = -(1/N) * \u2211 log P(w_i)\n",
    "\n",
    "Interpretation:\n",
    "- Lower perplexity = better model\n",
    "- PPL of K means model is as \"confused\" as if it had to choose uniformly\n",
    "  from K possibilities at each step\n",
    "- PPL = 1 means perfect prediction (100% confidence on correct tokens)\n",
    "- PPL = vocab_size means random guessing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a632649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(logits: torch.Tensor, targets: torch.Tensor) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate perplexity from logits and targets.\n",
    "\n",
    "    Returns:\n",
    "        (perplexity, cross_entropy_loss)\n",
    "    \"\"\"\n",
    "    # Calculate cross-entropy loss\n",
    "    ce_loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    # Perplexity is exp of the loss\n",
    "    perplexity = torch.exp(ce_loss)\n",
    "\n",
    "    return perplexity.item(), ce_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Example: Comparing Model Quality with Perplexity\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073df8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Two models predicting the sentence \"the cat sat\"\n",
    "vocab = [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"ran\", \"jumped\"]\n",
    "vocab_size = len(vocab)\n",
    "targets = torch.tensor([0, 1, 2])  # \"the cat sat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e71191",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Target sequence: {[vocab[i] for i in targets]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ee008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A: Good model (confident and correct)\n",
    "logits_good = torch.tensor([\n",
    "    [3.0, 0.5, 0.3, 0.2, 0.1, 0.1, 0.1, 0.1],  # Strongly predicts \"the\"\n",
    "    [0.2, 3.5, 0.4, 0.2, 0.1, 0.1, 0.1, 0.1],  # Strongly predicts \"cat\"\n",
    "    [0.1, 0.2, 3.2, 0.3, 0.2, 0.1, 0.1, 0.1],  # Strongly predicts \"sat\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model B: Mediocre model (less confident)\n",
    "logits_mediocre = torch.tensor([\n",
    "    [1.5, 1.0, 1.0, 0.8, 0.5, 0.5, 0.4, 0.3],  # Weakly predicts \"the\"\n",
    "    [0.8, 1.8, 1.0, 0.7, 0.6, 0.5, 0.4, 0.3],  # Weakly predicts \"cat\"\n",
    "    [0.7, 0.9, 1.6, 1.0, 0.8, 0.6, 0.5, 0.4],  # Weakly predicts \"sat\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model C: Bad model (nearly random)\n",
    "logits_bad = torch.tensor([\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],  # Nearly uniform\n",
    "    [1.1, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0],  # Nearly uniform\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],  # Nearly uniform\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beabeb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"Good Model\", logits_good),\n",
    "    (\"Mediocre Model\", logits_mediocre),\n",
    "    (\"Bad Model (Random)\", logits_bad),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, logits in models:\n",
    "    ppl, ce = calculate_perplexity(logits, targets)\n",
    "    results.append((name, ppl, ce))\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Cross-Entropy Loss: {ce:.4f}\")\n",
    "    print(f\"  Perplexity: {ppl:.4f}\")\n",
    "\n",
    "    # Show probability distribution for first prediction\n",
    "    probs = F.softmax(logits[0], dim=0)\n",
    "    print(f\"  Probability of correct token 'the': {probs[0]:.4f}\")\n",
    "    print(f\"  Top-3 predicted tokens: {[(vocab[i], probs[i].item()) for i in torch.topk(probs, 3).indices]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b83070",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*40}\")\n",
    "print(\"INTERPRETATION:\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Random baseline perplexity \u2248 {vocab_size:.1f}\")\n",
    "print(f\"\\nGood model: Low perplexity ({results[0][1]:.2f}) = confident, accurate predictions\")\n",
    "print(f\"Mediocre model: Medium perplexity ({results[1][1]:.2f}) = less confident\")\n",
    "print(f\"Bad model: High perplexity ({results[2][1]:.2f}) = nearly random guessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize perplexity comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4af9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left plot: Perplexity comparison\n",
    "names = [r[0] for r in results]\n",
    "ppls = [r[1] for r in results]\n",
    "colors = ['green', 'orange', 'red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = axes[0].bar(range(len(names)), ppls, color=colors, edgecolor='black', linewidth=2, alpha=0.7)\n",
    "axes[0].set_ylabel('Perplexity', fontsize=12)\n",
    "axes[0].set_title('Perplexity Comparison Across Models', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(range(len(names)))\n",
    "axes[0].set_xticklabels(names, rotation=15, ha='right')\n",
    "axes[0].axhline(y=vocab_size, color='purple', linestyle='--', linewidth=2,\n",
    "                label=f'Random baseline ({vocab_size})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c3712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add value labels\n",
    "for bar, ppl in zip(bars, ppls):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{ppl:.2f}',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right plot: Perplexity vs loss relationship\n",
    "ce_range = np.linspace(0, 3, 100)\n",
    "ppl_range = np.exp(ce_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes[1].plot(ce_range, ppl_range, linewidth=2, color='blue')\n",
    "axes[1].set_xlabel('Cross-Entropy Loss', fontsize=12)\n",
    "axes[1].set_ylabel('Perplexity', fontsize=12)\n",
    "axes[1].set_title('Perplexity = exp(Cross-Entropy Loss)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark our models on the curve\n",
    "for name, ppl, ce in results:\n",
    "    color = {'Good Model': 'green', 'Mediocre Model': 'orange', 'Bad Model (Random)': 'red'}[name]\n",
    "    axes[1].plot(ce, ppl, 'o', markersize=12, color=color, label=name, markeredgecolor='black', markeredgewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa04a4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"02_perplexity_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved visualization: {VIZ_DIR / '02_perplexity_comparison.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea088b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: KL DIVERGENCE (Q30)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3: KL DIVERGENCE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eec3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW CONTEXT:\n",
    "KL Divergence measures how one probability distribution differs from another.\n",
    "Critical for:\n",
    "- Knowledge distillation (student mimics teacher)\n",
    "- Variational inference in VAEs\n",
    "- Policy optimization in RL (RLHF for LLMs)\n",
    "\n",
    "Mathematical Definition:\n",
    "    KL(P || Q) = \u2211 P(x) * log(P(x) / Q(x))\n",
    "\n",
    "Properties:\n",
    "- Always non-negative: KL(P || Q) \u2265 0\n",
    "- Zero only when P = Q (distributions are identical)\n",
    "- NOT symmetric: KL(P || Q) \u2260 KL(Q || P)\n",
    "- NOT a distance metric (doesn't satisfy triangle inequality)\n",
    "\n",
    "Cross-entropy connection:\n",
    "    H(P, Q) = H(P) + KL(P || Q)\n",
    "    where H(P) is entropy of P, H(P, Q) is cross-entropy\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ba83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_from_scratch(p: torch.Tensor, q: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate KL divergence from scratch: KL(P || Q).\n",
    "\n",
    "    Args:\n",
    "        p: True distribution (batch_size, num_classes)\n",
    "        q: Approximate distribution (batch_size, num_classes)\n",
    "\n",
    "    Returns:\n",
    "        KL divergence\n",
    "    \"\"\"\n",
    "    print(\"\\nKL Divergence Calculation Steps:\")\n",
    "    print(f\"Distribution P (true):\\n{p}\")\n",
    "    print(f\"Distribution Q (approximate):\\n{q}\")\n",
    "\n",
    "    # KL(P || Q) = sum(P * log(P/Q))\n",
    "    # = sum(P * (log(P) - log(Q)))\n",
    "\n",
    "    # Add small epsilon for numerical stability\n",
    "    eps = 1e-10\n",
    "    p_safe = torch.clamp(p, min=eps)\n",
    "    q_safe = torch.clamp(q, min=eps)\n",
    "\n",
    "    # Calculate log ratio\n",
    "    log_ratio = torch.log(p_safe) - torch.log(q_safe)\n",
    "    print(f\"\\nlog(P/Q):\\n{log_ratio}\")\n",
    "\n",
    "    # Weight by P and sum\n",
    "    kl = torch.sum(p * log_ratio, dim=1)\n",
    "    print(f\"\\nKL divergence per sample: {kl}\")\n",
    "\n",
    "    # Average over batch\n",
    "    kl_mean = torch.mean(kl)\n",
    "    print(f\"Average KL divergence: {kl_mean.item():.6f}\")\n",
    "\n",
    "    return kl_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Example: Knowledge Distillation (Student Learning from Teacher)\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher model (large, confident)\n",
    "teacher_logits = torch.tensor([\n",
    "    [4.0, 1.0, 0.5, 0.2],  # Very confident about class 0\n",
    "    [0.3, 3.5, 0.8, 0.4],  # Very confident about class 1\n",
    "])\n",
    "teacher_probs = F.softmax(teacher_logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4cbbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student model (smaller, learning)\n",
    "student_logits_good = torch.tensor([\n",
    "    [3.5, 1.2, 0.6, 0.3],  # Close to teacher\n",
    "    [0.4, 3.2, 0.9, 0.5],  # Close to teacher\n",
    "])\n",
    "student_probs_good = F.softmax(student_logits_good, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f830dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logits_bad = torch.tensor([\n",
    "    [2.0, 2.0, 1.0, 1.0],  # Far from teacher (more uniform)\n",
    "    [1.0, 2.0, 1.5, 1.2],  # Far from teacher\n",
    "])\n",
    "student_probs_bad = F.softmax(student_logits_bad, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef1a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Teacher's confident predictions:\")\n",
    "print(f\"Sample 1: {teacher_probs[0]}\")\n",
    "print(f\"Sample 2: {teacher_probs[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95783ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Good Student (close to teacher)\")\n",
    "print(\"=\"*40)\n",
    "kl_good_scratch = kl_divergence_from_scratch(teacher_probs, student_probs_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Bad Student (far from teacher)\")\n",
    "print(\"=\"*40)\n",
    "kl_bad_scratch = kl_divergence_from_scratch(teacher_probs, student_probs_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c73c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"PyTorch Implementation\")\n",
    "print(\"=\"*40)\n",
    "kl_good_pytorch = F.kl_div(student_probs_good.log(), teacher_probs, reduction='batchmean')\n",
    "kl_bad_pytorch = F.kl_div(student_probs_bad.log(), teacher_probs, reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14233bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nGood student KL: {kl_good_pytorch.item():.6f}\")\n",
    "print(f\"Bad student KL: {kl_bad_pytorch.item():.6f}\")\n",
    "print(f\"\\nDifference (good): {abs(kl_good_scratch - kl_good_pytorch).item():.10f}\")\n",
    "print(f\"Difference (bad): {abs(kl_bad_scratch - kl_bad_pytorch).item():.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0bf37",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Lower KL divergence ({kl_good_pytorch.item():.4f}) = student closely mimics teacher\")\n",
    "print(f\"Higher KL divergence ({kl_bad_pytorch.item():.4f}) = student predictions differ significantly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate asymmetry of KL divergence\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Demonstrating KL Divergence Asymmetry\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = torch.tensor([[0.7, 0.2, 0.1]])\n",
    "p2 = torch.tensor([[0.3, 0.5, 0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_p1_p2 = F.kl_div(p2.log(), p1, reduction='batchmean')\n",
    "kl_p2_p1 = F.kl_div(p1.log(), p2, reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80646fab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"Distribution P1: {p1[0]}\")\n",
    "print(f\"Distribution P2: {p2[0]}\")\n",
    "print(f\"\\nKL(P1 || P2) = {kl_p1_p2.item():.6f}\")\n",
    "print(f\"KL(P2 || P1) = {kl_p2_p1.item():.6f}\")\n",
    "print(f\"\\nThese are NOT equal! KL divergence is asymmetric.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd20001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KL divergence\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Teacher vs Students probability distributions\n",
    "x = np.arange(4)\n",
    "width = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = axes[0, 0]\n",
    "ax.bar(x - width, teacher_probs[0].numpy(), width, label='Teacher', color='blue', alpha=0.7, edgecolor='black')\n",
    "ax.bar(x, student_probs_good[0].numpy(), width, label='Good Student', color='green', alpha=0.7, edgecolor='black')\n",
    "ax.bar(x + width, student_probs_bad[0].numpy(), width, label='Bad Student', color='red', alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Class', fontsize=11)\n",
    "ax.set_ylabel('Probability', fontsize=11)\n",
    "ax.set_title('Knowledge Distillation: Distribution Comparison (Sample 1)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: KL divergence comparison\n",
    "ax = axes[0, 1]\n",
    "kl_values = [kl_good_pytorch.item(), kl_bad_pytorch.item()]\n",
    "colors = ['green', 'red']\n",
    "bars = ax.bar(['Good Student', 'Bad Student'], kl_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('KL Divergence from Teacher', fontsize=11)\n",
    "ax.set_title('KL Divergence: Measuring Student-Teacher Alignment', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee211cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bar, kl in zip(bars, kl_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{kl:.4f}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: KL divergence heatmap\n",
    "ax = axes[1, 0]\n",
    "# Create a grid of distributions\n",
    "n_points = 20\n",
    "probs_grid = []\n",
    "for i in range(n_points):\n",
    "    for j in range(n_points - i):\n",
    "        k = n_points - i - j\n",
    "        if k >= 0:\n",
    "            # Three-class distribution\n",
    "            p1 = i / n_points\n",
    "            p2 = j / n_points\n",
    "            p3 = k / n_points\n",
    "            if abs(p1 + p2 + p3 - 1.0) < 0.01:  # Valid probability distribution\n",
    "                probs_grid.append([p1, p2, p3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_grid = torch.tensor(probs_grid)\n",
    "reference = torch.tensor([[0.5, 0.3, 0.2]])  # Reference distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75989fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate KL for each point\n",
    "kl_grid = []\n",
    "for p in probs_grid:\n",
    "    kl = F.kl_div(p.unsqueeze(0).log(), reference, reduction='batchmean')\n",
    "    kl_grid.append(kl.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter\n",
    "scatter = ax.scatter(probs_grid[:, 0], probs_grid[:, 1], c=kl_grid,\n",
    "                    cmap='RdYlGn_r', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax.plot(reference[0, 0], reference[0, 1], 'b*', markersize=20,\n",
    "        label='Reference', markeredgecolor='black', markeredgewidth=2)\n",
    "ax.set_xlabel('P(class 0)', fontsize=11)\n",
    "ax.set_ylabel('P(class 1)', fontsize=11)\n",
    "ax.set_title('KL Divergence Landscape (3-class case)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "plt.colorbar(scatter, ax=ax, label='KL Divergence')\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e815e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Asymmetry demonstration\n",
    "ax = axes[1, 1]\n",
    "# Generate pairs of distributions\n",
    "dist_pairs = []\n",
    "kl_forward = []\n",
    "kl_reverse = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    alpha = i / 19  # 0 to 1\n",
    "    p = torch.tensor([[0.8, 0.15, 0.05]])\n",
    "    q = torch.tensor([[0.8 * (1-alpha) + 0.3 * alpha,\n",
    "                      0.15 * (1-alpha) + 0.5 * alpha,\n",
    "                      0.05 * (1-alpha) + 0.2 * alpha]])\n",
    "\n",
    "    dist_pairs.append(alpha)\n",
    "    kl_forward.append(F.kl_div(q.log(), p, reduction='batchmean').item())\n",
    "    kl_reverse.append(F.kl_div(p.log(), q, reduction='batchmean').item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2727d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(dist_pairs, kl_forward, 'o-', label='KL(P || Q)', linewidth=2, markersize=6, color='blue')\n",
    "ax.plot(dist_pairs, kl_reverse, 's-', label='KL(Q || P)', linewidth=2, markersize=6, color='red')\n",
    "ax.set_xlabel('Distribution Difference (\u03b1)', fontsize=11)\n",
    "ax.set_ylabel('KL Divergence', fontsize=11)\n",
    "ax.set_title('KL Divergence Asymmetry', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965da82e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"03_kl_divergence_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved visualization: {VIZ_DIR / '03_kl_divergence_analysis.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: ACTIVATION FUNCTIONS - ReLU AND VARIANTS (Q31)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4: ACTIVATION FUNCTIONS - ReLU AND VARIANTS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8627b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW CONTEXT:\n",
    "Activation functions introduce non-linearity, enabling neural networks\n",
    "to learn complex patterns. Without them, deep networks would collapse\n",
    "to a single linear transformation.\n",
    "\n",
    "Key Activation Functions for LLMs:\n",
    "\n",
    "1. ReLU (Rectified Linear Unit):\n",
    "   f(x) = max(0, x)\n",
    "   f'(x) = 1 if x > 0, else 0\n",
    "\n",
    "   Pros: Simple, fast, helps with vanishing gradients\n",
    "   Cons: \"Dying ReLU\" problem (neurons stuck at 0)\n",
    "\n",
    "2. Leaky ReLU:\n",
    "   f(x) = max(\u03b1x, x) where \u03b1 = 0.01 typically\n",
    "   f'(x) = 1 if x > 0, else \u03b1\n",
    "\n",
    "   Pros: Fixes dying ReLU, allows small negative values\n",
    "   Cons: Extra hyperparameter\n",
    "\n",
    "3. GELU (Gaussian Error Linear Unit):\n",
    "   f(x) = x * \u03a6(x) where \u03a6 is CDF of standard normal\n",
    "   Approximation: f(x) = 0.5 * x * (1 + tanh(\u221a(2/\u03c0) * (x + 0.044715 * x\u00b3)))\n",
    "\n",
    "   Pros: Smooth, probabilistic interpretation, used in BERT, GPT\n",
    "   Cons: More computationally expensive\n",
    "\n",
    "4. SwiGLU (Swish-Gated Linear Unit):\n",
    "   Used in modern LLMs (LLaMA, PaLM)\n",
    "   Combines Swish activation with gating\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11535d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_from_scratch(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"ReLU: max(0, x)\"\"\"\n",
    "    return torch.maximum(torch.zeros_like(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75230643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"ReLU derivative: 1 if x > 0, else 0\"\"\"\n",
    "    return (x > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ee9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_from_scratch(x: torch.Tensor, alpha: float = 0.01) -> torch.Tensor:\n",
    "    \"\"\"Leaky ReLU: max(\u03b1x, x)\"\"\"\n",
    "    return torch.maximum(alpha * x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e57d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_derivative(x: torch.Tensor, alpha: float = 0.01) -> torch.Tensor:\n",
    "    \"\"\"Leaky ReLU derivative: 1 if x > 0, else \u03b1\"\"\"\n",
    "    return torch.where(x > 0, torch.ones_like(x), torch.full_like(x, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d66ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu_from_scratch(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    GELU approximation used in practice.\n",
    "    f(x) = 0.5 * x * (1 + tanh(\u221a(2/\u03c0) * (x + 0.044715 * x\u00b3)))\n",
    "    \"\"\"\n",
    "    sqrt_2_over_pi = math.sqrt(2.0 / math.pi)\n",
    "    return 0.5 * x * (1.0 + torch.tanh(sqrt_2_over_pi * (x + 0.044715 * x**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu_derivative(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    GELU derivative (approximate).\n",
    "    This is a simplified version for demonstration.\n",
    "    \"\"\"\n",
    "    sqrt_2_over_pi = math.sqrt(2.0 / math.pi)\n",
    "    tanh_arg = sqrt_2_over_pi * (x + 0.044715 * x**3)\n",
    "    tanh_out = torch.tanh(tanh_arg)\n",
    "\n",
    "    # Derivative of tanh argument\n",
    "    dtanh_arg = sqrt_2_over_pi * (1 + 3 * 0.044715 * x**2)\n",
    "\n",
    "    # Chain rule\n",
    "    cdf_approx = 0.5 * (1.0 + tanh_out)\n",
    "    pdf_approx = 0.5 * x * (1 - tanh_out**2) * dtanh_arg\n",
    "\n",
    "    return cdf_approx + pdf_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Comparing Activation Functions\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test values\n",
    "x_test = torch.tensor([-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input values:\", x_test.numpy())\n",
    "print(\"\\nReLU outputs:\", relu_from_scratch(x_test).numpy())\n",
    "print(\"ReLU (PyTorch):\", F.relu(x_test).numpy())\n",
    "print(\"Match:\", torch.allclose(relu_from_scratch(x_test), F.relu(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dfbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLeaky ReLU outputs:\", leaky_relu_from_scratch(x_test).numpy())\n",
    "print(\"Leaky ReLU (PyTorch):\", F.leaky_relu(x_test, 0.01).numpy())\n",
    "print(\"Match:\", torch.allclose(leaky_relu_from_scratch(x_test), F.leaky_relu(x_test, 0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57655c68",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\nGELU outputs:\", gelu_from_scratch(x_test).numpy())\n",
    "print(\"GELU (PyTorch):\", F.gelu(x_test).numpy())\n",
    "print(\"Match (approximate):\", torch.allclose(gelu_from_scratch(x_test), F.gelu(x_test), atol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions and their derivatives\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Visualizing Activation Functions and Derivatives\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = torch.linspace(-3, 3, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55204694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate activations\n",
    "relu_out = relu_from_scratch(x_range)\n",
    "leaky_relu_out = leaky_relu_from_scratch(x_range)\n",
    "gelu_out = gelu_from_scratch(x_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate derivatives\n",
    "relu_grad = relu_derivative(x_range)\n",
    "leaky_relu_grad = leaky_relu_derivative(x_range)\n",
    "gelu_grad = gelu_derivative(x_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f218eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row 1: ReLU\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(x_range, relu_out, linewidth=2.5, color='blue', label='ReLU')\n",
    "ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.set_xlabel('x', fontsize=11)\n",
    "ax1.set_ylabel('f(x)', fontsize=11)\n",
    "ax1.set_title('ReLU: max(0, x)', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2081adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(x_range, relu_grad, linewidth=2.5, color='red', label=\"ReLU'\")\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.set_xlabel('x', fontsize=11)\n",
    "ax2.set_ylabel(\"f'(x)\", fontsize=11)\n",
    "ax2.set_title(\"ReLU Derivative\", fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "ax2.set_ylim([-0.2, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfd369",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.text(0.5, 0.7, 'ReLU Properties:', ha='center', fontsize=12, fontweight='bold', transform=ax3.transAxes)\n",
    "ax3.text(0.5, 0.55, '\u2713 Simple and fast', ha='center', fontsize=10, transform=ax3.transAxes)\n",
    "ax3.text(0.5, 0.45, '\u2713 No vanishing gradient (x > 0)', ha='center', fontsize=10, transform=ax3.transAxes)\n",
    "ax3.text(0.5, 0.35, '\u2717 Dying ReLU problem', ha='center', fontsize=10, transform=ax3.transAxes, color='red')\n",
    "ax3.text(0.5, 0.25, '\u2717 Not differentiable at 0', ha='center', fontsize=10, transform=ax3.transAxes, color='red')\n",
    "ax3.text(0.5, 0.1, 'Used in: Earlier CNNs, some RNNs', ha='center', fontsize=9, style='italic', transform=ax3.transAxes)\n",
    "ax3.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row 2: Leaky ReLU\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.plot(x_range, leaky_relu_out, linewidth=2.5, color='green', label='Leaky ReLU (\u03b1=0.01)')\n",
    "ax4.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax4.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax4.set_xlabel('x', fontsize=11)\n",
    "ax4.set_ylabel('f(x)', fontsize=11)\n",
    "ax4.set_title('Leaky ReLU: max(\u03b1x, x)', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.plot(x_range, leaky_relu_grad, linewidth=2.5, color='darkgreen', label=\"Leaky ReLU'\")\n",
    "ax5.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax5.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax5.set_xlabel('x', fontsize=11)\n",
    "ax5.set_ylabel(\"f'(x)\", fontsize=11)\n",
    "ax5.set_title(\"Leaky ReLU Derivative\", fontsize=12, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "ax5.legend()\n",
    "ax5.set_ylim([-0.2, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.text(0.5, 0.7, 'Leaky ReLU Properties:', ha='center', fontsize=12, fontweight='bold', transform=ax6.transAxes)\n",
    "ax6.text(0.5, 0.55, '\u2713 Fixes dying ReLU', ha='center', fontsize=10, transform=ax6.transAxes)\n",
    "ax6.text(0.5, 0.45, '\u2713 Allows negative values', ha='center', fontsize=10, transform=ax6.transAxes)\n",
    "ax6.text(0.5, 0.35, '\u2713 Better gradient flow', ha='center', fontsize=10, transform=ax6.transAxes)\n",
    "ax6.text(0.5, 0.25, '~ Extra hyperparameter \u03b1', ha='center', fontsize=10, transform=ax6.transAxes, color='orange')\n",
    "ax6.text(0.5, 0.1, 'Used in: Various architectures', ha='center', fontsize=9, style='italic', transform=ax6.transAxes)\n",
    "ax6.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b65eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row 3: GELU\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "ax7.plot(x_range, gelu_out, linewidth=2.5, color='purple', label='GELU')\n",
    "ax7.plot(x_range, x_range, '--', linewidth=1.5, color='gray', alpha=0.5, label='Identity')\n",
    "ax7.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax7.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax7.set_xlabel('x', fontsize=11)\n",
    "ax7.set_ylabel('f(x)', fontsize=11)\n",
    "ax7.set_title('GELU: x\u00b7\u03a6(x)', fontsize=12, fontweight='bold')\n",
    "ax7.grid(True, alpha=0.3)\n",
    "ax7.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "ax8.plot(x_range, gelu_grad, linewidth=2.5, color='darkviolet', label=\"GELU'\")\n",
    "ax8.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax8.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax8.set_xlabel('x', fontsize=11)\n",
    "ax8.set_ylabel(\"f'(x)\", fontsize=11)\n",
    "ax8.set_title(\"GELU Derivative\", fontsize=12, fontweight='bold')\n",
    "ax8.grid(True, alpha=0.3)\n",
    "ax8.legend()\n",
    "ax8.set_ylim([-0.2, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc4dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "ax9.text(0.5, 0.7, 'GELU Properties:', ha='center', fontsize=12, fontweight='bold', transform=ax9.transAxes)\n",
    "ax9.text(0.5, 0.55, '\u2713 Smooth (differentiable)', ha='center', fontsize=10, transform=ax9.transAxes)\n",
    "ax9.text(0.5, 0.45, '\u2713 Probabilistic interpretation', ha='center', fontsize=10, transform=ax9.transAxes)\n",
    "ax9.text(0.5, 0.35, '\u2713 Better empirical performance', ha='center', fontsize=10, transform=ax9.transAxes)\n",
    "ax9.text(0.5, 0.25, '~ More expensive to compute', ha='center', fontsize=10, transform=ax9.transAxes, color='orange')\n",
    "ax9.text(0.5, 0.1, 'Used in: BERT, GPT, most modern LLMs', ha='center', fontsize=9, style='italic', transform=ax9.transAxes)\n",
    "ax9.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(VIZ_DIR / \"04_activation_functions_comprehensive.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\u2713 Saved visualization: {VIZ_DIR / '04_activation_functions_comprehensive.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7de2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All activations together\n",
    "axes[0].plot(x_range, relu_out, linewidth=2, label='ReLU', color='blue')\n",
    "axes[0].plot(x_range, leaky_relu_out, linewidth=2, label='Leaky ReLU', color='green')\n",
    "axes[0].plot(x_range, gelu_out, linewidth=2, label='GELU', color='purple')\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('f(x)', fontsize=12)\n",
    "axes[0].set_title('Activation Functions Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a96386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All derivatives together\n",
    "axes[1].plot(x_range, relu_grad, linewidth=2, label=\"ReLU'\", color='blue')\n",
    "axes[1].plot(x_range, leaky_relu_grad, linewidth=2, label=\"Leaky ReLU'\", color='green')\n",
    "axes[1].plot(x_range, gelu_grad, linewidth=2, label=\"GELU'\", color='purple')\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel(\"f'(x)\", fontsize=12)\n",
    "axes[1].set_title('Derivatives Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([-0.2, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0259bc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"05_activation_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\u2713 Saved visualization: {VIZ_DIR / '05_activation_comparison.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: CHAIN RULE AND BACKPROPAGATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 5: CHAIN RULE AND BACKPROPAGATION\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a0ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW CONTEXT:\n",
    "The chain rule is the mathematical foundation of backpropagation,\n",
    "which enables training of deep neural networks.\n",
    "\n",
    "Chain Rule:\n",
    "    If z = f(g(x)), then dz/dx = (dz/dg) * (dg/dx)\n",
    "\n",
    "In neural networks:\n",
    "    - Forward pass: compute outputs layer by layer\n",
    "    - Backward pass: compute gradients using chain rule\n",
    "\n",
    "Example: Two-layer network\n",
    "    x -> Linear -> ReLU -> Linear -> Loss\n",
    "\n",
    "Gradient flow:\n",
    "    dL/dW1 = (dL/dz2) * (dz2/dz1) * (dz1/dW1)\n",
    "\n",
    "This is why activation function derivatives matter!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ac79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork:\n",
    "    \"\"\"\n",
    "    Manually implemented 2-layer network for educational purposes.\n",
    "\n",
    "    Architecture:\n",
    "        Input (2) -> Linear (3) -> ReLU -> Linear (1) -> Output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize weights with small random values\n",
    "        self.W1 = torch.randn(2, 3) * 0.1  # Input to hidden\n",
    "        self.b1 = torch.zeros(3)\n",
    "        self.W2 = torch.randn(3, 1) * 0.1  # Hidden to output\n",
    "        self.b2 = torch.zeros(1)\n",
    "\n",
    "        # Storage for intermediate values (needed for backward pass)\n",
    "        self.x = None\n",
    "        self.z1 = None  # Before activation\n",
    "        self.a1 = None  # After activation\n",
    "        self.z2 = None  # Final output\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with detailed logging.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"FORWARD PASS\")\n",
    "        print(\"=\"*40)\n",
    "\n",
    "        self.x = x\n",
    "        print(f\"Input x: {x}\")\n",
    "        print(f\"  shape: {x.shape}\")\n",
    "\n",
    "        # Layer 1: Linear transformation\n",
    "        self.z1 = x @ self.W1 + self.b1\n",
    "        print(f\"\\nLayer 1 (before activation) z1 = x @ W1 + b1\")\n",
    "        print(f\"  z1: {self.z1}\")\n",
    "        print(f\"  shape: {self.z1.shape}\")\n",
    "\n",
    "        # ReLU activation\n",
    "        self.a1 = relu_from_scratch(self.z1)\n",
    "        print(f\"\\nReLU activation: a1 = max(0, z1)\")\n",
    "        print(f\"  a1: {self.a1}\")\n",
    "        print(f\"  (Note: negative values zeroed out)\")\n",
    "\n",
    "        # Layer 2: Linear transformation\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        print(f\"\\nLayer 2 (output) z2 = a1 @ W2 + b2\")\n",
    "        print(f\"  z2: {self.z2}\")\n",
    "        print(f\"  shape: {self.z2.shape}\")\n",
    "\n",
    "        return self.z2\n",
    "\n",
    "    def backward(self, target: torch.Tensor) -> dict:\n",
    "        \"\"\"\n",
    "        Backward pass using chain rule.\n",
    "\n",
    "        Loss: L = 0.5 * (z2 - target)^2 (MSE)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"BACKWARD PASS (Chain Rule)\")\n",
    "        print(\"=\"*40)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = 0.5 * (self.z2 - target) ** 2\n",
    "        print(f\"Loss (MSE): {loss.item():.6f}\")\n",
    "        print(f\"  L = 0.5 * (z2 - target)^2\")\n",
    "        print(f\"  L = 0.5 * ({self.z2.item():.4f} - {target.item():.4f})^2\")\n",
    "\n",
    "        # Gradient of loss w.r.t. output\n",
    "        # dL/dz2 = z2 - target\n",
    "        dL_dz2 = self.z2 - target\n",
    "        print(f\"\\n1. Gradient of loss w.r.t. output:\")\n",
    "        print(f\"   dL/dz2 = z2 - target = {dL_dz2.item():.6f}\")\n",
    "\n",
    "        # Gradient w.r.t. W2 and b2\n",
    "        # z2 = a1 @ W2 + b2\n",
    "        # dL/dW2 = a1^T @ dL/dz2\n",
    "        # dL/db2 = dL/dz2\n",
    "        dL_dW2 = self.a1.T @ dL_dz2\n",
    "        dL_db2 = dL_dz2.sum(dim=0)\n",
    "\n",
    "        print(f\"\\n2. Gradient w.r.t. Layer 2 parameters:\")\n",
    "        print(f\"   dL/dW2 = a1^T @ dL/dz2\")\n",
    "        print(f\"   dL/dW2:\\n{dL_dW2}\")\n",
    "        print(f\"   dL/db2: {dL_db2}\")\n",
    "\n",
    "        # Gradient w.r.t. a1 (chain rule!)\n",
    "        # dL/da1 = dL/dz2 @ W2^T\n",
    "        dL_da1 = dL_dz2 @ self.W2.T\n",
    "        print(f\"\\n3. Gradient w.r.t. hidden activations (chain rule):\")\n",
    "        print(f\"   dL/da1 = dL/dz2 @ W2^T\")\n",
    "        print(f\"   dL/da1: {dL_da1}\")\n",
    "\n",
    "        # Gradient w.r.t. z1 (through ReLU)\n",
    "        # ReLU gradient: 1 if z1 > 0, else 0\n",
    "        # dL/dz1 = dL/da1 * d(ReLU)/dz1\n",
    "        relu_grad = relu_derivative(self.z1)\n",
    "        dL_dz1 = dL_da1 * relu_grad\n",
    "\n",
    "        print(f\"\\n4. Gradient through ReLU (chain rule):\")\n",
    "        print(f\"   ReLU gradient: {relu_grad}\")\n",
    "        print(f\"   dL/dz1 = dL/da1 * d(ReLU)/dz1\")\n",
    "        print(f\"   dL/dz1: {dL_dz1}\")\n",
    "        print(f\"   (Note: zero gradient where ReLU was inactive)\")\n",
    "\n",
    "        # Gradient w.r.t. W1 and b1\n",
    "        # z1 = x @ W1 + b1\n",
    "        # dL/dW1 = x^T @ dL/dz1\n",
    "        # dL/db1 = dL/dz1\n",
    "        dL_dW1 = self.x.T @ dL_dz1\n",
    "        dL_db1 = dL_dz1.sum(dim=0)\n",
    "\n",
    "        print(f\"\\n5. Gradient w.r.t. Layer 1 parameters:\")\n",
    "        print(f\"   dL/dW1 = x^T @ dL/dz1\")\n",
    "        print(f\"   dL/dW1:\\n{dL_dW1}\")\n",
    "        print(f\"   dL/db1: {dL_db1}\")\n",
    "\n",
    "        print(f\"\\n\" + \"=\"*40)\n",
    "        print(\"CHAIN RULE IN ACTION:\")\n",
    "        print(\"=\"*40)\n",
    "        print(\"dL/dW1 = dL/dz2 * dz2/da1 * da1/dz1 * dz1/dW1\")\n",
    "        print(\"       = (z2-target) * W2 * ReLU'(z1) * x\")\n",
    "        print(\"\\nEach gradient flows backward through the network,\")\n",
    "        print(\"multiplied by local derivatives at each layer!\")\n",
    "\n",
    "        return {\n",
    "            'dL_dW1': dL_dW1,\n",
    "            'dL_db1': dL_db1,\n",
    "            'dL_dW2': dL_dW2,\n",
    "            'dL_db2': dL_db2,\n",
    "            'loss': loss.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Manual Backpropagation Example\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99735939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple network\n",
    "net = SimpleNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c17fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and target\n",
    "x = torch.tensor([[1.0, 2.0]])\n",
    "target = torch.tensor([[3.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input: {x}\")\n",
    "print(f\"Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "output = net.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a48491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nPrediction: {output.item():.4f}\")\n",
    "print(f\"Target: {target.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa23cc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "gradients = net.backward(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with PyTorch autograd\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Verification with PyTorch Autograd\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch network with same weights\n",
    "W1_torch = torch.tensor(net.W1.numpy(), requires_grad=True)\n",
    "b1_torch = torch.tensor(net.b1.numpy(), requires_grad=True)\n",
    "W2_torch = torch.tensor(net.W2.numpy(), requires_grad=True)\n",
    "b2_torch = torch.tensor(net.b2.numpy(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bfd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "z1_torch = x @ W1_torch + b1_torch\n",
    "a1_torch = F.relu(z1_torch)\n",
    "z2_torch = a1_torch @ W2_torch + b2_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1bbf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss_torch = 0.5 * (z2_torch - target) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass (PyTorch autograd)\n",
    "loss_torch.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0277abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparing manual gradients with PyTorch autograd:\")\n",
    "print(f\"\\ndL/dW1 difference: {torch.abs(gradients['dL_dW1'] - W1_torch.grad).max().item():.10f}\")\n",
    "print(f\"dL/db1 difference: {torch.abs(gradients['dL_db1'] - b1_torch.grad).max().item():.10f}\")\n",
    "print(f\"dL/dW2 difference: {torch.abs(gradients['dL_dW2'] - W2_torch.grad).max().item():.10f}\")\n",
    "print(f\"dL/db2 difference: {torch.abs(gradients['dL_db2'] - b2_torch.grad).max().item():.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7311fb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n\u2713 Manual backpropagation matches PyTorch autograd!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a971c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient flow\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Visualizing Gradient Flow\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e382c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Network architecture with gradient magnitudes\n",
    "ax = axes[0, 0]\n",
    "ax.text(0.5, 0.9, 'Gradient Flow Through Network', ha='center',\n",
    "        fontsize=14, fontweight='bold', transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw network layers\n",
    "layer_positions = [0.1, 0.4, 0.7]\n",
    "layer_names = ['Input\\n(2)', 'Hidden\\n(ReLU)\\n(3)', 'Output\\n(1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (pos, name) in enumerate(zip(layer_positions, layer_names)):\n",
    "    circle = plt.Circle((pos, 0.5), 0.08, color='lightblue', ec='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(pos, 0.5, name, ha='center', va='center', fontsize=9, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw connections with gradient magnitudes\n",
    "grad_W1_mag = torch.abs(gradients['dL_dW1']).mean().item()\n",
    "grad_W2_mag = torch.abs(gradients['dL_dW2']).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1 connection\n",
    "arrow1 = plt.Arrow(0.18, 0.5, 0.14, 0, width=0.1, color='red', alpha=0.7)\n",
    "ax.add_patch(arrow1)\n",
    "ax.text(0.25, 0.65, f'\u2207W1\\n{grad_W1_mag:.4f}', ha='center', fontsize=9,\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085abdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W2 connection\n",
    "arrow2 = plt.Arrow(0.48, 0.5, 0.14, 0, width=0.1, color='red', alpha=0.7)\n",
    "ax.add_patch(arrow2)\n",
    "ax.text(0.55, 0.65, f'\u2207W2\\n{grad_W2_mag:.4f}', ha='center', fontsize=9,\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "ax.text(0.9, 0.5, 'Loss', ha='center', va='center', fontsize=11, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='salmon', alpha=0.7))\n",
    "arrow3 = plt.Arrow(0.78, 0.5, 0.07, 0, width=0.08, color='blue', alpha=0.7)\n",
    "ax.add_patch(arrow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84143755",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c350a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Gradient magnitudes by layer\n",
    "ax = axes[0, 1]\n",
    "layers = ['W1', 'b1', 'W2', 'b2']\n",
    "grad_mags = [\n",
    "    torch.abs(gradients['dL_dW1']).mean().item(),\n",
    "    torch.abs(gradients['dL_db1']).mean().item(),\n",
    "    torch.abs(gradients['dL_dW2']).mean().item(),\n",
    "    torch.abs(gradients['dL_db2']).mean().item(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'lightblue', 'red', 'lightcoral']\n",
    "bars = ax.bar(layers, grad_mags, color=colors, edgecolor='black', linewidth=2, alpha=0.7)\n",
    "ax.set_ylabel('Average Gradient Magnitude', fontsize=11)\n",
    "ax.set_title('Gradient Magnitudes by Parameter', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9cf09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bar, mag in zip(bars, grad_mags):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{mag:.4f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: ReLU impact on gradients\n",
    "ax = axes[1, 0]\n",
    "z1_vals = net.z1[0].numpy()\n",
    "dL_dz1_vals = gradients['dL_dW1'].sum(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ae024",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos = np.arange(len(z1_vals))\n",
    "width = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cd2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars1 = ax.bar(x_pos - width/2, z1_vals, width, label='Pre-activation (z1)',\n",
    "               color='green', alpha=0.7, edgecolor='black')\n",
    "bars2 = ax.bar(x_pos + width/2, dL_dz1_vals, width, label='Gradient (dL/dz1)',\n",
    "               color='orange', alpha=0.7, edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xlabel('Hidden Unit', fontsize=11)\n",
    "ax.set_ylabel('Value', fontsize=11)\n",
    "ax.set_title('ReLU Effect: Negative Pre-activations \u2192 Zero Gradients', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f'Unit {i}' for i in range(len(z1_vals))])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.axhline(y=0, color='k', linestyle='--', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86962eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Chain rule visualization\n",
    "ax = axes[1, 1]\n",
    "ax.text(0.5, 0.9, 'Chain Rule in Backpropagation', ha='center',\n",
    "        fontsize=13, fontweight='bold', transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_text = \"\"\"\n",
    "Forward Pass:\n",
    "  x \u2192 z\u2081 = x\u00b7W\u2081 + b\u2081\n",
    "    \u2192 a\u2081 = ReLU(z\u2081)\n",
    "      \u2192 z\u2082 = a\u2081\u00b7W\u2082 + b\u2082\n",
    "        \u2192 L = \u00bd(z\u2082 - y)\u00b2\n",
    "\n",
    "Backward Pass (Chain Rule):\n",
    "  dL/dW\u2081 = dL/dz\u2082 \u00b7 dz\u2082/da\u2081 \u00b7 da\u2081/dz\u2081 \u00b7 dz\u2081/dW\u2081\n",
    "         = (z\u2082-y) \u00b7 W\u2082 \u00b7 ReLU'(z\u2081) \u00b7 x\n",
    "\n",
    "  Each term is a local gradient!\n",
    "\n",
    "  This is the power of backpropagation:\n",
    "  - Compute local gradients\n",
    "  - Chain them together\n",
    "  - Efficient O(n) complexity\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.text(0.05, 0.75, chain_text, ha='left', va='top', fontsize=9,\n",
    "        family='monospace', transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921b7be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"06_chain_rule_backpropagation.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\u2713 Saved visualization: {VIZ_DIR / '06_chain_rule_backpropagation.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: LOSS LANDSCAPES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 6: LOSS LANDSCAPES\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW CONTEXT:\n",
    "Understanding loss landscapes helps explain:\n",
    "- Why optimization is challenging\n",
    "- The role of learning rate\n",
    "- Local minima vs global minima\n",
    "- Why initialization matters\n",
    "\n",
    "We'll visualize a simple 2D loss landscape for a toy problem.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_landscape(w1_range, w2_range, X, y):\n",
    "    \"\"\"\n",
    "    Create a loss landscape for a simple linear regression problem.\n",
    "\n",
    "    Model: y = w1 * x1 + w2 * x2\n",
    "    Loss: MSE\n",
    "    \"\"\"\n",
    "    W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "    Loss = np.zeros_like(W1)\n",
    "\n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            w = torch.tensor([[W1[i, j]], [W2[i, j]]], dtype=torch.float32)\n",
    "            pred = X @ w\n",
    "            loss = torch.mean((pred - y) ** 2)\n",
    "            Loss[i, j] = loss.item()\n",
    "\n",
    "    return W1, W2, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffa1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create toy dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 20\n",
    "X_np = np.random.randn(n_samples, 2)\n",
    "true_w = np.array([[2.0], [3.0]])\n",
    "y_np = X_np @ true_w + np.random.randn(n_samples, 1) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f723c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset: {n_samples} samples\")\n",
    "print(f\"True weights: w1={true_w[0, 0]:.2f}, w2={true_w[1, 0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c60e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss landscape\n",
    "w_range = np.linspace(-1, 5, 100)\n",
    "W1, W2, Loss = create_loss_landscape(w_range, w_range, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoss landscape computed over {len(w_range)}x{len(w_range)} grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent and record trajectory\n",
    "w_init = torch.tensor([[0.0], [0.0]], requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "n_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b98d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = [w_init.detach().numpy().copy()]\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce3b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(n_steps):\n",
    "    pred = X @ w_init\n",
    "    loss = torch.mean((pred - y) ** 2)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w_init -= learning_rate * w_init.grad\n",
    "        trajectory.append(w_init.numpy().copy())\n",
    "        w_init.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac29363",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = np.array(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67d309",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"\\nGradient descent: {n_steps} steps\")\n",
    "print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Final weights: w1={trajectory[-1][0, 0]:.2f}, w2={trajectory[-1][1, 0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss landscape\n",
    "fig = plt.figure(figsize=(16, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: 2D contour plot with trajectory\n",
    "ax1 = fig.add_subplot(131)\n",
    "contour = ax1.contour(W1, W2, Loss, levels=20, cmap='viridis', alpha=0.6)\n",
    "ax1.clabel(contour, inline=True, fontsize=8)\n",
    "contourf = ax1.contourf(W1, W2, Loss, levels=20, cmap='viridis', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef025c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory\n",
    "ax1.plot(trajectory[:, 0, 0], trajectory[:, 1, 0], 'r.-', linewidth=2,\n",
    "         markersize=8, label='Gradient Descent Path')\n",
    "ax1.plot(trajectory[0, 0, 0], trajectory[0, 1, 0], 'go', markersize=12,\n",
    "         label='Start', markeredgecolor='black', markeredgewidth=2)\n",
    "ax1.plot(trajectory[-1, 0, 0], trajectory[-1, 1, 0], 'r*', markersize=18,\n",
    "         label='End', markeredgecolor='black', markeredgewidth=2)\n",
    "ax1.plot(true_w[0, 0], true_w[1, 0], 'b*', markersize=18,\n",
    "         label='True Optimum', markeredgecolor='black', markeredgewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239062aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1.set_xlabel('w\u2081', fontsize=12)\n",
    "ax1.set_ylabel('w\u2082', fontsize=12)\n",
    "ax1.set_title('Loss Landscape with Gradient Descent Trajectory', fontsize=13, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.colorbar(contourf, ax=ax1, label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: 3D surface plot\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "surf = ax2.plot_surface(W1, W2, Loss, cmap='viridis', alpha=0.6, edgecolor='none')\n",
    "ax2.plot(trajectory[:, 0, 0], trajectory[:, 1, 0],\n",
    "         [losses[min(i, len(losses)-1)] for i in range(len(trajectory))],\n",
    "         'r.-', linewidth=2, markersize=6, label='GD Path')\n",
    "ax2.set_xlabel('w\u2081', fontsize=11)\n",
    "ax2.set_ylabel('w\u2082', fontsize=11)\n",
    "ax2.set_zlabel('Loss', fontsize=11)\n",
    "ax2.set_title('3D Loss Surface', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(surf, ax=ax2, shrink=0.5, aspect=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd283db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Loss over iterations\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.plot(losses, linewidth=2, color='blue', marker='o', markersize=4)\n",
    "ax3.set_xlabel('Iteration', fontsize=12)\n",
    "ax3.set_ylabel('Loss', fontsize=12)\n",
    "ax3.set_title('Loss Convergence During Training', fontsize=13, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a882fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add annotations\n",
    "ax3.annotate('Fast initial decrease', xy=(5, losses[5]), xytext=(15, losses[5]*3),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
    "            fontsize=10)\n",
    "ax3.annotate('Slower convergence', xy=(40, losses[40]), xytext=(25, losses[40]*0.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=1.5),\n",
    "            fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e918b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"07_loss_landscape.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\u2713 Saved visualization: {VIZ_DIR / '07_loss_landscape.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE SUMMARY FOR INTERVIEW PREPARATION\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "KEY TAKEAWAYS FOR LLM INTERVIEWS:\n",
    "\n",
    "1. CROSS-ENTROPY LOSS:\n",
    "   - Foundation of language model training\n",
    "   - Formula: L = -log(p_target)\n",
    "   - Penalizes confident wrong predictions heavily\n",
    "   - Used with softmax for multi-class classification\n",
    "\n",
    "2. PERPLEXITY:\n",
    "   - Primary LLM evaluation metric\n",
    "   - PPL = exp(average cross-entropy)\n",
    "   - Lower is better (less \"surprised\" by test data)\n",
    "   - Interpretable: PPL=K means like choosing from K options\n",
    "\n",
    "3. KL DIVERGENCE:\n",
    "   - Measures distribution difference\n",
    "   - KL(P || Q) = sum(P * log(P/Q))\n",
    "   - Used in knowledge distillation, VAEs, RLHF\n",
    "   - NOT symmetric! KL(P||Q) \u2260 KL(Q||P)\n",
    "\n",
    "4. ACTIVATION FUNCTIONS:\n",
    "   - ReLU: Simple, fast, but can \"die\"\n",
    "   - Leaky ReLU: Fixes dying ReLU problem\n",
    "   - GELU: Smooth, used in modern LLMs (BERT, GPT)\n",
    "   - Derivatives crucial for backpropagation!\n",
    "\n",
    "5. CHAIN RULE & BACKPROPAGATION:\n",
    "   - Foundation of neural network training\n",
    "   - Efficiently computes gradients layer by layer\n",
    "   - dL/dW1 = dL/dz2 * dz2/da1 * da1/dz1 * dz1/dW1\n",
    "   - Each layer computes local gradient\n",
    "\n",
    "6. LOSS LANDSCAPES:\n",
    "   - Visualizes optimization challenges\n",
    "   - Shows why learning rate matters\n",
    "   - Explains local vs global minima\n",
    "   - Convex in simple cases, complex in deep networks\n",
    "\n",
    "INTERVIEW TIPS:\n",
    "- Be able to derive cross-entropy from first principles\n",
    "- Explain why perplexity is better than raw loss\n",
    "- Describe when to use KL divergence vs cross-entropy\n",
    "- Know which activation functions are used in modern LLMs\n",
    "- Walk through backpropagation for a simple network\n",
    "- Discuss optimization challenges (local minima, saddle points)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65fee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL VISUALIZATIONS SAVED TO:\")\n",
    "print(f\"  {VIZ_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved files\n",
    "saved_files = sorted(VIZ_DIR.glob(\"*.png\"))\n",
    "print(\"\\nGenerated visualizations:\")\n",
    "for i, file in enumerate(saved_files, 1):\n",
    "    print(f\"  {i}. {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEMO COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYou now have a comprehensive understanding of loss functions\")\n",
    "print(\"and mathematical foundations critical for LLM interviews.\")\n",
    "print(\"\\nReview the visualizations and run this script multiple times\")\n",
    "print(\"to reinforce your understanding. Good luck with your interviews!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}