{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Tokenization Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5891b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comprehensive Tokenization Demo for LLM Interview Preparation\n",
    "==============================================================\n",
    "\n",
    "This module demonstrates key tokenization concepts for ML/LLM interviews:\n",
    "- Q1: What is tokenization?\n",
    "- Q16: How do LLMs handle Out-of-Vocabulary (OOV) words?\n",
    "\n",
    "Topics covered:\n",
    "1. Word-level tokenization\n",
    "2. Character-level tokenization\n",
    "3. Byte-Pair Encoding (BPE) from scratch\n",
    "4. Subword tokenization\n",
    "5. OOV handling with different approaches\n",
    "6. Vocabulary building process\n",
    "\n",
    "Author: Interview Preparation Material\n",
    "Date: 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5cf5d",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 1: BASIC TOKENIZATION APPROACHES\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbfa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLevelTokenizer:\n",
    "    \"\"\"\n",
    "    Word-level tokenization: Split text into words.\n",
    "\n",
    "    Pros:\n",
    "    - Simple and intuitive\n",
    "    - Preserves word boundaries\n",
    "    - Fast\n",
    "\n",
    "    Cons:\n",
    "    - Large vocabulary size\n",
    "    - Can't handle OOV (Out-of-Vocabulary) words\n",
    "    - Poor generalization to unseen words\n",
    "    - Requires extensive vocabulary for good coverage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
    "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "        self.next_id = 4\n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"Build vocabulary from training texts.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"WORD-LEVEL TOKENIZATION - TRAINING\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            # Simple word splitting (lowercase, split on whitespace/punctuation)\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            word_counts.update(words)\n",
    "\n",
    "        print(f\"\\nTotal unique words found: {len(word_counts)}\")\n",
    "        print(f\"Most common words: {word_counts.most_common(10)}\")\n",
    "\n",
    "        # Add words to vocabulary\n",
    "        for word, count in word_counts.items():\n",
    "            if word not in self.vocab:\n",
    "                self.vocab[word] = self.next_id\n",
    "                self.id_to_token[self.next_id] = word\n",
    "                self.next_id += 1\n",
    "\n",
    "        print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text to token IDs.\"\"\"\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        # Use <UNK> token for words not in vocabulary (OOV handling)\n",
    "        return [self.vocab.get(word, self.vocab[\"<UNK>\"]) for word in words]\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Convert token IDs back to text.\"\"\"\n",
    "        return \" \".join([self.id_to_token.get(tid, \"<UNK>\") for tid in token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9795ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLevelTokenizer:\n",
    "    \"\"\"\n",
    "    Character-level tokenization: Split text into individual characters.\n",
    "\n",
    "    Pros:\n",
    "    - Very small vocabulary (typically 50-300 characters)\n",
    "    - No OOV problem (can represent any text)\n",
    "    - Good for morphologically rich languages\n",
    "\n",
    "    Cons:\n",
    "    - Very long sequences (increases computational cost)\n",
    "    - Model needs to learn word structure from scratch\n",
    "    - Loses semantic word boundaries\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
    "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "        self.next_id = 4\n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"Build character vocabulary from training texts.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CHARACTER-LEVEL TOKENIZATION - TRAINING\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        char_counts = Counter()\n",
    "        for text in texts:\n",
    "            char_counts.update(text)\n",
    "\n",
    "        print(f\"\\nTotal unique characters found: {len(char_counts)}\")\n",
    "        print(f\"Characters: {sorted(char_counts.keys())}\")\n",
    "\n",
    "        # Add characters to vocabulary\n",
    "        for char, count in char_counts.items():\n",
    "            if char not in self.vocab:\n",
    "                self.vocab[char] = self.next_id\n",
    "                self.id_to_token[self.next_id] = char\n",
    "                self.next_id += 1\n",
    "\n",
    "        print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text to token IDs.\"\"\"\n",
    "        return [self.vocab.get(char, self.vocab[\"<UNK>\"]) for char in text]\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Convert token IDs back to text.\"\"\"\n",
    "        return \"\".join([self.id_to_token.get(tid, \"<UNK>\") for tid in token_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd62404",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 2: BYTE-PAIR ENCODING (BPE) - THE HEART OF MODERN TOKENIZATION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer:\n",
    "    \"\"\"\n",
    "    Byte-Pair Encoding (BPE) Tokenizer - Manual Implementation from Scratch\n",
    "\n",
    "    BPE is the foundation of modern tokenization used in GPT, BERT, and other LLMs.\n",
    "\n",
    "    Algorithm:\n",
    "    1. Start with character-level vocabulary\n",
    "    2. Iteratively merge the most frequent pair of consecutive tokens\n",
    "    3. Repeat until desired vocabulary size is reached\n",
    "\n",
    "    Pros:\n",
    "    - Balanced vocabulary size (between char and word level)\n",
    "    - Handles OOV words by breaking into subwords\n",
    "    - Data-driven approach (learns from corpus)\n",
    "    - Good compression ratio\n",
    "\n",
    "    Cons:\n",
    "    - Tokenization is not always linguistically meaningful\n",
    "    - Training can be slow on large corpora\n",
    "    - Greedy algorithm (not optimal)\n",
    "\n",
    "    This is how GPT, RoBERTa, and many other models handle tokenization!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int = 300):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Target vocabulary size (including special tokens)\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.merges = []  # List of (pair, merged_token) tuples\n",
    "        self.special_tokens = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n",
    "\n",
    "    def _get_word_frequencies(self, texts: List[str]) -> Dict[Tuple[str, ...], int]:\n",
    "        \"\"\"\n",
    "        Count word frequencies and represent each word as a tuple of characters.\n",
    "        We add a special end-of-word marker '</w>' to distinguish word boundaries.\n",
    "\n",
    "        Example: \"hello\" -> ('h', 'e', 'l', 'l', 'o', '</w>')\n",
    "        \"\"\"\n",
    "        word_freqs = defaultdict(int)\n",
    "\n",
    "        for text in texts:\n",
    "            # Extract words\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            for word in words:\n",
    "                # Represent word as tuple of chars with end marker\n",
    "                word_tuple = tuple(word) + ('</w>',)\n",
    "                word_freqs[word_tuple] += 1\n",
    "\n",
    "        return word_freqs\n",
    "\n",
    "    def _get_pair_frequencies(self, word_freqs: Dict[Tuple[str, ...], int]) -> Counter:\n",
    "        \"\"\"\n",
    "        Count how often each pair of consecutive tokens appears.\n",
    "\n",
    "        Example: For ('h', 'e', 'l', 'l', 'o', '</w>'):\n",
    "        Pairs: ('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o'), ('o', '</w>')\n",
    "        \"\"\"\n",
    "        pair_freqs = Counter()\n",
    "\n",
    "        for word, freq in word_freqs.items():\n",
    "            # Get all consecutive pairs in this word\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "\n",
    "        return pair_freqs\n",
    "\n",
    "    def _merge_pair(self, word: Tuple[str, ...], pair: Tuple[str, str],\n",
    "                    merged: str) -> Tuple[str, ...]:\n",
    "        \"\"\"\n",
    "        Merge all instances of a pair in a word.\n",
    "\n",
    "        Example:\n",
    "            word = ('h', 'e', 'l', 'l', 'o', '</w>')\n",
    "            pair = ('l', 'l')\n",
    "            merged = 'll'\n",
    "            result = ('h', 'e', 'll', 'o', '</w>')\n",
    "        \"\"\"\n",
    "        new_word = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(word):\n",
    "            # Check if current position matches the pair\n",
    "            if i < len(word) - 1 and (word[i], word[i + 1]) == pair:\n",
    "                new_word.append(merged)\n",
    "                i += 2  # Skip both tokens\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "\n",
    "        return tuple(new_word)\n",
    "\n",
    "    def train(self, texts: List[str], verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Train BPE tokenizer by iteratively merging most frequent pairs.\n",
    "\n",
    "        This is the core BPE algorithm!\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"BYTE-PAIR ENCODING (BPE) - TRAINING\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"\\nTarget vocabulary size: {self.vocab_size}\")\n",
    "\n",
    "        # Step 1: Get initial word frequencies (character-level)\n",
    "        word_freqs = self._get_word_frequencies(texts)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nInitial words (character-level representation):\")\n",
    "            for word, freq in list(word_freqs.items())[:5]:\n",
    "                print(f\"  {''.join(word):20s} -> {word} (freq: {freq})\")\n",
    "\n",
    "        # Step 2: Build initial vocabulary from characters\n",
    "        base_vocab = set()\n",
    "        for word in word_freqs.keys():\n",
    "            base_vocab.update(word)\n",
    "\n",
    "        self.vocab = {token: idx for idx, token in enumerate(self.special_tokens)}\n",
    "        for token in sorted(base_vocab):\n",
    "            self.vocab[token] = len(self.vocab)\n",
    "\n",
    "        initial_vocab_size = len(self.vocab)\n",
    "        if verbose:\n",
    "            print(f\"\\nInitial vocabulary size (character-level): {initial_vocab_size}\")\n",
    "            print(f\"Sample tokens: {list(self.vocab.keys())[:20]}\")\n",
    "\n",
    "        # Step 3: Iteratively merge most frequent pairs\n",
    "        num_merges = self.vocab_size - initial_vocab_size\n",
    "        if verbose:\n",
    "            print(f\"\\nPerforming {num_merges} merge operations...\")\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "        for merge_idx in range(num_merges):\n",
    "            # Count pair frequencies\n",
    "            pair_freqs = self._get_pair_frequencies(word_freqs)\n",
    "\n",
    "            if not pair_freqs:\n",
    "                if verbose:\n",
    "                    print(f\"\\nNo more pairs to merge. Stopping at {len(self.vocab)} tokens.\")\n",
    "                break\n",
    "\n",
    "            # Get most frequent pair\n",
    "            most_frequent_pair = pair_freqs.most_common(1)[0]\n",
    "            pair, freq = most_frequent_pair\n",
    "\n",
    "            # Create merged token\n",
    "            merged_token = ''.join(pair)\n",
    "\n",
    "            # Store merge operation\n",
    "            self.merges.append((pair, merged_token))\n",
    "\n",
    "            # Add merged token to vocabulary\n",
    "            self.vocab[merged_token] = len(self.vocab)\n",
    "\n",
    "            if verbose and (merge_idx < 10 or merge_idx % 50 == 0):\n",
    "                print(f\"Merge {merge_idx + 1:3d}: {pair[0]:10s} + {pair[1]:10s} \"\n",
    "                      f\"-> {merged_token:15s} (freq: {freq:5d})\")\n",
    "\n",
    "            # Update word frequencies with merged token\n",
    "            new_word_freqs = {}\n",
    "            for word, word_freq in word_freqs.items():\n",
    "                new_word = self._merge_pair(word, pair, merged_token)\n",
    "                new_word_freqs[new_word] = word_freq\n",
    "            word_freqs = new_word_freqs\n",
    "\n",
    "        if verbose:\n",
    "            print(\"-\" * 70)\n",
    "            print(f\"\\n\u2713 Training complete!\")\n",
    "            print(f\"  Final vocabulary size: {len(self.vocab)}\")\n",
    "            print(f\"  Total merges performed: {len(self.merges)}\")\n",
    "            print(f\"\\nSample merged tokens:\")\n",
    "            for token in list(self.vocab.keys())[-10:]:\n",
    "                print(f\"  {token}\")\n",
    "\n",
    "    def _tokenize_word(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize a single word using learned BPE merges.\n",
    "\n",
    "        This shows how BPE handles OOV words: it breaks them into known subwords!\n",
    "        \"\"\"\n",
    "        # Start with character-level representation\n",
    "        word_tokens = list(word) + ['</w>']\n",
    "\n",
    "        # Apply merges in the order they were learned\n",
    "        for pair, merged_token in self.merges:\n",
    "            i = 0\n",
    "            while i < len(word_tokens) - 1:\n",
    "                if (word_tokens[i], word_tokens[i + 1]) == pair:\n",
    "                    # Merge this pair\n",
    "                    word_tokens = (word_tokens[:i] +\n",
    "                                  [merged_token] +\n",
    "                                  word_tokens[i + 2:])\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "        return word_tokens\n",
    "\n",
    "    def encode(self, text: str, verbose: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode text to token IDs.\n",
    "\n",
    "        Demonstrates OOV handling: Even unseen words can be tokenized!\n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "        all_tokens = []\n",
    "        for word in words:\n",
    "            word_tokens = self._tokenize_word(word)\n",
    "            all_tokens.extend(word_tokens)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nTokenization breakdown:\")\n",
    "            print(f\"  Original text: {text}\")\n",
    "            print(f\"  Tokens: {all_tokens}\")\n",
    "\n",
    "        # Convert tokens to IDs (use <UNK> for any tokens not in vocab)\n",
    "        token_ids = [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in all_tokens]\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Convert token IDs back to text.\"\"\"\n",
    "        id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "        tokens = [id_to_token.get(tid, \"<UNK>\") for tid in token_ids]\n",
    "\n",
    "        # Join tokens and remove end-of-word markers\n",
    "        text = ''.join(tokens).replace('</w>', ' ').strip()\n",
    "        return text\n",
    "\n",
    "    def show_vocabulary_sample(self, n: int = 30):\n",
    "        \"\"\"Display a sample of the learned vocabulary.\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"VOCABULARY SAMPLE (showing {n} tokens)\")\n",
    "        print('='*70)\n",
    "\n",
    "        tokens = list(self.vocab.keys())\n",
    "\n",
    "        print(\"\\nSpecial tokens:\")\n",
    "        for token in tokens[:4]:\n",
    "            print(f\"  {token:20s} -> ID {self.vocab[token]}\")\n",
    "\n",
    "        print(\"\\nCharacter tokens:\")\n",
    "        char_tokens = [t for t in tokens[4:] if len(t) == 1][:10]\n",
    "        for token in char_tokens:\n",
    "            print(f\"  '{token}':20s -> ID {self.vocab[token]}\")\n",
    "\n",
    "        print(\"\\nSubword tokens (merged):\")\n",
    "        subword_tokens = [t for t in tokens if len(t) > 1 and t != '</w>'][-n:]\n",
    "        for token in subword_tokens:\n",
    "            print(f\"  {token:20s} -> ID {self.vocab[token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670eb74e",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 3: COMPARISON AND OOV HANDLING DEMONSTRATION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenization_methods():\n",
    "    \"\"\"\n",
    "    Compare different tokenization methods on the same text.\n",
    "    Demonstrates how each method handles vocabulary and sequence length.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TOKENIZATION METHOD COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Training corpus\n",
    "    training_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog\",\n",
    "        \"Machine learning is transforming the world\",\n",
    "        \"Natural language processing enables computers to understand text\",\n",
    "        \"Tokenization is a fundamental step in text processing\",\n",
    "        \"Deep learning models require careful preprocessing\",\n",
    "        \"The transformer architecture revolutionized NLP\",\n",
    "        \"Attention mechanisms allow models to focus on relevant parts\",\n",
    "        \"Large language models are trained on massive datasets\",\n",
    "    ]\n",
    "\n",
    "    # Test text with OOV words\n",
    "    test_text = \"The extraordinary supercomputer processes unimaginable amounts of information\"\n",
    "\n",
    "    print(\"\\nTraining corpus:\")\n",
    "    for i, text in enumerate(training_texts, 1):\n",
    "        print(f\"  {i}. {text}\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Test text (contains OOV words):\")\n",
    "    print(f\"  \\\"{test_text}\\\"\")\n",
    "    print('='*70)\n",
    "\n",
    "    # Method 1: Word-level\n",
    "    print(\"\\n\\n\" + \"+\"*70)\n",
    "    print(\"METHOD 1: WORD-LEVEL TOKENIZATION\")\n",
    "    print(\"+\"*70)\n",
    "\n",
    "    word_tokenizer = WordLevelTokenizer()\n",
    "    word_tokenizer.train(training_texts)\n",
    "\n",
    "    word_ids = word_tokenizer.encode(test_text)\n",
    "    word_decoded = word_tokenizer.decode(word_ids)\n",
    "\n",
    "    print(f\"\\nTest text encoding:\")\n",
    "    print(f\"  Token IDs: {word_ids}\")\n",
    "    print(f\"  Number of tokens: {len(word_ids)}\")\n",
    "    print(f\"  Decoded: {word_decoded}\")\n",
    "\n",
    "    # Count UNK tokens\n",
    "    unk_count = word_ids.count(word_tokenizer.vocab[\"<UNK>\"])\n",
    "    print(f\"\\n  \u26a0\ufe0f  OOV words replaced with <UNK>: {unk_count} out of {len(word_ids)} tokens\")\n",
    "    print(f\"  \ud83d\udcca Vocabulary size: {len(word_tokenizer.vocab)}\")\n",
    "\n",
    "    # Method 2: Character-level\n",
    "    print(\"\\n\\n\" + \"+\"*70)\n",
    "    print(\"METHOD 2: CHARACTER-LEVEL TOKENIZATION\")\n",
    "    print(\"+\"*70)\n",
    "\n",
    "    char_tokenizer = CharLevelTokenizer()\n",
    "    char_tokenizer.train(training_texts)\n",
    "\n",
    "    char_ids = char_tokenizer.encode(test_text)\n",
    "    char_decoded = char_tokenizer.decode(char_ids)\n",
    "\n",
    "    print(f\"\\nTest text encoding:\")\n",
    "    print(f\"  Token IDs: {char_ids[:50]}... (truncated)\")\n",
    "    print(f\"  Number of tokens: {len(char_ids)}\")\n",
    "    print(f\"  Decoded: {char_decoded}\")\n",
    "\n",
    "    unk_count = char_ids.count(char_tokenizer.vocab[\"<UNK>\"])\n",
    "    print(f\"\\n  \u2713 OOV words: {unk_count} (character-level handles all text!)\")\n",
    "    print(f\"  \ud83d\udcca Vocabulary size: {len(char_tokenizer.vocab)}\")\n",
    "\n",
    "    # Method 3: BPE\n",
    "    print(\"\\n\\n\" + \"+\"*70)\n",
    "    print(\"METHOD 3: BYTE-PAIR ENCODING (BPE)\")\n",
    "    print(\"+\"*70)\n",
    "\n",
    "    bpe_tokenizer = SimpleBPETokenizer(vocab_size=200)\n",
    "    bpe_tokenizer.train(training_texts, verbose=True)\n",
    "\n",
    "    print(f\"\\nTest text encoding:\")\n",
    "    bpe_ids = bpe_tokenizer.encode(test_text, verbose=True)\n",
    "    bpe_decoded = bpe_tokenizer.decode(bpe_ids)\n",
    "\n",
    "    print(f\"\\n  Token IDs: {bpe_ids}\")\n",
    "    print(f\"  Number of tokens: {len(bpe_ids)}\")\n",
    "    print(f\"  Decoded: {bpe_decoded}\")\n",
    "\n",
    "    unk_count = bpe_ids.count(bpe_tokenizer.vocab[\"<UNK>\"])\n",
    "    print(f\"\\n  \u2713 OOV words: {unk_count} (BPE breaks unknown words into subwords!)\")\n",
    "    print(f\"  \ud83d\udcca Vocabulary size: {len(bpe_tokenizer.vocab)}\")\n",
    "\n",
    "    # Summary comparison\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    comparison_data = [\n",
    "        (\"Method\", \"Vocab Size\", \"Token Count\", \"OOV Handling\"),\n",
    "        (\"-\" * 20, \"-\" * 12, \"-\" * 12, \"-\" * 30),\n",
    "        (\"Word-level\", len(word_tokenizer.vocab), len(word_ids),\n",
    "         f\"{unk_count} <UNK> tokens (poor)\"),\n",
    "        (\"Character-level\", len(char_tokenizer.vocab), len(char_ids),\n",
    "         \"Perfect (no OOV)\"),\n",
    "        (\"BPE (subword)\", len(bpe_tokenizer.vocab), len(bpe_ids),\n",
    "         \"Excellent (breaks into subwords)\"),\n",
    "    ]\n",
    "\n",
    "    for row in comparison_data:\n",
    "        print(f\"{row[0]:20s} | {str(row[1]):12s} | {str(row[2]):12s} | {row[3]:30s}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\"\"\n",
    "    1. Word-level:\n",
    "       - Largest vocabulary, but can't handle unseen words\n",
    "       - Best for: Fixed vocabulary tasks, simple applications\n",
    "\n",
    "    2. Character-level:\n",
    "       - Smallest vocabulary, handles all text\n",
    "       - Very long sequences (computational cost)\n",
    "       - Best for: Character-level tasks, morphologically rich languages\n",
    "\n",
    "    3. BPE (Subword):\n",
    "       - Balanced vocabulary size and sequence length\n",
    "       - Handles OOV by breaking into known subwords\n",
    "       - Best for: Modern LLMs (GPT, BERT, etc.)\n",
    "       - This is the STANDARD for most LLMs today!\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f7990c",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 4: OOV HANDLING DEMONSTRATION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25864b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_oov_handling():\n",
    "    \"\"\"\n",
    "    Detailed demonstration of how different tokenizers handle OOV words.\n",
    "\n",
    "    This is crucial for interview question: \"How do LLMs handle OOV words?\"\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"OUT-OF-VOCABULARY (OOV) HANDLING DEMONSTRATION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Simple training corpus\n",
    "    training_texts = [\n",
    "        \"the cat sat on the mat\",\n",
    "        \"the dog ran in the park\",\n",
    "        \"a cat and a dog are friends\",\n",
    "    ]\n",
    "\n",
    "    # Test cases with increasing OOV complexity\n",
    "    test_cases = [\n",
    "        (\"the cat sat\", \"All words in vocabulary\"),\n",
    "        (\"the elephant sat\", \"One OOV word: 'elephant'\"),\n",
    "        (\"the cat slept\", \"One OOV word: 'slept'\"),\n",
    "        (\"extraordinary elephants communicate\", \"All OOV words\"),\n",
    "        (\"antidisestablishmentarianism\", \"Very long OOV word\"),\n",
    "    ]\n",
    "\n",
    "    print(\"\\nTraining corpus (small, limited vocabulary):\")\n",
    "    for text in training_texts:\n",
    "        print(f\"  - {text}\")\n",
    "\n",
    "    # Train BPE tokenizer\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    bpe = SimpleBPETokenizer(vocab_size=100)\n",
    "    bpe.train(training_texts, verbose=False)\n",
    "    print(f\"BPE vocabulary size: {len(bpe.vocab)}\")\n",
    "\n",
    "    # Show how BPE breaks down OOV words\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HOW BPE HANDLES OOV WORDS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    for test_text, description in test_cases:\n",
    "        print(f\"\\n{'-'*70}\")\n",
    "        print(f\"Test: {description}\")\n",
    "        print(f\"Text: \\\"{test_text}\\\"\")\n",
    "        print(f\"{'-'*70}\")\n",
    "\n",
    "        # Tokenize\n",
    "        words = test_text.split()\n",
    "        for word in words:\n",
    "            tokens = bpe._tokenize_word(word)\n",
    "            token_ids = [bpe.vocab.get(t, bpe.vocab[\"<UNK>\"]) for t in tokens]\n",
    "\n",
    "            in_vocab = all(t in bpe.vocab for t in tokens)\n",
    "            status = \"\u2713 IN VOCAB\" if in_vocab else \"\u26a0 OOV (decomposed)\"\n",
    "\n",
    "            print(f\"\\n  Word: '{word}' {status}\")\n",
    "            print(f\"    Subword tokens: {tokens}\")\n",
    "            print(f\"    Token IDs: {token_ids}\")\n",
    "\n",
    "            if not in_vocab:\n",
    "                print(f\"    \u2192 BPE broke this OOV word into {len(tokens)} known subwords!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY CONCEPT: SUBWORD TOKENIZATION FOR OOV HANDLING\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\"\"\n",
    "    How LLMs handle OOV words (Interview Answer):\n",
    "\n",
    "    1. Traditional word-level: Replace OOV with <UNK> token (loses information)\n",
    "\n",
    "    2. Modern approach (BPE/WordPiece/SentencePiece):\n",
    "       - Break OOV words into known subword units\n",
    "       - NEVER need <UNK> token (in practice)\n",
    "       - Examples:\n",
    "         * \"unhappiness\" \u2192 [\"un\", \"happi\", \"ness\"]\n",
    "         * \"COVID-19\" \u2192 [\"CO\", \"VID\", \"-\", \"19\"]\n",
    "         * \"transformer\" \u2192 [\"trans\", \"former\"]\n",
    "\n",
    "    3. Why this works:\n",
    "       - Most words share common prefixes, suffixes, roots\n",
    "       - Model can compose meaning from subwords\n",
    "       - Open vocabulary: Can represent any text\n",
    "       - More efficient than character-level (shorter sequences)\n",
    "\n",
    "    4. Used in:\n",
    "       - GPT (BPE)\n",
    "       - BERT (WordPiece, similar to BPE)\n",
    "       - T5, ALBERT (SentencePiece)\n",
    "       - RoBERTa (Byte-level BPE)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77273e2d",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 5: VOCABULARY BUILDING VISUALIZATION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6355b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vocabulary_building():\n",
    "    \"\"\"\n",
    "    Step-by-step visualization of how BPE builds vocabulary.\n",
    "    Great for understanding the algorithm deeply.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"BPE VOCABULARY BUILDING - STEP-BY-STEP VISUALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Very simple corpus for clear visualization\n",
    "    texts = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
    "\n",
    "    print(\"\\nTraining corpus:\")\n",
    "    for word in texts:\n",
    "        print(f\"  - {word}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP-BY-STEP MERGE PROCESS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Manual demonstration (simplified)\n",
    "    print(\"\\nInitial state (character-level):\")\n",
    "    initial_words = {\n",
    "        ('l', 'o', 'w', '</w>'): 1,\n",
    "        ('l', 'o', 'w', 'e', 'r', '</w>'): 1,\n",
    "        ('n', 'e', 'w', 'e', 's', 't', '</w>'): 1,\n",
    "        ('w', 'i', 'd', 'e', 's', 't', '</w>'): 1,\n",
    "    }\n",
    "\n",
    "    for word, freq in initial_words.items():\n",
    "        print(f\"  {''.join(word):15s} -> {word}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Let's trace the first few merges:\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    # Train with verbose output\n",
    "    bpe = SimpleBPETokenizer(vocab_size=50)\n",
    "\n",
    "    # Manually show merge process\n",
    "    word_freqs = bpe._get_word_frequencies(texts)\n",
    "\n",
    "    for merge_step in range(5):\n",
    "        pair_freqs = bpe._get_pair_frequencies(word_freqs)\n",
    "\n",
    "        if not pair_freqs:\n",
    "            break\n",
    "\n",
    "        most_frequent = pair_freqs.most_common(1)[0]\n",
    "        pair, freq = most_frequent\n",
    "        merged = ''.join(pair)\n",
    "\n",
    "        print(f\"\\nMerge {merge_step + 1}:\")\n",
    "        print(f\"  Most frequent pair: {pair[0]} + {pair[1]} = '{merged}' (appears {freq} times)\")\n",
    "\n",
    "        # Show before/after for each word\n",
    "        print(f\"  Effects on vocabulary:\")\n",
    "        new_word_freqs = {}\n",
    "        for word, word_freq in word_freqs.items():\n",
    "            if pair[0] in word and pair[1] in word:\n",
    "                new_word = bpe._merge_pair(word, pair, merged)\n",
    "                if word != new_word:\n",
    "                    print(f\"    {''.join(word):20s} -> {''.join(new_word)}\")\n",
    "                new_word_freqs[new_word] = word_freq\n",
    "            else:\n",
    "                new_word_freqs[word] = word_freq\n",
    "\n",
    "        word_freqs = new_word_freqs\n",
    "        bpe.merges.append((pair, merged))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL VOCABULARY STRUCTURE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"\\nVocabulary layers:\")\n",
    "    print(\"  Layer 1 (Base): Individual characters\")\n",
    "    print(\"  Layer 2: Common pairs (e.g., 'es', 'st', 'er')\")\n",
    "    print(\"  Layer 3: Longer subwords (e.g., 'est', 'low', 'new')\")\n",
    "    print(\"  Layer 4+: Full words (e.g., 'lowest', 'newest')\")\n",
    "\n",
    "    print(\"\\nThis hierarchical structure allows:\")\n",
    "    print(\"  \u2713 Efficient representation of common words\")\n",
    "    print(\"  \u2713 Graceful handling of rare/OOV words\")\n",
    "    print(\"  \u2713 Balanced between vocabulary size and sequence length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514dbb90",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 6: PRACTICAL INTERVIEW INSIGHTS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f2ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interview_key_points():\n",
    "    \"\"\"\n",
    "    Summary of key points for interviews.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"INTERVIEW KEY POINTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"\"\"\n",
    "    Q1: What is tokenization?\n",
    "    -------------------------\n",
    "    Answer:\n",
    "    Tokenization is the process of breaking down text into smaller units called\n",
    "    tokens, which are the basic units that machine learning models process.\n",
    "\n",
    "    Three main approaches:\n",
    "    1. Word-level: Split on whitespace/punctuation\n",
    "       - Pro: Intuitive, preserves word semantics\n",
    "       - Con: Large vocabulary, poor OOV handling\n",
    "\n",
    "    2. Character-level: Individual characters\n",
    "       - Pro: Small vocabulary, no OOV problem\n",
    "       - Con: Very long sequences, loses word structure\n",
    "\n",
    "    3. Subword-level (BPE/WordPiece): Data-driven\n",
    "       - Pro: Balanced, handles OOV, used in modern LLMs\n",
    "       - Con: Not linguistically motivated\n",
    "\n",
    "    Modern LLMs use subword tokenization (BPE variants).\n",
    "\n",
    "\n",
    "    Q16: How do LLMs handle OOV (Out-of-Vocabulary) words?\n",
    "    --------------------------------------------------------\n",
    "    Answer:\n",
    "    Modern LLMs use subword tokenization (BPE, WordPiece, SentencePiece) which\n",
    "    handles OOV words by breaking them into known subword units.\n",
    "\n",
    "    Process:\n",
    "    1. Word not in vocabulary \u2192 Break into smaller subwords\n",
    "    2. Continue until all pieces are in vocabulary\n",
    "    3. In worst case \u2192 Break down to characters (always in vocab)\n",
    "\n",
    "    Example: \"unhappiness\" (OOV) \u2192 [\"un\", \"happiness\"] or [\"un\", \"happi\", \"ness\"]\n",
    "\n",
    "    Advantages:\n",
    "    - Open vocabulary (can represent any text)\n",
    "    - No information loss from <UNK> tokens\n",
    "    - Model learns compositional semantics\n",
    "    - More efficient than character-level\n",
    "\n",
    "    This is why GPT can handle:\n",
    "    - Rare words: \"antidisestablishmentarianism\"\n",
    "    - Neologisms: \"COVID-19\", \"blockchain\"\n",
    "    - Typos: \"helllo\" \u2192 [\"hell\", \"lo\"]\n",
    "    - Code: \"def_function_name\" \u2192 [\"def\", \"_\", \"function\", \"_\", \"name\"]\n",
    "\n",
    "\n",
    "    Implementation in practice:\n",
    "    --------------------------\n",
    "    - GPT: BPE (Byte-Pair Encoding)\n",
    "    - BERT: WordPiece (similar to BPE)\n",
    "    - T5/ALBERT: SentencePiece (works directly on Unicode)\n",
    "    - RoBERTa: Byte-level BPE (handles any byte sequence)\n",
    "\n",
    "\n",
    "    Important details for interviews:\n",
    "    ---------------------------------\n",
    "    1. Vocabulary size typical ranges:\n",
    "       - GPT-2: 50,257 tokens\n",
    "       - BERT: 30,522 tokens\n",
    "       - GPT-3: 50,257 tokens\n",
    "\n",
    "    2. Special tokens:\n",
    "       - <PAD>: Padding for batch processing\n",
    "       - <UNK>: Unknown (rarely used with BPE)\n",
    "       - <BOS>/<EOS>: Begin/End of sequence\n",
    "       - <SEP>/<CLS>: BERT-specific\n",
    "\n",
    "    3. Trade-offs:\n",
    "       - Larger vocab \u2192 shorter sequences, larger embedding matrix\n",
    "       - Smaller vocab \u2192 longer sequences, more computation\n",
    "       - Sweet spot: 30k-50k tokens for most applications\n",
    "\n",
    "    4. Why this matters:\n",
    "       - Affects model size (embedding matrix)\n",
    "       - Affects inference speed (sequence length)\n",
    "       - Affects model's ability to generalize\n",
    "       - Critical for multilingual models\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f0fee",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "MAIN EXECUTION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaf700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Run all demonstrations.\n",
    "    \"\"\"\n",
    "    print(\"\"\"\n",
    "    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "    \u2551                                                                       \u2551\n",
    "    \u2551           TOKENIZATION DEMO FOR LLM INTERVIEW PREPARATION            \u2551\n",
    "    \u2551                                                                       \u2551\n",
    "    \u2551  This demo covers:                                                    \u2551\n",
    "    \u2551  - Q1: What is tokenization?                                         \u2551\n",
    "    \u2551  - Q16: How do LLMs handle OOV words?                                \u2551\n",
    "    \u2551                                                                       \u2551\n",
    "    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "    \"\"\")\n",
    "\n",
    "    # Run all demonstrations\n",
    "    compare_tokenization_methods()\n",
    "    demonstrate_oov_handling()\n",
    "    visualize_vocabulary_building()\n",
    "\n",
    "    # Show interview key points\n",
    "    interview_key_points()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DEMO COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nNext steps for interview preparation:\")\n",
    "    print(\"  1. Re-read the code and understand each section\")\n",
    "    print(\"  2. Practice explaining BPE algorithm in your own words\")\n",
    "    print(\"  3. Be ready to discuss trade-offs of different approaches\")\n",
    "    print(\"  4. Know vocabulary sizes of popular models (GPT, BERT)\")\n",
    "    print(\"  5. Understand why subword tokenization is the standard\")\n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389331c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}