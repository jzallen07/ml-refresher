{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Gradient Flow Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c1bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient Flow and Optimization Demo for LLM Interviews\n",
    "=======================================================\n",
    "\n",
    "This demo covers key concepts for interview questions:\n",
    "- Q26: How do embeddings handle gradient flow with sparse updates?\n",
    "- Q27: How do gradients flow through transformer layers?\n",
    "- Q48: Hyperparameter sensitivity in LLM training\n",
    "\n",
    "Key Concepts Demonstrated:\n",
    "1. Embedding gradient sparsity and updates\n",
    "2. Gradient flow through neural networks\n",
    "3. Learning rate effects on convergence\n",
    "4. Hyperparameter sensitivity\n",
    "5. Gradient clipping for stability\n",
    "\n",
    "Author: Educational Demo\n",
    "Date: 2025-12-03\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d14e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbc01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "VIZ_DIR = Path(\"/Users/zack/dev/ml-refresher/data/interview_viz\")\n",
    "VIZ_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecacb1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"GRADIENT FLOW AND OPTIMIZATION DEMO FOR LLM INTERVIEWS\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796a1a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 1: EMBEDDING GRADIENT SPARSITY (Q26)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: EMBEDDING GRADIENT VISUALIZATION - SPARSE UPDATES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Interview Talking Points:\n",
    "- Embeddings only update for tokens present in the batch\n",
    "- This creates sparse gradient updates (most embedding rows get zero gradient)\n",
    "- Rare tokens update less frequently than common tokens\n",
    "- This is memory-efficient: only active embeddings need gradient computation\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862a016",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demonstrate_embedding_gradients():\n",
    "    \"\"\"\n",
    "    Show how embedding gradients are sparse - only active tokens get updates.\n",
    "\n",
    "    Key Interview Point: In a batch, only the embeddings corresponding to\n",
    "    tokens actually present receive gradient updates. This is why embedding\n",
    "    layers are memory-efficient during training.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Setting up Embedding Layer ---\")\n",
    "\n",
    "    # Create embedding layer\n",
    "    vocab_size = 1000\n",
    "    embed_dim = 128\n",
    "    embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    print(f\"Vocabulary Size: {vocab_size}\")\n",
    "    print(f\"Embedding Dimension: {embed_dim}\")\n",
    "    print(f\"Total Parameters: {vocab_size * embed_dim:,}\")\n",
    "\n",
    "    # Sample batch with only a few unique tokens\n",
    "    batch_size = 8\n",
    "    seq_length = 10\n",
    "\n",
    "    # Use only tokens 5, 10, 15, 20 (4 unique tokens out of 1000)\n",
    "    input_tokens = torch.tensor([\n",
    "        [5, 10, 15, 20, 5, 10, 15, 20, 5, 10],\n",
    "        [10, 15, 20, 5, 10, 15, 20, 5, 10, 15],\n",
    "        [15, 20, 5, 10, 15, 20, 5, 10, 15, 20],\n",
    "        [20, 5, 10, 15, 20, 5, 10, 15, 20, 5],\n",
    "        [5, 10, 15, 20, 5, 10, 15, 20, 5, 10],\n",
    "        [10, 15, 20, 5, 10, 15, 20, 5, 10, 15],\n",
    "        [15, 20, 5, 10, 15, 20, 5, 10, 15, 20],\n",
    "        [20, 5, 10, 15, 20, 5, 10, 15, 20, 5],\n",
    "    ])\n",
    "\n",
    "    unique_tokens = torch.unique(input_tokens)\n",
    "    print(f\"\\nBatch shape: {input_tokens.shape}\")\n",
    "    print(f\"Unique tokens in batch: {unique_tokens.tolist()}\")\n",
    "    print(f\"Token frequency in batch:\")\n",
    "    for token in unique_tokens:\n",
    "        count = (input_tokens == token).sum().item()\n",
    "        print(f\"  Token {token}: {count} occurrences\")\n",
    "\n",
    "    # Forward pass\n",
    "    embedded = embedding(input_tokens)\n",
    "\n",
    "    # Create a simple loss (mean of embeddings)\n",
    "    loss = embedded.mean()\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Analyze gradient sparsity\n",
    "    print(\"\\n--- Analyzing Gradient Sparsity ---\")\n",
    "    grad = embedding.weight.grad\n",
    "\n",
    "    # Count how many embedding rows have non-zero gradients\n",
    "    non_zero_rows = (grad.abs().sum(dim=1) > 0).sum().item()\n",
    "    zero_rows = vocab_size - non_zero_rows\n",
    "\n",
    "    print(f\"Embeddings with gradients: {non_zero_rows}/{vocab_size}\")\n",
    "    print(f\"Embeddings with zero gradients: {zero_rows}/{vocab_size}\")\n",
    "    print(f\"Sparsity: {100 * zero_rows / vocab_size:.2f}%\")\n",
    "\n",
    "    print(\"\\nGradient norms for active tokens:\")\n",
    "    for token in unique_tokens:\n",
    "        grad_norm = grad[token].norm().item()\n",
    "        print(f\"  Token {token}: gradient norm = {grad_norm:.6f}\")\n",
    "\n",
    "    print(\"\\nGradient norms for inactive tokens (should be zero):\")\n",
    "    inactive_tokens = [0, 1, 2, 100, 500]\n",
    "    for token in inactive_tokens:\n",
    "        grad_norm = grad[token].norm().item()\n",
    "        print(f\"  Token {token}: gradient norm = {grad_norm:.6f}\")\n",
    "\n",
    "    # Visualize gradient sparsity\n",
    "    print(\"\\n--- Creating Visualization ---\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # Plot 1: Gradient norm per embedding row\n",
    "    ax = axes[0, 0]\n",
    "    grad_norms = grad.norm(dim=1).detach().numpy()\n",
    "    ax.plot(grad_norms, alpha=0.7, linewidth=0.5)\n",
    "    ax.scatter(unique_tokens.numpy(), grad_norms[unique_tokens.numpy()],\n",
    "               color='red', s=100, zorder=5, label='Active tokens')\n",
    "    ax.set_xlabel('Embedding Index')\n",
    "    ax.set_ylabel('Gradient Norm')\n",
    "    ax.set_title('Gradient Sparsity: Only Active Tokens Updated')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    # Plot 2: Gradient distribution (log scale)\n",
    "    ax = axes[0, 1]\n",
    "    non_zero_grads = grad_norms[grad_norms > 0]\n",
    "    ax.hist(non_zero_grads, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Gradient Norm')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Distribution of Non-Zero Gradients (n={len(non_zero_grads)})')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    # Plot 3: Heatmap of gradients for active tokens\n",
    "    ax = axes[1, 0]\n",
    "    active_grads = grad[unique_tokens].detach().numpy()\n",
    "    im = ax.imshow(active_grads, aspect='auto', cmap='RdBu_r',\n",
    "                   vmin=-active_grads.std()*3, vmax=active_grads.std()*3)\n",
    "    ax.set_xlabel('Embedding Dimension')\n",
    "    ax.set_ylabel('Active Token ID')\n",
    "    ax.set_yticks(range(len(unique_tokens)))\n",
    "    ax.set_yticklabels([f'Token {t}' for t in unique_tokens.tolist()])\n",
    "    ax.set_title('Gradient Values for Active Tokens')\n",
    "    plt.colorbar(im, ax=ax, label='Gradient Value')\n",
    "\n",
    "    # Plot 4: Sparsity statistics\n",
    "    ax = axes[1, 1]\n",
    "    categories = ['Active\\nEmbeddings', 'Zero-Gradient\\nEmbeddings']\n",
    "    values = [non_zero_rows, zero_rows]\n",
    "    colors = ['#2ecc71', '#e74c3c']\n",
    "    bars = ax.bar(categories, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Embedding Gradient Sparsity')\n",
    "\n",
    "    # Add percentage labels on bars\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val}\\n({100*val/vocab_size:.1f}%)',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VIZ_DIR / 'embedding_gradient_sparsity.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {VIZ_DIR / 'embedding_gradient_sparsity.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "    return grad_norms, unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33f590",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "grad_norms, active_tokens = demonstrate_embedding_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cf75e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 2: GRADIENT FLOW THROUGH NETWORK LAYERS (Q27)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: GRADIENT FLOW THROUGH NEURAL NETWORK\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Interview Talking Points:\n",
    "- Gradients can vanish (shrink) or explode (grow) through layers\n",
    "- Deep networks need careful initialization and normalization\n",
    "- Monitor gradient magnitudes at each layer during training\n",
    "- Residual connections help maintain gradient flow\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee462c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimpleTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified transformer block for gradient flow analysis.\n",
    "\n",
    "    Interview Point: Transformers use residual connections and layer norm\n",
    "    to maintain healthy gradient flow through many layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.attention = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        # Feed-forward with residual\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172924b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demonstrate_gradient_flow():\n",
    "    \"\"\"\n",
    "    Track gradient magnitudes as they flow backward through layers.\n",
    "\n",
    "    Key Interview Point: In well-designed networks (like transformers),\n",
    "    gradients should have similar magnitudes across layers. Large variations\n",
    "    indicate potential training problems.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building Multi-Layer Network ---\")\n",
    "\n",
    "    # Create a simple deep network\n",
    "    d_model = 64\n",
    "    num_layers = 6\n",
    "    batch_size = 4\n",
    "    seq_length = 8\n",
    "\n",
    "    # Build stacked transformer blocks\n",
    "    layers = [SimpleTransformerBlock(d_model) for _ in range(num_layers)]\n",
    "    model = nn.Sequential(*layers)\n",
    "\n",
    "    print(f\"Model depth: {num_layers} transformer blocks\")\n",
    "    print(f\"Hidden dimension: {d_model}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # Create input\n",
    "    x = torch.randn(batch_size, seq_length, d_model)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "    # Forward pass\n",
    "    print(\"\\n--- Forward Pass ---\")\n",
    "    activations = [x]\n",
    "    current = x\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        current = layer(current)\n",
    "        activations.append(current)\n",
    "        print(f\"Layer {i+1} output: mean={current.mean():.4f}, std={current.std():.4f}\")\n",
    "\n",
    "    # Compute loss\n",
    "    loss = current.mean()\n",
    "    print(f\"\\nLoss: {loss.item():.6f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    print(\"\\n--- Backward Pass - Gradient Flow ---\")\n",
    "    loss.backward()\n",
    "\n",
    "    # Collect gradient statistics for each layer\n",
    "    gradient_stats = []\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        layer_grads = []\n",
    "        for name, param in layer.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                grad_mean = param.grad.mean().item()\n",
    "                grad_std = param.grad.std().item()\n",
    "                layer_grads.append(grad_norm)\n",
    "                print(f\"Layer {i+1} - {name}: norm={grad_norm:.6f}, \"\n",
    "                      f\"mean={grad_mean:.6e}, std={grad_std:.6e}\")\n",
    "\n",
    "        avg_grad_norm = np.mean(layer_grads)\n",
    "        gradient_stats.append({\n",
    "            'layer': i + 1,\n",
    "            'avg_norm': avg_grad_norm,\n",
    "            'min_norm': min(layer_grads),\n",
    "            'max_norm': max(layer_grads)\n",
    "        })\n",
    "\n",
    "    print(\"\\n--- Gradient Flow Summary ---\")\n",
    "    print(f\"{'Layer':<10} {'Avg Grad Norm':<15} {'Min':<15} {'Max':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    for stat in gradient_stats:\n",
    "        print(f\"{stat['layer']:<10} {stat['avg_norm']:<15.6f} \"\n",
    "              f\"{stat['min_norm']:<15.6f} {stat['max_norm']:<15.6f}\")\n",
    "\n",
    "    # Visualize gradient flow\n",
    "    print(\"\\n--- Creating Gradient Flow Visualization ---\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # Plot 1: Gradient norms by layer\n",
    "    ax = axes[0, 0]\n",
    "    layers_idx = [s['layer'] for s in gradient_stats]\n",
    "    avg_norms = [s['avg_norm'] for s in gradient_stats]\n",
    "    min_norms = [s['min_norm'] for s in gradient_stats]\n",
    "    max_norms = [s['max_norm'] for s in gradient_stats]\n",
    "\n",
    "    ax.plot(layers_idx, avg_norms, 'o-', linewidth=2, markersize=8, label='Average')\n",
    "    ax.fill_between(layers_idx, min_norms, max_norms, alpha=0.3, label='Min-Max Range')\n",
    "    ax.set_xlabel('Layer Number (1=earliest, 6=latest)')\n",
    "    ax.set_ylabel('Gradient Norm')\n",
    "    ax.set_title('Gradient Flow Through Layers')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_xticks(layers_idx)\n",
    "\n",
    "    # Plot 2: Gradient norm ratios (detect vanishing/exploding)\n",
    "    ax = axes[0, 1]\n",
    "    if len(avg_norms) > 1:\n",
    "        ratios = [avg_norms[i] / avg_norms[i+1] for i in range(len(avg_norms)-1)]\n",
    "        ax.plot(range(1, len(ratios)+1), ratios, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "        ax.axhline(y=1.0, color='red', linestyle='--', label='Ratio = 1 (ideal)')\n",
    "        ax.fill_between(range(1, len(ratios)+1), 0.5, 2.0, alpha=0.2, color='green',\n",
    "                        label='Healthy range (0.5-2.0)')\n",
    "        ax.set_xlabel('Layer Transition')\n",
    "        ax.set_ylabel('Gradient Norm Ratio (layer_i / layer_i+1)')\n",
    "        ax.set_title('Gradient Stability Across Layers')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    # Plot 3: Activation statistics through layers\n",
    "    ax = axes[1, 0]\n",
    "    activation_means = [a.mean().item() for a in activations]\n",
    "    activation_stds = [a.std().item() for a in activations]\n",
    "\n",
    "    x_pos = range(len(activations))\n",
    "    ax.plot(x_pos, activation_means, 'o-', label='Mean', linewidth=2, markersize=8)\n",
    "    ax.plot(x_pos, activation_stds, 's-', label='Std Dev', linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('Layer (0=input)')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('Activation Statistics Through Network')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_xticks(x_pos)\n",
    "\n",
    "    # Plot 4: Gradient distribution (all parameters)\n",
    "    ax = axes[1, 1]\n",
    "    all_grads = []\n",
    "    for layer in layers:\n",
    "        for param in layer.parameters():\n",
    "            if param.grad is not None:\n",
    "                all_grads.extend(param.grad.flatten().detach().numpy())\n",
    "\n",
    "    all_grads = np.array(all_grads)\n",
    "    ax.hist(all_grads, bins=100, alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Gradient Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Gradient Distribution (n={len(all_grads):,})')\n",
    "    ax.set_yscale('log')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VIZ_DIR / 'gradient_flow_through_layers.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {VIZ_DIR / 'gradient_flow_through_layers.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "    return gradient_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eebed5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "gradient_stats = demonstrate_gradient_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e005a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3: LEARNING RATE EXPERIMENTS (Q48)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3: LEARNING RATE SENSITIVITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Interview Talking Points:\n",
    "- Learning rate is the most important hyperparameter\n",
    "- Too high: training unstable, loss diverges\n",
    "- Too low: training too slow, may get stuck in local minima\n",
    "- LLMs typically use learning rate warmup and decay schedules\n",
    "- Common starting point: 3e-4 to 1e-3 for Adam optimizer\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f9e2d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_with_learning_rate(lr: float, num_steps: int = 100) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train a simple model with a specific learning rate and track metrics.\n",
    "\n",
    "    Interview Point: Demonstrates how learning rate affects convergence\n",
    "    speed and stability.\n",
    "    \"\"\"\n",
    "    # Simple 2-layer network for a toy task\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(10, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, 1)\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Simple regression task: learn to predict sum of inputs\n",
    "    losses = []\n",
    "    grad_norms = []\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Generate random data\n",
    "        x = torch.randn(32, 10)\n",
    "        y = x.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(x)\n",
    "        loss = F.mse_loss(pred, y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Track gradient norm\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm += p.grad.norm().item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        grad_norms.append(total_norm)\n",
    "\n",
    "    return losses, grad_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c556ef9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demonstrate_learning_rate_effects():\n",
    "    \"\"\"\n",
    "    Compare training with different learning rates.\n",
    "\n",
    "    Key Interview Point: Learning rate selection significantly impacts\n",
    "    training dynamics. LLMs use sophisticated schedules (warmup, cosine decay)\n",
    "    to balance fast convergence and stability.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Testing Multiple Learning Rates ---\")\n",
    "\n",
    "    learning_rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    results = {}\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTraining with LR={lr:.0e}\")\n",
    "        losses, grad_norms = train_with_learning_rate(lr, num_steps=100)\n",
    "\n",
    "        final_loss = losses[-1]\n",
    "        min_loss = min(losses)\n",
    "        avg_grad = np.mean(grad_norms)\n",
    "\n",
    "        print(f\"  Final loss: {final_loss:.6f}\")\n",
    "        print(f\"  Min loss: {min_loss:.6f}\")\n",
    "        print(f\"  Avg gradient norm: {avg_grad:.6f}\")\n",
    "\n",
    "        # Check for divergence\n",
    "        if np.isnan(final_loss) or final_loss > 100:\n",
    "            print(f\"  \u26a0\ufe0f  DIVERGED - Learning rate too high!\")\n",
    "        elif final_loss < 0.01:\n",
    "            print(f\"  \u2713 CONVERGED - Good learning rate\")\n",
    "        else:\n",
    "            print(f\"  \u26a0\ufe0f  SLOW - Learning rate might be too low\")\n",
    "\n",
    "        results[lr] = {\n",
    "            'losses': losses,\n",
    "            'grad_norms': grad_norms,\n",
    "            'final_loss': final_loss,\n",
    "            'min_loss': min_loss\n",
    "        }\n",
    "\n",
    "    # Visualize learning rate comparison\n",
    "    print(\"\\n--- Creating Learning Rate Comparison ---\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # Plot 1: Loss curves for all learning rates\n",
    "    ax = axes[0, 0]\n",
    "    for lr, data in results.items():\n",
    "        losses = data['losses']\n",
    "        # Clip extreme values for visualization\n",
    "        losses_clipped = np.clip(losses, 0, 10)\n",
    "        ax.plot(losses_clipped, label=f'LR={lr:.0e}', linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Loss (clipped at 10)')\n",
    "    ax.set_title('Loss Curves: Learning Rate Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    # Plot 2: Final loss vs learning rate\n",
    "    ax = axes[0, 1]\n",
    "    lrs = list(results.keys())\n",
    "    final_losses = [results[lr]['final_loss'] for lr in lrs]\n",
    "    final_losses_clipped = np.clip(final_losses, 1e-6, 100)\n",
    "\n",
    "    ax.plot(lrs, final_losses_clipped, 'o-', linewidth=2, markersize=10, color='purple')\n",
    "    ax.set_xlabel('Learning Rate')\n",
    "    ax.set_ylabel('Final Loss')\n",
    "    ax.set_title('Final Loss vs Learning Rate')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    # Mark the optimal LR\n",
    "    best_lr = min(results.keys(), key=lambda lr: results[lr]['final_loss'])\n",
    "    best_loss = results[best_lr]['final_loss']\n",
    "    ax.scatter([best_lr], [best_loss], color='red', s=200, zorder=5,\n",
    "               marker='*', label=f'Best: {best_lr:.0e}')\n",
    "    ax.legend()\n",
    "\n",
    "    # Plot 3: Gradient norms\n",
    "    ax = axes[1, 0]\n",
    "    for lr, data in results.items():\n",
    "        grad_norms = data['grad_norms']\n",
    "        ax.plot(grad_norms, label=f'LR={lr:.0e}', linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Gradient Norm')\n",
    "    ax.set_title('Gradient Norms During Training')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    # Plot 4: Learning rate recommendations\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "\n",
    "    recommendations = \"\"\"\n",
    "Learning Rate Guidelines for LLMs:\n",
    "\n",
    "1. TYPICAL RANGES:\n",
    "   \u2022 Small models: 1e-3 to 1e-4\n",
    "   \u2022 Large models: 1e-4 to 3e-5\n",
    "   \u2022 Fine-tuning: 1e-5 to 1e-6\n",
    "\n",
    "2. WARMUP STRATEGY:\n",
    "   \u2022 Start with small LR (e.g., 1e-6)\n",
    "   \u2022 Linearly increase to max LR\n",
    "   \u2022 Typical warmup: 2-10% of total steps\n",
    "\n",
    "3. DECAY SCHEDULE:\n",
    "   \u2022 Cosine decay (most common)\n",
    "   \u2022 Linear decay\n",
    "   \u2022 Step decay\n",
    "\n",
    "4. WARNING SIGNS:\n",
    "   \u2022 Loss spikes: LR too high\n",
    "   \u2022 No improvement: LR too low\n",
    "   \u2022 NaN/Inf: Definitely too high!\n",
    "\n",
    "5. ADAPTIVE OPTIMIZERS:\n",
    "   \u2022 Adam: Most common for LLMs\n",
    "   \u2022 AdamW: Adam with weight decay\n",
    "   \u2022 Learning rate still crucial!\n",
    "\"\"\"\n",
    "\n",
    "    ax.text(0.05, 0.95, recommendations, transform=ax.transAxes,\n",
    "            fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VIZ_DIR / 'learning_rate_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {VIZ_DIR / 'learning_rate_comparison.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275c344",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "lr_results = demonstrate_learning_rate_effects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c867ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 4: HYPERPARAMETER SENSITIVITY ANALYSIS (Q48)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 4: HYPERPARAMETER SENSITIVITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Interview Talking Points:\n",
    "- Different hyperparameters have different sensitivity levels\n",
    "- Most sensitive: learning rate, batch size, model size\n",
    "- Moderately sensitive: warmup steps, weight decay, dropout\n",
    "- Less sensitive: optimizer choice (within Adam family)\n",
    "- Always validate hyperparameters on a smaller dataset first\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644b330",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_with_hyperparameters(\n",
    "    lr: float = 1e-3,\n",
    "    batch_size: int = 32,\n",
    "    weight_decay: float = 0.01,\n",
    "    dropout: float = 0.1,\n",
    "    num_steps: int = 50\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train with specific hyperparameters and return training metrics.\n",
    "\n",
    "    Interview Point: Shows how to systematically test hyperparameter sensitivity.\n",
    "    \"\"\"\n",
    "    # Model with dropout\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(10, 50),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, 1)\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Generate batch\n",
    "        x = torch.randn(batch_size, 10)\n",
    "        y = x.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Training step\n",
    "        pred = model(x)\n",
    "        loss = F.mse_loss(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return {'losses': losses, 'final_loss': losses[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972abae2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demonstrate_hyperparameter_sensitivity():\n",
    "    \"\"\"\n",
    "    Systematically vary hyperparameters to show their impact.\n",
    "\n",
    "    Key Interview Point: Understanding which hyperparameters matter most\n",
    "    helps prioritize tuning efforts and debug training issues.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Testing Hyperparameter Sensitivity ---\")\n",
    "\n",
    "    # Baseline\n",
    "    baseline_config = {\n",
    "        'lr': 1e-3,\n",
    "        'batch_size': 32,\n",
    "        'weight_decay': 0.01,\n",
    "        'dropout': 0.1\n",
    "    }\n",
    "\n",
    "    print(\"\\nBaseline configuration:\")\n",
    "    for k, v in baseline_config.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    # Test variations of each hyperparameter\n",
    "    experiments = {\n",
    "        'learning_rate': {\n",
    "            'param': 'lr',\n",
    "            'values': [1e-2, 5e-3, 1e-3, 5e-4, 1e-4],\n",
    "            'results': []\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'param': 'batch_size',\n",
    "            'values': [8, 16, 32, 64, 128],\n",
    "            'results': []\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'param': 'weight_decay',\n",
    "            'values': [0.0, 0.001, 0.01, 0.1, 0.5],\n",
    "            'results': []\n",
    "        },\n",
    "        'dropout': {\n",
    "            'param': 'dropout',\n",
    "            'values': [0.0, 0.05, 0.1, 0.2, 0.3],\n",
    "            'results': []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Run experiments\n",
    "    for exp_name, exp_config in experiments.items():\n",
    "        print(f\"\\n--- Testing {exp_name} sensitivity ---\")\n",
    "        param_name = exp_config['param']\n",
    "\n",
    "        for value in exp_config['values']:\n",
    "            config = baseline_config.copy()\n",
    "            config[param_name] = value\n",
    "\n",
    "            print(f\"  {param_name}={value}...\", end=\" \")\n",
    "            result = train_with_hyperparameters(**config, num_steps=50)\n",
    "            exp_config['results'].append(result['final_loss'])\n",
    "            print(f\"final_loss={result['final_loss']:.6f}\")\n",
    "\n",
    "    # Visualize sensitivity\n",
    "    print(\"\\n--- Creating Hyperparameter Sensitivity Visualization ---\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    exp_names = list(experiments.keys())\n",
    "\n",
    "    for idx, (exp_name, ax) in enumerate(zip(exp_names, axes.flat)):\n",
    "        exp_config = experiments[exp_name]\n",
    "        values = exp_config['values']\n",
    "        results = exp_config['results']\n",
    "\n",
    "        # Plot\n",
    "        ax.plot(range(len(values)), results, 'o-', linewidth=2, markersize=10)\n",
    "        ax.set_xticks(range(len(values)))\n",
    "        ax.set_xticklabels([str(v) for v in values], rotation=45)\n",
    "        ax.set_xlabel(exp_name.replace('_', ' ').title())\n",
    "        ax.set_ylabel('Final Loss')\n",
    "        ax.set_title(f'Sensitivity to {exp_name.replace(\"_\", \" \").title()}')\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "        # Mark baseline if present\n",
    "        param_name = exp_config['param']\n",
    "        baseline_val = baseline_config[param_name]\n",
    "        if baseline_val in values:\n",
    "            baseline_idx = values.index(baseline_val)\n",
    "            ax.scatter([baseline_idx], [results[baseline_idx]],\n",
    "                      color='red', s=200, zorder=5, marker='*',\n",
    "                      label='Baseline')\n",
    "            ax.legend()\n",
    "\n",
    "        # Calculate sensitivity score (coefficient of variation)\n",
    "        cv = np.std(results) / np.mean(results) * 100\n",
    "        ax.text(0.02, 0.98, f'CV: {cv:.1f}%', transform=ax.transAxes,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round',\n",
    "                facecolor='yellow', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VIZ_DIR / 'hyperparameter_sensitivity.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {VIZ_DIR / 'hyperparameter_sensitivity.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "    # Print sensitivity ranking\n",
    "    print(\"\\n--- Hyperparameter Sensitivity Ranking ---\")\n",
    "    sensitivities = {}\n",
    "    for exp_name, exp_config in experiments.items():\n",
    "        results = exp_config['results']\n",
    "        cv = np.std(results) / np.mean(results) * 100\n",
    "        sensitivities[exp_name] = cv\n",
    "\n",
    "    ranked = sorted(sensitivities.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nRanked by sensitivity (coefficient of variation):\")\n",
    "    for rank, (name, cv) in enumerate(ranked, 1):\n",
    "        sensitivity_level = \"HIGH\" if cv > 50 else \"MEDIUM\" if cv > 20 else \"LOW\"\n",
    "        print(f\"{rank}. {name:20s}: {cv:6.2f}% - {sensitivity_level}\")\n",
    "\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15545a67",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hyperparam_results = demonstrate_hyperparameter_sensitivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf8869",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 5: GRADIENT CLIPPING DEMONSTRATION (Q48)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 5: GRADIENT CLIPPING FOR TRAINING STABILITY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Interview Talking Points:\n",
    "- Gradient clipping prevents exploding gradients\n",
    "- Two methods: clip by norm (most common) or clip by value\n",
    "- Typical threshold: 1.0 for clip by norm\n",
    "- Essential for training RNNs and deep transformers\n",
    "- Doesn't solve vanishing gradients (need better architecture for that)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4b916",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_with_gradient_clipping(\n",
    "    clip_value: float = None,\n",
    "    num_steps: int = 100\n",
    ") -> Tuple[List[float], List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train a model with gradient clipping and track metrics.\n",
    "\n",
    "    Interview Point: Gradient clipping is crucial for stable training,\n",
    "    especially with large models or variable-length sequences.\n",
    "    \"\"\"\n",
    "    # Deeper model more prone to gradient issues\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(10, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 1)\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)  # Higher LR to induce instability\n",
    "\n",
    "    losses = []\n",
    "    grad_norms_before = []\n",
    "    grad_norms_after = []\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Generate data with occasional large values (simulating difficult examples)\n",
    "        x = torch.randn(32, 10)\n",
    "        if step % 10 == 0:  # Occasional spike\n",
    "            x = x * 5\n",
    "        y = x.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(x)\n",
    "        loss = F.mse_loss(pred, y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute gradient norm before clipping\n",
    "        total_norm_before = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm_before += p.grad.norm().item() ** 2\n",
    "        total_norm_before = total_norm_before ** 0.5\n",
    "        grad_norms_before.append(total_norm_before)\n",
    "\n",
    "        # Apply gradient clipping if specified\n",
    "        if clip_value is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "        # Compute gradient norm after clipping\n",
    "        total_norm_after = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm_after += p.grad.norm().item() ** 2\n",
    "        total_norm_after = total_norm_after ** 0.5\n",
    "        grad_norms_after.append(total_norm_after)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses, grad_norms_before, grad_norms_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816bce1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demonstrate_gradient_clipping():\n",
    "    \"\"\"\n",
    "    Compare training with and without gradient clipping.\n",
    "\n",
    "    Key Interview Point: Gradient clipping is a simple but effective technique\n",
    "    to prevent training instability from exploding gradients.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Testing Gradient Clipping ---\")\n",
    "\n",
    "    clip_values = [None, 0.5, 1.0, 5.0]\n",
    "    results = {}\n",
    "\n",
    "    for clip_val in clip_values:\n",
    "        label = \"No clipping\" if clip_val is None else f\"Clip={clip_val}\"\n",
    "        print(f\"\\nTraining with {label}\")\n",
    "\n",
    "        losses, grad_before, grad_after = train_with_gradient_clipping(\n",
    "            clip_value=clip_val, num_steps=100\n",
    "        )\n",
    "\n",
    "        print(f\"  Final loss: {losses[-1]:.6f}\")\n",
    "        print(f\"  Max gradient norm (before clip): {max(grad_before):.6f}\")\n",
    "        print(f\"  Max gradient norm (after clip): {max(grad_after):.6f}\")\n",
    "        print(f\"  Avg gradient norm (before clip): {np.mean(grad_before):.6f}\")\n",
    "        print(f\"  Avg gradient norm (after clip): {np.mean(grad_after):.6f}\")\n",
    "\n",
    "        # Count clipping events\n",
    "        if clip_val is not None:\n",
    "            clip_events = sum(1 for g in grad_before if g > clip_val)\n",
    "            print(f\"  Gradient clipping events: {clip_events}/{len(grad_before)}\")\n",
    "\n",
    "        results[label] = {\n",
    "            'losses': losses,\n",
    "            'grad_before': grad_before,\n",
    "            'grad_after': grad_after,\n",
    "            'clip_value': clip_val\n",
    "        }\n",
    "\n",
    "    # Visualize gradient clipping effects\n",
    "    print(\"\\n--- Creating Gradient Clipping Visualization ---\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # Plot 1: Loss curves\n",
    "    ax = axes[0, 0]\n",
    "    for label, data in results.items():\n",
    "        losses = np.clip(data['losses'], 0, 50)  # Clip for visualization\n",
    "        ax.plot(losses, label=label, linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Loss (clipped at 50)')\n",
    "    ax.set_title('Training Loss: Effect of Gradient Clipping')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    # Plot 2: Gradient norms before clipping\n",
    "    ax = axes[0, 1]\n",
    "    for label, data in results.items():\n",
    "        grad_norms = data['grad_before']\n",
    "        ax.plot(grad_norms, label=label, linewidth=2, alpha=0.8)\n",
    "\n",
    "        # Show clipping threshold\n",
    "        if data['clip_value'] is not None:\n",
    "            ax.axhline(y=data['clip_value'], linestyle='--', alpha=0.5,\n",
    "                      label=f\"{label} threshold\")\n",
    "\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Gradient Norm (before clipping)')\n",
    "    ax.set_title('Gradient Magnitudes Before Clipping')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    # Plot 3: Before vs After clipping for one example\n",
    "    ax = axes[1, 0]\n",
    "    clip_example = \"Clip=1.0\"\n",
    "    if clip_example in results:\n",
    "        data = results[clip_example]\n",
    "        steps = range(len(data['grad_before']))\n",
    "\n",
    "        ax.plot(steps, data['grad_before'], label='Before Clipping',\n",
    "                linewidth=2, alpha=0.8, color='red')\n",
    "        ax.plot(steps, data['grad_after'], label='After Clipping',\n",
    "                linewidth=2, alpha=0.8, color='green')\n",
    "        ax.axhline(y=data['clip_value'], linestyle='--', color='black',\n",
    "                  label=f\"Threshold={data['clip_value']}\")\n",
    "\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Gradient Norm')\n",
    "        ax.set_title(f'Gradient Clipping Effect (Threshold={data[\"clip_value\"]})')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    # Plot 4: Guidelines and best practices\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "\n",
    "    guidelines = \"\"\"\n",
    "Gradient Clipping Best Practices:\n",
    "\n",
    "1. WHEN TO USE:\n",
    "   \u2713 Training RNNs/LSTMs\n",
    "   \u2713 Very deep networks\n",
    "   \u2713 Variable sequence lengths\n",
    "   \u2713 When you see loss spikes\n",
    "\n",
    "2. TYPICAL THRESHOLDS:\n",
    "   \u2022 Transformers: 1.0 - 5.0\n",
    "   \u2022 RNNs: 0.5 - 1.0\n",
    "   \u2022 Very deep CNNs: 1.0 - 10.0\n",
    "\n",
    "3. TWO METHODS:\n",
    "   a) Clip by norm (most common):\n",
    "      torch.nn.utils.clip_grad_norm_(\n",
    "          model.parameters(), max_norm=1.0)\n",
    "\n",
    "   b) Clip by value:\n",
    "      torch.nn.utils.clip_grad_value_(\n",
    "          model.parameters(), clip_value=0.5)\n",
    "\n",
    "4. MONITORING:\n",
    "   \u2022 Log gradient norms\n",
    "   \u2022 Track clipping frequency\n",
    "   \u2022 Adjust threshold if clipping\n",
    "     happens too often (>50%)\n",
    "\n",
    "5. LIMITATIONS:\n",
    "   \u2717 Doesn't fix vanishing gradients\n",
    "   \u2717 Can slow convergence if too aggressive\n",
    "   \u2713 Simple and effective for stability\n",
    "\"\"\"\n",
    "\n",
    "    ax.text(0.05, 0.95, guidelines, transform=ax.transAxes,\n",
    "            fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VIZ_DIR / 'gradient_clipping_demo.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {VIZ_DIR / 'gradient_clipping_demo.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb445e13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "clipping_results = demonstrate_gradient_clipping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY AND KEY TAKEAWAYS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY: KEY INTERVIEW POINTS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc032789",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "\u2551                  GRADIENT FLOW & OPTIMIZATION - KEY POINTS                \u2551\n",
    "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "1. EMBEDDING GRADIENTS (Q26):\n",
    "   \u2022 Only embeddings for tokens in the batch receive gradient updates\n",
    "   \u2022 Creates sparse gradient updates (typically 99%+ sparsity)\n",
    "   \u2022 Memory efficient - don't need to compute/store all embedding gradients\n",
    "   \u2022 Rare tokens update less frequently than common tokens\n",
    "\n",
    "   Interview Answer: \"Embedding layers only update parameters for tokens\n",
    "   present in the current batch, making gradient updates very sparse. This\n",
    "   is memory efficient and scales well to large vocabularies.\"\n",
    "\n",
    "2. GRADIENT FLOW THROUGH LAYERS (Q27):\n",
    "   \u2022 Monitor gradient magnitudes at each layer\n",
    "   \u2022 Vanishing gradients: magnitudes shrink through layers (bad)\n",
    "   \u2022 Exploding gradients: magnitudes grow through layers (bad)\n",
    "   \u2022 Healthy: similar magnitudes across layers (good)\n",
    "   \u2022 Solutions: residual connections, layer norm, careful initialization\n",
    "\n",
    "   Interview Answer: \"In transformers, residual connections and layer\n",
    "   normalization help maintain stable gradient flow through many layers.\n",
    "   We can verify this by checking that gradient norms are similar across\n",
    "   layers during training.\"\n",
    "\n",
    "3. LEARNING RATE SELECTION (Q48):\n",
    "   \u2022 Most critical hyperparameter for training\n",
    "   \u2022 Too high: unstable training, divergence\n",
    "   \u2022 Too low: slow convergence, local minima\n",
    "   \u2022 LLMs typically use: warmup + cosine decay\n",
    "   \u2022 Starting points: 1e-4 to 3e-5 for large models\n",
    "\n",
    "   Interview Answer: \"Learning rate is the most sensitive hyperparameter.\n",
    "   LLMs typically use a warmup phase (2-10% of training) to stabilize\n",
    "   early training, followed by cosine decay to fine-tune convergence.\"\n",
    "\n",
    "4. HYPERPARAMETER SENSITIVITY (Q48):\n",
    "   \u2022 High sensitivity: learning rate, batch size\n",
    "   \u2022 Medium sensitivity: warmup steps, weight decay, dropout\n",
    "   \u2022 Lower sensitivity: optimizer choice (within Adam variants)\n",
    "   \u2022 Always validate on smaller scale before full training\n",
    "\n",
    "   Interview Answer: \"Learning rate and batch size are most sensitive.\n",
    "   I'd tune those first on a small dataset, then adjust regularization\n",
    "   (weight decay, dropout) as needed. Optimizer choice matters less -\n",
    "   AdamW is a safe default for LLMs.\"\n",
    "\n",
    "5. GRADIENT CLIPPING (Q48):\n",
    "   \u2022 Prevents exploding gradients\n",
    "   \u2022 Essential for RNNs and deep transformers\n",
    "   \u2022 Typical threshold: 1.0 for clip_grad_norm_\n",
    "   \u2022 Monitor: track gradient norms and clipping frequency\n",
    "   \u2022 Doesn't solve vanishing gradients (need architecture changes)\n",
    "\n",
    "   Interview Answer: \"Gradient clipping caps gradient norms to prevent\n",
    "   training instability. It's standard practice in LLM training, typically\n",
    "   with a max_norm of 1.0. We monitor clipping frequency - if it happens\n",
    "   too often, we may need to adjust the learning rate or threshold.\"\n",
    "\n",
    "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "\u2551                         PRACTICAL RECOMMENDATIONS                          \u2551\n",
    "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "For LLM Training:\n",
    "1. Start with AdamW optimizer (lr=3e-4, weight_decay=0.01)\n",
    "2. Use warmup for 2-10% of total steps\n",
    "3. Apply cosine decay schedule\n",
    "4. Enable gradient clipping (max_norm=1.0)\n",
    "5. Monitor: loss, gradient norms, learning rate\n",
    "6. Log everything - helps debug training issues\n",
    "\n",
    "For Debugging Training Issues:\n",
    "\u2022 Loss spikes \u2192 reduce LR or increase gradient clipping\n",
    "\u2022 Slow convergence \u2192 increase LR or check data\n",
    "\u2022 NaN/Inf \u2192 definitely reduce LR, check for numerical instability\n",
    "\u2022 Vanishing gradients \u2192 improve architecture (add residual connections)\n",
    "\u2022 Exploding gradients \u2192 gradient clipping, reduce LR\n",
    "\n",
    "For Interview Success:\n",
    "\u2713 Know the tradeoffs (not just \"best practices\")\n",
    "\u2713 Explain WHY techniques work, not just HOW\n",
    "\u2713 Be ready to discuss debugging strategies\n",
    "\u2713 Connect concepts to real training scenarios\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d7f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ce0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION FILES CREATED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAll visualizations saved to: {VIZ_DIR}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. embedding_gradient_sparsity.png\")\n",
    "print(\"  2. gradient_flow_through_layers.png\")\n",
    "print(\"  3. learning_rate_comparison.png\")\n",
    "print(\"  4. hyperparameter_sensitivity.png\")\n",
    "print(\"  5. gradient_clipping_demo.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0985da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEMO COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nYou now have comprehensive visualizations and explanations for:\")\n",
    "print(\"  \u2022 Embedding gradient sparsity\")\n",
    "print(\"  \u2022 Gradient flow through networks\")\n",
    "print(\"  \u2022 Learning rate effects\")\n",
    "print(\"  \u2022 Hyperparameter sensitivity\")\n",
    "print(\"  \u2022 Gradient clipping\")\n",
    "print(\"\\nReview the visualizations and practice explaining these concepts!\")\n",
    "print(\"Good luck with your LLM interviews! \ud83d\ude80\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}