{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Positional Encoding Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Positional Encoding in Transformers - Interview Preparation Demo\n",
    "\n",
    "This demo covers essential concepts for LLM interviews:\n",
    "- Q17: How do transformers handle sequence order?\n",
    "- Q21: What is positional encoding and why is it needed?\n",
    "- Q43: Sinusoidal vs learned positional embeddings\n",
    "- Q46: Residual connections and layer normalization\n",
    "\n",
    "Key Interview Points:\n",
    "1. Self-attention is permutation-invariant (order-agnostic)\n",
    "2. Positional encoding injects position information\n",
    "3. Sinusoidal encoding allows extrapolation to longer sequences\n",
    "4. Different encoding strategies have different tradeoffs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "output_dir = Path(\"/Users/zack/dev/ml-refresher/data/interview_viz\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb595c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"POSITIONAL ENCODING IN TRANSFORMERS - INTERVIEW DEMO\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75108251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 1: Why Positional Encoding is Needed\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: Why Do We Need Positional Encoding?\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW ANSWER:\n",
    "Self-attention is PERMUTATION-INVARIANT, meaning it treats input as a set,\n",
    "not a sequence. Without positional encoding, \"dog bites man\" and \"man bites dog\"\n",
    "would produce identical representations.\n",
    "\n",
    "Mathematically, for a permutation \u03c0:\n",
    "    Attention(X_\u03c0) = Attention(X)_\u03c0\n",
    "\n",
    "This is because attention computes QK^T which is symmetric w.r.t. reordering.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Simplified attention to demonstrate order-agnostic behavior\"\"\"\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.scale = math.sqrt(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        Returns: attention weights and output\n",
    "        \"\"\"\n",
    "        # For simplicity, use x as Q, K, V\n",
    "        Q = K = V = x\n",
    "\n",
    "        # Attention scores: QK^T / sqrt(d_model)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # Attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e47534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate order-agnostic behavior\n",
    "d_model = 4\n",
    "seq_len = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d156d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple sequence\n",
    "x1 = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0],  # Position 0: token \"dog\"\n",
    "    [0.0, 1.0, 0.0, 0.0],  # Position 1: token \"bites\"\n",
    "    [0.0, 0.0, 1.0, 0.0],  # Position 2: token \"man\"\n",
    "]).unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd28dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute the sequence (swap positions 0 and 2)\n",
    "x2 = torch.tensor([\n",
    "    [0.0, 0.0, 1.0, 0.0],  # Position 0: token \"man\"\n",
    "    [0.0, 1.0, 0.0, 0.0],  # Position 1: token \"bites\"\n",
    "    [1.0, 0.0, 0.0, 0.0],  # Position 2: token \"dog\"\n",
    "]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdcbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = SimpleAttention(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out1, attn1 = attention(x1)\n",
    "    out2, attn2 = attention(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe726248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDemonstration: Attention without positional encoding\")\n",
    "print(\"\\nSequence 1 (dog bites man):\")\n",
    "print(x1.squeeze())\n",
    "print(\"\\nSequence 2 (man bites dog):\")\n",
    "print(x2.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062f1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAttention output 1 (sorted by position):\")\n",
    "print(out1.squeeze())\n",
    "print(\"\\nAttention output 2 (sorted by position):\")\n",
    "print(out2.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8656c315",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# The outputs are permutations of each other!\n",
    "print(\"\\n\u26a0\ufe0f  IMPORTANT: The attention outputs are just permuted versions!\")\n",
    "print(\"This shows why we NEED positional information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 2: Sinusoidal Positional Encoding (Original Transformer)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: Sinusoidal Positional Encoding\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff478ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW ANSWER:\n",
    "The original Transformer paper (Vaswani et al., 2017) uses sinusoidal functions:\n",
    "\n",
    "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "Where:\n",
    "- pos: position in sequence (0 to max_len-1)\n",
    "- i: dimension index (0 to d_model/2-1)\n",
    "- Even dimensions use sine, odd dimensions use cosine\n",
    "\n",
    "Key Advantages:\n",
    "1. No learned parameters (deterministic)\n",
    "2. Can extrapolate to longer sequences than seen during training\n",
    "3. Allows model to easily learn relative positions (linear combination)\n",
    "4. Different wavelengths for different dimensions (10000^0 to 10000^1)\n",
    "\n",
    "The wavelengths form a geometric progression from 2\u03c0 to 10000\u00b72\u03c0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements sinusoidal positional encoding from \"Attention Is All You Need\"\n",
    "\n",
    "    INTERVIEW INSIGHT:\n",
    "    - This encoding is added to input embeddings, not concatenated\n",
    "    - Each dimension has a different frequency\n",
    "    - Low dimensions change quickly (high frequency)\n",
    "    - High dimensions change slowly (low frequency)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        # Shape: (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Position indices: [0, 1, 2, ..., max_len-1]\n",
    "        # Shape: (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Dimension indices for the geometric progression\n",
    "        # div_term represents: 10000^(2i/d_model) for i in [0, d_model/2)\n",
    "        # We use exp and log for numerical stability:\n",
    "        # 10000^(2i/d_model) = exp(2i * log(10000) / d_model)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() *\n",
    "            (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply sine to even dimensions (0, 2, 4, ...)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # Apply cosine to odd dimensions (1, 3, 5, ...)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension: (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register as buffer (not a parameter, but part of state)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x with positional encoding added\n",
    "        \"\"\"\n",
    "        # Add positional encoding to input\n",
    "        # Broadcasting handles batch dimension automatically\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "    def get_encoding(self, max_len=None):\n",
    "        \"\"\"Get the positional encoding matrix for visualization\"\"\"\n",
    "        if max_len is None:\n",
    "            return self.pe.squeeze(0)\n",
    "        return self.pe.squeeze(0)[:max_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and visualize sinusoidal positional encoding\n",
    "d_model = 128\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoder = SinusoidalPositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "pe_matrix = pos_encoder.get_encoding(max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSinusoidal Positional Encoding Shape: {pe_matrix.shape}\")\n",
    "print(f\"(sequence_length={max_len}, d_model={d_model})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d89fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show encoding for a few positions\n",
    "print(\"\\nSample encoding values for first 3 positions:\")\n",
    "for pos in range(3):\n",
    "    print(f\"\\nPosition {pos}:\")\n",
    "    print(f\"  First 8 dimensions: {pe_matrix[pos, :8]}\")\n",
    "    print(f\"  Last 8 dimensions:  {pe_matrix[pos, -8:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e48e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the positional encoding\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Heatmap of positional encodings\n",
    "ax1 = axes[0, 0]\n",
    "im1 = ax1.imshow(pe_matrix.numpy(), aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\n",
    "ax1.set_xlabel('Embedding Dimension')\n",
    "ax1.set_ylabel('Position in Sequence')\n",
    "ax1.set_title('Sinusoidal Positional Encoding Heatmap\\n(Each row is a position vector)')\n",
    "plt.colorbar(im1, ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Different dimensions over positions\n",
    "ax2 = axes[0, 1]\n",
    "dimensions_to_plot = [0, 1, 32, 33, 64, 65, 96, 97]\n",
    "for dim in dimensions_to_plot:\n",
    "    ax2.plot(pe_matrix[:50, dim].numpy(),\n",
    "             label=f'Dim {dim}', alpha=0.7)\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Encoding Value')\n",
    "ax2.set_title('Positional Encoding: Different Dimensions\\n(Low dims change fast, high dims change slow)')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Same position, different dimensions\n",
    "ax3 = axes[1, 0]\n",
    "positions_to_plot = [0, 10, 25, 50, 75]\n",
    "for pos in positions_to_plot:\n",
    "    ax3.plot(pe_matrix[pos, :64].numpy(),\n",
    "             label=f'Pos {pos}', alpha=0.7, marker='o', markersize=2)\n",
    "ax3.set_xlabel('Dimension')\n",
    "ax3.set_ylabel('Encoding Value')\n",
    "ax3.set_title('Different Positions Across Dimensions\\n(First 64 dimensions shown)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e32f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Wavelength demonstration\n",
    "ax4 = axes[1, 1]\n",
    "# Show how wavelength increases with dimension\n",
    "wavelengths = []\n",
    "for i in range(0, d_model, 2):\n",
    "    # Wavelength = 2\u03c0 * 10000^(i/d_model)\n",
    "    wavelength = 2 * math.pi * (10000 ** (i / d_model))\n",
    "    wavelengths.append(wavelength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e84482",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax4.semilogy(range(0, d_model, 2), wavelengths, marker='o')\n",
    "ax4.set_xlabel('Dimension Index')\n",
    "ax4.set_ylabel('Wavelength (log scale)')\n",
    "ax4.set_title('Wavelength by Dimension\\n(Geometric progression from 2\u03c0 to 10000\u00b72\u03c0)')\n",
    "ax4.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91b20d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'positional_encoding_sinusoidal.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved visualization: {output_dir / 'positional_encoding_sinusoidal.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 3: Learned Positional Embeddings\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3: Learned Positional Embeddings\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e743e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW ANSWER:\n",
    "Alternative to sinusoidal: Learn positional embeddings like token embeddings.\n",
    "\n",
    "Implementation: nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "Advantages:\n",
    "+ Can learn task-specific positional patterns\n",
    "+ May perform better on fixed-length sequences\n",
    "+ Used in BERT, GPT-2\n",
    "\n",
    "Disadvantages:\n",
    "- Cannot extrapolate beyond max_seq_len seen during training\n",
    "- Requires learning parameters (memory overhead)\n",
    "- May overfit to training sequence lengths\n",
    "\n",
    "Trade-off: Flexibility vs. Generalization\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09eb0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned positional embeddings (like in BERT, GPT-2)\n",
    "\n",
    "    INTERVIEW INSIGHT:\n",
    "    - Each position gets a learnable embedding vector\n",
    "    - Similar to token embeddings, but for positions\n",
    "    - Must specify maximum sequence length upfront\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Learnable embedding for each position\n",
    "        self.position_embeddings = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        # Initialize with small random values\n",
    "        nn.init.normal_(self.position_embeddings.weight, mean=0, std=0.02)\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Create position indices [0, 1, 2, ..., seq_len-1]\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "\n",
    "        # Get positional embeddings\n",
    "        pos_embeddings = self.position_embeddings(positions)\n",
    "\n",
    "        # Add to input\n",
    "        x = x + pos_embeddings\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a216cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learned positional embedding\n",
    "learned_pos = LearnedPositionalEmbedding(d_model=128, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLearned Positional Embedding:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in learned_pos.parameters()):,}\")\n",
    "print(f\"  Max sequence length: {learned_pos.max_len}\")\n",
    "print(f\"  Embedding dimension: {learned_pos.d_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show initial random embeddings\n",
    "with torch.no_grad():\n",
    "    learned_pe = learned_pos.position_embeddings.weight[:100].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65496f5f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"\\nInitial learned embeddings (before training):\")\n",
    "print(f\"  Shape: {learned_pe.shape}\")\n",
    "print(f\"  Mean: {learned_pe.mean():.4f}\")\n",
    "print(f\"  Std: {learned_pe.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d3cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 4: Comparison of Positional Encoding Methods\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 4: Comparing Positional Encoding Methods\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea744e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW QUESTION: Which positional encoding should we use?\n",
    "\n",
    "ANSWER: It depends on the use case!\n",
    "\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502    Property     \u2502   Sinusoidal     \u2502      Learned        \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Parameters      \u2502 None (0)         \u2502 max_len \u00d7 d_model   \u2502\n",
    "\u2502 Extrapolation   \u2502 Yes (any length) \u2502 No (fixed max_len)  \u2502\n",
    "\u2502 Training        \u2502 Not needed       \u2502 Learned from data   \u2502\n",
    "\u2502 Performance     \u2502 Good baseline    \u2502 Often better        \u2502\n",
    "\u2502 Used in         \u2502 Original Trans.  \u2502 BERT, GPT-2, GPT-3  \u2502\n",
    "\u2502 Best for        \u2502 Variable lengths \u2502 Fixed-length tasks  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Modern trend: Most large models use LEARNED positional embeddings\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_encodings(seq_len=50, d_model=64):\n",
    "    \"\"\"Compare sinusoidal and learned (initialized) encodings\"\"\"\n",
    "\n",
    "    # Sinusoidal\n",
    "    sin_encoder = SinusoidalPositionalEncoding(d_model, max_len=seq_len)\n",
    "    sin_pe = sin_encoder.get_encoding(max_len=seq_len).numpy()\n",
    "\n",
    "    # Learned (random initialization)\n",
    "    learned_encoder = LearnedPositionalEmbedding(d_model, max_len=seq_len)\n",
    "    with torch.no_grad():\n",
    "        learned_pe = learned_encoder.position_embeddings.weight.numpy()\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Sinusoidal\n",
    "    im1 = axes[0].imshow(sin_pe, aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[0].set_title('Sinusoidal Encoding\\n(Deterministic, wavelength-based)')\n",
    "    axes[0].set_xlabel('Dimension')\n",
    "    axes[0].set_ylabel('Position')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "    # Learned (initialized)\n",
    "    im2 = axes[1].imshow(learned_pe, aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[1].set_title('Learned Encoding (Random Init)\\n(Before training)')\n",
    "    axes[1].set_xlabel('Dimension')\n",
    "    axes[1].set_ylabel('Position')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "    # Difference\n",
    "    # Normalize both to [-1, 1] for fair comparison\n",
    "    sin_pe_norm = sin_pe / (np.abs(sin_pe).max() + 1e-8)\n",
    "    learned_pe_norm = learned_pe / (np.abs(learned_pe).max() + 1e-8)\n",
    "    diff = np.abs(sin_pe_norm - learned_pe_norm)\n",
    "\n",
    "    im3 = axes[2].imshow(diff, aspect='auto', cmap='viridis')\n",
    "    axes[2].set_title('Absolute Difference (Normalized)\\n(Shows structural differences)')\n",
    "    axes[2].set_xlabel('Dimension')\n",
    "    axes[2].set_ylabel('Position')\n",
    "    plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'positional_encoding_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n\u2713 Saved comparison: {output_dir / 'positional_encoding_comparison.png'}\")\n",
    "\n",
    "    # Statistics\n",
    "    print(\"\\nStatistical Comparison:\")\n",
    "    print(f\"Sinusoidal encoding:\")\n",
    "    print(f\"  Mean: {sin_pe.mean():.4f}, Std: {sin_pe.std():.4f}\")\n",
    "    print(f\"  Min: {sin_pe.min():.4f}, Max: {sin_pe.max():.4f}\")\n",
    "\n",
    "    print(f\"\\nLearned encoding (init):\")\n",
    "    print(f\"  Mean: {learned_pe.mean():.4f}, Std: {learned_pe.std():.4f}\")\n",
    "    print(f\"  Min: {learned_pe.min():.4f}, Max: {learned_pe.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85aab39",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "compare_encodings(seq_len=50, d_model=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 5: Residual Connections and Layer Normalization\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 5: Residual Connections and Layer Normalization\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW ANSWER:\n",
    "Transformers use two key architectural components:\n",
    "\n",
    "1. RESIDUAL CONNECTIONS (Skip Connections):\n",
    "   output = LayerNorm(x + Sublayer(x))\n",
    "\n",
    "   Why?\n",
    "   - Helps with gradient flow (prevents vanishing gradients)\n",
    "   - Allows direct path from input to output\n",
    "   - Enables training of very deep networks\n",
    "   - Identity mapping as initialization\n",
    "\n",
    "2. LAYER NORMALIZATION:\n",
    "   - Normalizes across feature dimension (not batch)\n",
    "   - Stabilizes training\n",
    "   - Reduces internal covariate shift\n",
    "   - Applied BEFORE or AFTER sublayer (Pre-LN vs Post-LN)\n",
    "\n",
    "Modern trend: Pre-LN (norm before sublayer) is more stable\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb83bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer block with:\n",
    "    1. Multi-head attention\n",
    "    2. Residual connection + Layer norm\n",
    "    3. Feed-forward network\n",
    "    4. Residual connection + Layer norm\n",
    "\n",
    "    INTERVIEW INSIGHT:\n",
    "    This is the core building block of transformers.\n",
    "    GPT-3 has 96 of these stacked!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, pre_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),  # Modern transformers use GELU instead of ReLU\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Pre-LN architecture (modern):\n",
    "            x = x + Attention(LayerNorm(x))\n",
    "            x = x + FFN(LayerNorm(x))\n",
    "\n",
    "        Post-LN architecture (original):\n",
    "            x = LayerNorm(x + Attention(x))\n",
    "            x = LayerNorm(x + FFN(x))\n",
    "        \"\"\"\n",
    "        if self.pre_norm:\n",
    "            # Pre-LN: Normalize before sublayer\n",
    "            # Attention block\n",
    "            normed = self.norm1(x)\n",
    "            attn_out, _ = self.attention(normed, normed, normed, attn_mask=mask)\n",
    "            x = x + self.dropout(attn_out)\n",
    "\n",
    "            # FFN block\n",
    "            normed = self.norm2(x)\n",
    "            ffn_out = self.ffn(normed)\n",
    "            x = x + ffn_out\n",
    "        else:\n",
    "            # Post-LN: Normalize after sublayer\n",
    "            # Attention block\n",
    "            attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "            x = self.norm1(x + self.dropout(attn_out))\n",
    "\n",
    "            # FFN block\n",
    "            ffn_out = self.ffn(x)\n",
    "            x = self.norm2(x + ffn_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate residual connections\n",
    "print(\"\\nDemonstrating Residual Connections:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a41b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "batch_size = 2\n",
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612bf14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input\n",
    "x = torch.randn(batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer block\n",
    "block = TransformerBlock(d_model=d_model, num_heads=4, d_ff=256, pre_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nInput mean: {x.mean():.4f}, std: {x.std():.4f}\")\n",
    "print(f\"Output mean: {output.mean():.4f}, std: {output.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f7a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of residual connections on gradient flow\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Visualizing Residual Connection Benefits\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb5c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradient_flow(num_layers=6):\n",
    "    \"\"\"\n",
    "    Demonstrate why residual connections help with gradient flow\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing gradient flow through {num_layers} layers:\")\n",
    "\n",
    "    # Create a simple network with residual connections\n",
    "    layers_with_residual = nn.ModuleList([\n",
    "        TransformerBlock(d_model=64, num_heads=4, d_ff=256)\n",
    "        for _ in range(num_layers)\n",
    "    ])\n",
    "\n",
    "    # Create input that requires gradient\n",
    "    x = torch.randn(1, 10, 64, requires_grad=True)\n",
    "\n",
    "    # Forward pass\n",
    "    out = x\n",
    "    for layer in layers_with_residual:\n",
    "        out = layer(out)\n",
    "\n",
    "    # Compute loss and backward\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    # Check gradient\n",
    "    print(f\"\\nWith residual connections:\")\n",
    "    print(f\"  Input gradient norm: {x.grad.norm().item():.4f}\")\n",
    "    print(f\"  Input gradient mean: {x.grad.mean().item():.6f}\")\n",
    "    print(f\"  Gradient is well-behaved \u2713\")\n",
    "\n",
    "    return x.grad.norm().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8075270",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "grad_norm = analyze_gradient_flow(num_layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a610e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 6: Complete Example with All Components\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 6: Complete Transformer Encoder Example\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d320eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer Encoder with:\n",
    "    1. Token embeddings\n",
    "    2. Positional encoding\n",
    "    3. Multiple transformer blocks\n",
    "    4. Final layer norm\n",
    "\n",
    "    INTERVIEW INSIGHT:\n",
    "    This is the architecture used in BERT (encoder-only).\n",
    "    GPT uses decoder-only (with causal masking).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        d_ff=2048,\n",
    "        max_seq_len=512,\n",
    "        dropout=0.1,\n",
    "        pos_encoding_type='sinusoidal'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        if pos_encoding_type == 'sinusoidal':\n",
    "            self.pos_encoder = SinusoidalPositionalEncoding(\n",
    "                d_model, max_seq_len, dropout\n",
    "            )\n",
    "        else:\n",
    "            self.pos_encoder = LearnedPositionalEmbedding(\n",
    "                d_model, max_seq_len, dropout\n",
    "            )\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with proper scaling\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Token indices (batch_size, seq_len)\n",
    "            mask: Attention mask (optional)\n",
    "\n",
    "        Returns:\n",
    "            Encoded representations (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Token embedding with scaling\n",
    "        # INTERVIEW POINT: Scale by sqrt(d_model) to prevent embeddings\n",
    "        # from being too small relative to positional encodings\n",
    "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        # Final normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ba5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small transformer encoder\n",
    "vocab_size = 1000\n",
    "model = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    num_layers=3,\n",
    "    d_ff=512,\n",
    "    max_seq_len=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTransformer Encoder Architecture:\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"  Model dimension: {model.d_model}\")\n",
    "print(f\"  Number of layers: {len(model.layers)}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a24f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example forward pass\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nExample forward pass:\")\n",
    "print(f\"  Input shape: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c87361",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac68b5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Output mean: {output.mean():.4f}, std: {output.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae4b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 7: Interview Quick Reference\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERVIEW QUICK REFERENCE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33318aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_qa = \"\"\"\n",
    "Q1: Why do transformers need positional encoding?\n",
    "A: Self-attention is permutation-invariant. Without positional info,\n",
    "   \"dog bites man\" = \"man bites dog\". PE adds position information.\n",
    "\n",
    "Q2: What's the formula for sinusoidal positional encoding?\n",
    "A: PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "   PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "Q3: Sinusoidal vs Learned positional embeddings?\n",
    "A: Sinusoidal: No params, can extrapolate, deterministic\n",
    "   Learned: Better performance, task-specific, fixed max length\n",
    "   Modern models mostly use learned (BERT, GPT).\n",
    "\n",
    "Q4: Why use both sin and cos?\n",
    "A: Allows model to learn relative positions as linear combinations.\n",
    "   For any fixed offset k, PE(pos+k) can be represented as a linear\n",
    "   function of PE(pos).\n",
    "\n",
    "Q5: What are residual connections?\n",
    "A: output = x + Sublayer(x)\n",
    "   Benefits: Gradient flow, identity mapping, enables deep networks\n",
    "   Used in EVERY transformer layer.\n",
    "\n",
    "Q6: Pre-LN vs Post-LN?\n",
    "A: Pre-LN: Norm before sublayer (more stable, modern choice)\n",
    "   Post-LN: Norm after sublayer (original transformer)\n",
    "   Pre-LN is better for very deep models.\n",
    "\n",
    "Q7: How is positional encoding added?\n",
    "A: ADDED to token embeddings, not concatenated!\n",
    "   Token embeddings are scaled by sqrt(d_model) first.\n",
    "\n",
    "Q8: Can transformers handle sequences longer than training?\n",
    "A: With sinusoidal: Yes (can extrapolate)\n",
    "   With learned: No (fixed max_len)\n",
    "   Recent work: Relative position encodings (T5, Transformer-XL)\n",
    "\n",
    "Q9: What's the computational complexity?\n",
    "A: Self-attention: O(n\u00b2 \u00b7 d) where n=seq_len, d=d_model\n",
    "   This is why long sequences are expensive!\n",
    "\n",
    "Q10: What's the difference between encoder and decoder?\n",
    "A: Encoder: Bidirectional attention (sees all tokens)\n",
    "   Decoder: Causal masking (only sees previous tokens)\n",
    "   BERT uses encoder, GPT uses decoder, T5 uses both.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168671b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(interview_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Save Summary\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254eacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = f\"\"\"\n",
    "Generated visualizations saved to:\n",
    "  {output_dir}/\n",
    "\n",
    "Files created:\n",
    "  1. positional_encoding_sinusoidal.png - Detailed sinusoidal encoding analysis\n",
    "  2. positional_encoding_comparison.png - Sinusoidal vs Learned comparison\n",
    "\n",
    "Key Takeaways for Interviews:\n",
    "\u2713 Positional encoding is ESSENTIAL (attention is order-agnostic)\n",
    "\u2713 Sinusoidal encoding uses sin/cos with different wavelengths\n",
    "\u2713 Modern models mostly use learned positional embeddings\n",
    "\u2713 Residual connections enable deep networks via gradient flow\n",
    "\u2713 Layer normalization stabilizes training\n",
    "\u2713 Pre-LN architecture is more stable than Post-LN\n",
    "\n",
    "Implementation highlights:\n",
    "- Sinusoidal: No parameters, can extrapolate\n",
    "- Learned: Better performance, fixed max length\n",
    "- Residual: x + Sublayer(x) in every layer\n",
    "- LayerNorm: Normalized across features, not batch\n",
    "\n",
    "This demo covers interview questions Q17, Q21, Q43, Q46.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f885fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to file\n",
    "with open(output_dir / 'positional_encoding_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "    f.write(\"\\n\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    f.write(\"INTERVIEW Q&A REFERENCE\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(interview_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\u2713 Summary saved to: {output_dir / 'positional_encoding_summary.txt'}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Demo complete! You're ready for transformer architecture interviews.\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}