{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Attention Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc05466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comprehensive Attention Mechanisms Demo for LLM Interview Preparation\n",
    "======================================================================\n",
    "\n",
    "This file demonstrates key attention mechanism concepts for interview questions:\n",
    "- Q2: Explain attention mechanisms\n",
    "- Q22: How does multi-head attention work?\n",
    "- Q23: Why is scaled dot-product attention important?\n",
    "- Q24: What is causal masking?\n",
    "- Q32: What are Query, Key, and Value in attention?\n",
    "\n",
    "Author: Educational Demo\n",
    "Purpose: LLM Interview Preparation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a8dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0f8fd",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 1: SCALED DOT-PRODUCT ATTENTION FROM SCRATCH\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ece680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    dropout: Optional[nn.Dropout] = None,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Implements scaled dot-product attention from the \"Attention is All You Need\" paper.\n",
    "\n",
    "    The attention mechanism allows the model to focus on different parts of the input\n",
    "    when producing each element of the output.\n",
    "\n",
    "    Formula: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V\n",
    "\n",
    "    Args:\n",
    "        query: Query tensor of shape (batch, num_heads, seq_len_q, d_k)\n",
    "        key: Key tensor of shape (batch, num_heads, seq_len_k, d_k)\n",
    "        value: Value tensor of shape (batch, num_heads, seq_len_v, d_v)\n",
    "        mask: Optional mask tensor to prevent attention to certain positions\n",
    "        dropout: Optional dropout layer\n",
    "        verbose: If True, print shape information\n",
    "\n",
    "    Returns:\n",
    "        output: Weighted sum of values, shape (batch, num_heads, seq_len_q, d_v)\n",
    "        attention_weights: Attention weights, shape (batch, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    Key Concepts (Interview Question 32):\n",
    "    -------------------------------------\n",
    "    - Query (Q): \"What am I looking for?\" - Represents the current position\n",
    "    - Key (K): \"What do I contain?\" - Represents all positions that could be attended to\n",
    "    - Value (V): \"What information do I have?\" - The actual content to retrieve\n",
    "\n",
    "    The attention score measures how much the query \"matches\" each key.\n",
    "    High scores mean the query finds that key relevant.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SCALED DOT-PRODUCT ATTENTION - STEP BY STEP\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nInput Shapes:\")\n",
    "        print(f\"  Query (Q): {query.shape}  # (batch, heads, seq_len_q, d_k)\")\n",
    "        print(f\"  Key (K):   {key.shape}  # (batch, heads, seq_len_k, d_k)\")\n",
    "        print(f\"  Value (V): {value.shape}  # (batch, heads, seq_len_v, d_v)\")\n",
    "\n",
    "    # Get the dimension of the key (d_k) for scaling\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nScaling Factor: sqrt(d_k) = sqrt({d_k}) = {np.sqrt(d_k):.4f}\")\n",
    "        print(\"  Why scale? To prevent dot products from growing too large,\")\n",
    "        print(\"  which would push softmax into regions with tiny gradients.\")\n",
    "\n",
    "    # Step 1: Compute attention scores (QK^T)\n",
    "    # This measures the similarity between queries and keys\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nStep 1 - Attention Scores (QK^T):\")\n",
    "        print(f\"  Shape: {scores.shape}  # (batch, heads, seq_len_q, seq_len_k)\")\n",
    "        print(f\"  Formula: scores[i,j] = dot_product(query[i], key[j])\")\n",
    "        print(f\"  Before scaling - Min: {scores.min():.4f}, Max: {scores.max():.4f}\")\n",
    "\n",
    "    # Step 2: Scale by sqrt(d_k) (Interview Question 23)\n",
    "    # This is crucial! Without scaling, softmax can saturate for large d_k\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nStep 2 - Scaled Scores (QK^T / sqrt(d_k)):\")\n",
    "        print(f\"  After scaling - Min: {scores.min():.4f}, Max: {scores.max():.4f}\")\n",
    "        print(f\"  Scaling prevents vanishing gradients in softmax\")\n",
    "\n",
    "    # Step 3: Apply mask if provided (Interview Question 24)\n",
    "    # Masking is used for:\n",
    "    # - Causal/autoregressive attention (can't see future tokens)\n",
    "    # - Padding (ignore padded positions)\n",
    "    if mask is not None:\n",
    "        if verbose:\n",
    "            print(f\"\\nStep 3 - Applying Mask:\")\n",
    "            print(f\"  Mask shape: {mask.shape}\")\n",
    "            print(f\"  Setting masked positions to -inf (will become 0 after softmax)\")\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Step 4: Apply softmax to get attention weights\n",
    "    # Softmax ensures weights sum to 1 and are non-negative\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nStep 4 - Attention Weights (softmax over keys):\")\n",
    "        print(f\"  Shape: {attention_weights.shape}\")\n",
    "        print(f\"  Sum along last dim (should be ~1.0): {attention_weights.sum(dim=-1)[0, 0, 0]:.6f}\")\n",
    "        print(f\"  Min: {attention_weights.min():.6f}, Max: {attention_weights.max():.6f}\")\n",
    "        print(f\"  Each row represents how much attention a query pays to all keys\")\n",
    "\n",
    "    # Step 5: Apply dropout (during training)\n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "\n",
    "    # Step 6: Weighted sum of values\n",
    "    # This produces the final output by combining values according to attention weights\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nStep 5 - Output (Attention_weights @ Value):\")\n",
    "        print(f\"  Shape: {output.shape}  # (batch, heads, seq_len_q, d_v)\")\n",
    "        print(f\"  Each output position is a weighted combination of all value vectors\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32dce6",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 2: MULTI-HEAD ATTENTION IMPLEMENTATION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f528fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism (Interview Question 22).\n",
    "\n",
    "    Instead of performing a single attention function, multi-head attention\n",
    "    projects Q, K, V into multiple subspaces (heads) and performs attention\n",
    "    in parallel, then concatenates the results.\n",
    "\n",
    "    Benefits:\n",
    "    1. Allows model to attend to information from different representation subspaces\n",
    "    2. Each head can learn different attention patterns (e.g., syntax vs semantics)\n",
    "    3. More expressive than single-head attention\n",
    "    4. Prevents the model from focusing too much on a single aspect\n",
    "\n",
    "    Formula:\n",
    "        MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\n",
    "        where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of the model (embedding dimension)\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        # Instead of separate projections per head, we use one large projection\n",
    "        # and split it into heads (more efficient)\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_k).\n",
    "\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # Reshape: (batch, seq_len, d_model) -> (batch, seq_len, num_heads, d_k)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "\n",
    "        # Transpose: (batch, seq_len, num_heads, d_k) -> (batch, num_heads, seq_len, d_k)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Combine heads back into a single tensor.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, num_heads, seq_len, d_k)\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "\n",
    "        # Transpose: (batch, num_heads, seq_len, d_k) -> (batch, seq_len, num_heads, d_k)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "\n",
    "        # Reshape: (batch, seq_len, num_heads, d_k) -> (batch, seq_len, d_model)\n",
    "        return x.view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        verbose: bool = False\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            query: Query tensor (batch, seq_len, d_model)\n",
    "            key: Key tensor (batch, seq_len, d_model)\n",
    "            value: Value tensor (batch, seq_len, d_model)\n",
    "            mask: Optional mask tensor\n",
    "            verbose: If True, print detailed information\n",
    "\n",
    "        Returns:\n",
    "            output: Output tensor (batch, seq_len, d_model)\n",
    "            attention_weights: Attention weights (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"MULTI-HEAD ATTENTION - FORWARD PASS\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"\\nConfiguration:\")\n",
    "            print(f\"  Model dimension (d_model): {self.d_model}\")\n",
    "            print(f\"  Number of heads: {self.num_heads}\")\n",
    "            print(f\"  Dimension per head (d_k): {self.d_k}\")\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nInput shapes:\")\n",
    "            print(f\"  Query: {query.shape}\")\n",
    "            print(f\"  Key:   {key.shape}\")\n",
    "            print(f\"  Value: {value.shape}\")\n",
    "\n",
    "        # Step 1: Linear projections\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nAfter linear projections:\")\n",
    "            print(f\"  Q: {Q.shape}\")\n",
    "            print(f\"  K: {K.shape}\")\n",
    "            print(f\"  V: {V.shape}\")\n",
    "\n",
    "        # Step 2: Split into multiple heads\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nAfter splitting into {self.num_heads} heads:\")\n",
    "            print(f\"  Q: {Q.shape}  # (batch, num_heads, seq_len, d_k)\")\n",
    "            print(f\"  K: {K.shape}\")\n",
    "            print(f\"  V: {V.shape}\")\n",
    "            print(f\"  Each head operates on a {self.d_k}-dimensional subspace\")\n",
    "\n",
    "        # Step 3: Scaled dot-product attention for all heads in parallel\n",
    "        attn_output, attention_weights = scaled_dot_product_attention(\n",
    "            Q, K, V, mask=mask, dropout=self.dropout, verbose=False\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nAfter attention:\")\n",
    "            print(f\"  Output: {attn_output.shape}\")\n",
    "            print(f\"  Attention weights: {attention_weights.shape}\")\n",
    "\n",
    "        # Step 4: Concatenate heads\n",
    "        attn_output = self.combine_heads(attn_output)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nAfter combining heads:\")\n",
    "            print(f\"  Output: {attn_output.shape}  # Back to (batch, seq_len, d_model)\")\n",
    "\n",
    "        # Step 5: Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nFinal output after linear projection:\")\n",
    "            print(f\"  Output: {output.shape}\")\n",
    "            print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adc3c4",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 3: CAUSAL MASKING FOR AUTOREGRESSIVE ATTENTION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756113c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len: int, device: torch.device = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal (lower triangular) mask for autoregressive attention.\n",
    "\n",
    "    In autoregressive models (like GPT), each position can only attend to\n",
    "    previous positions and itself, not future positions.\n",
    "\n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        device: Device to create the mask on\n",
    "\n",
    "    Returns:\n",
    "        Causal mask of shape (seq_len, seq_len)\n",
    "        1 indicates \"can attend\", 0 indicates \"cannot attend\"\n",
    "\n",
    "    Example for seq_len=4:\n",
    "        [[1, 0, 0, 0],   # Position 0 can only see position 0\n",
    "         [1, 1, 0, 0],   # Position 1 can see positions 0,1\n",
    "         [1, 1, 1, 0],   # Position 2 can see positions 0,1,2\n",
    "         [1, 1, 1, 1]]   # Position 3 can see all positions\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1374d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_causal_mask(seq_len: int = 8, save_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Visualize the causal mask structure.\n",
    "\n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        save_path: Path to save the visualization\n",
    "    \"\"\"\n",
    "    mask = create_causal_mask(seq_len).numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(mask, annot=True, fmt='.0f', cmap='Blues', cbar=True,\n",
    "                xticklabels=range(seq_len), yticklabels=range(seq_len))\n",
    "    plt.xlabel('Key Position (what we can attend to)')\n",
    "    plt.ylabel('Query Position (current token)')\n",
    "    plt.title('Causal Mask for Autoregressive Attention\\n'\n",
    "              '(1 = can attend, 0 = cannot attend)')\n",
    "\n",
    "    # Add explanation text\n",
    "    plt.text(seq_len/2, -1.5,\n",
    "             'Each row shows what a token can attend to.\\n'\n",
    "             'Token at position i can only attend to positions 0 to i (past and present).',\n",
    "             ha='center', fontsize=10, style='italic')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Causal mask visualization saved to: {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6585d",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 4: ATTENTION VISUALIZATION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_weights(\n",
    "    attention_weights: torch.Tensor,\n",
    "    tokens: list,\n",
    "    head_idx: int = 0,\n",
    "    save_path: Optional[str] = None,\n",
    "    title: str = \"Attention Weights\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        attention_weights: Attention weights (batch, num_heads, seq_len, seq_len)\n",
    "                          or (num_heads, seq_len, seq_len) or (seq_len, seq_len)\n",
    "        tokens: List of token strings for labels\n",
    "        head_idx: Which attention head to visualize\n",
    "        save_path: Path to save the visualization\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Extract weights for the specified head\n",
    "    if attention_weights.dim() == 4:  # (batch, num_heads, seq_len, seq_len)\n",
    "        weights = attention_weights[0, head_idx].detach().cpu().numpy()\n",
    "    elif attention_weights.dim() == 3:  # (num_heads, seq_len, seq_len)\n",
    "        weights = attention_weights[head_idx].detach().cpu().numpy()\n",
    "    else:  # (seq_len, seq_len)\n",
    "        weights = attention_weights.detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(weights, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "                xticklabels=tokens, yticklabels=tokens, cbar=True)\n",
    "    plt.xlabel('Key (attending to)', fontsize=12)\n",
    "    plt.ylabel('Query (current token)', fontsize=12)\n",
    "    plt.title(f'{title} (Head {head_idx})', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Add explanation\n",
    "    plt.text(len(tokens)/2, -1.5,\n",
    "             'Each cell (i,j) shows how much token i attends to token j.\\n'\n",
    "             'Higher values (darker red) = more attention.',\n",
    "             ha='center', fontsize=10, style='italic')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Attention weights visualization saved to: {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a296e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_heads(\n",
    "    attention_weights: torch.Tensor,\n",
    "    tokens: list,\n",
    "    save_path: Optional[str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns for all heads in a grid.\n",
    "\n",
    "    Args:\n",
    "        attention_weights: Attention weights (batch, num_heads, seq_len, seq_len)\n",
    "        tokens: List of token strings for labels\n",
    "        save_path: Path to save the visualization\n",
    "    \"\"\"\n",
    "    if attention_weights.dim() == 4:\n",
    "        weights = attention_weights[0].detach().cpu().numpy()\n",
    "    else:\n",
    "        weights = attention_weights.detach().cpu().numpy()\n",
    "\n",
    "    num_heads = weights.shape[0]\n",
    "    cols = min(4, num_heads)\n",
    "    rows = (num_heads + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    if num_heads == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for head_idx in range(num_heads):\n",
    "        ax = axes[head_idx]\n",
    "        sns.heatmap(weights[head_idx], annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "                   xticklabels=tokens, yticklabels=tokens, ax=ax, cbar=True)\n",
    "        ax.set_title(f'Head {head_idx}', fontweight='bold')\n",
    "        ax.set_xlabel('Key')\n",
    "        ax.set_ylabel('Query')\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for idx in range(num_heads, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.suptitle('Multi-Head Attention Patterns\\n'\n",
    "                 'Different heads learn different attention patterns',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Multi-head visualization saved to: {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638cb7c6",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 5: SOFTMAX NORMALIZATION DEMONSTRATION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d24d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_softmax_effect():\n",
    "    \"\"\"\n",
    "    Demonstrate the effect of softmax normalization on attention scores.\n",
    "\n",
    "    This shows why softmax is important for attention:\n",
    "    1. Converts unbounded scores to probabilities (0 to 1)\n",
    "    2. Ensures scores sum to 1 (valid probability distribution)\n",
    "    3. Amplifies differences (high scores get higher, low scores get lower)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOFTMAX NORMALIZATION EFFECT ON ATTENTION SCORES\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Example attention scores (before softmax)\n",
    "    scores = torch.tensor([2.0, 1.0, 0.5, -0.5, -1.0])\n",
    "\n",
    "    print(\"\\nOriginal Attention Scores (before softmax):\")\n",
    "    print(f\"  Scores: {scores.tolist()}\")\n",
    "    print(f\"  Sum: {scores.sum().item():.4f} (not normalized)\")\n",
    "    print(f\"  Min: {scores.min().item():.4f}, Max: {scores.max().item():.4f}\")\n",
    "\n",
    "    # Apply softmax\n",
    "    attention_probs = F.softmax(scores, dim=0)\n",
    "\n",
    "    print(\"\\nAfter Softmax Normalization:\")\n",
    "    print(f\"  Probabilities: {[f'{p:.4f}' for p in attention_probs.tolist()]}\")\n",
    "    print(f\"  Sum: {attention_probs.sum().item():.6f} (should be 1.0)\")\n",
    "    print(f\"  Min: {attention_probs.min().item():.6f}, Max: {attention_probs.max().item():.6f}\")\n",
    "\n",
    "    # Show how different temperatures affect attention\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"EFFECT OF SCALING (Temperature) ON SOFTMAX\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    temperatures = [0.1, 0.5, 1.0, 2.0, 10.0]\n",
    "\n",
    "    for temp in temperatures:\n",
    "        scaled_scores = scores / temp\n",
    "        probs = F.softmax(scaled_scores, dim=0)\n",
    "\n",
    "        print(f\"\\nTemperature = {temp}:\")\n",
    "        print(f\"  Scaled scores: {[f'{s:.2f}' for s in scaled_scores.tolist()]}\")\n",
    "        print(f\"  Probabilities: {[f'{p:.4f}' for p in probs.tolist()]}\")\n",
    "        print(f\"  Entropy: {-(probs * torch.log(probs + 1e-10)).sum().item():.4f}\")\n",
    "        print(f\"  Effect: {'Sharper (peaked)' if temp < 1.0 else 'Smoother (uniform)' if temp > 1.0 else 'Standard'}\")\n",
    "\n",
    "    print(\"\\nKey Insight:\")\n",
    "    print(\"  - Lower temperature (< 1) \u2192 Sharper attention (focus on top items)\")\n",
    "    print(\"  - Higher temperature (> 1) \u2192 Smoother attention (more uniform)\")\n",
    "    print(\"  - sqrt(d_k) scaling in attention acts as temperature control\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb7f27",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 6: EXAMPLE DEMONSTRATIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_1_basic_attention():\n",
    "    \"\"\"\n",
    "    Example 1: Basic scaled dot-product attention with a simple sentence.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE 1: BASIC ATTENTION MECHANISM\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Simple example with 5 tokens\n",
    "    batch_size = 1\n",
    "    num_heads = 1\n",
    "    seq_len = 5\n",
    "    d_k = 64\n",
    "\n",
    "    print(f\"\\nScenario: Processing a simple sentence with {seq_len} tokens\")\n",
    "    print(f\"Model dimension per head: {d_k}\")\n",
    "\n",
    "    # Create random Q, K, V tensors (in practice, these come from embeddings)\n",
    "    torch.manual_seed(42)\n",
    "    Q = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "    K = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "    V = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "    # Compute attention\n",
    "    output, attention_weights = scaled_dot_product_attention(Q, K, V, verbose=True)\n",
    "\n",
    "    # Visualize\n",
    "    tokens = ['The', 'cat', 'sat', 'on', 'mat']\n",
    "    viz_path = Path('/Users/zack/dev/ml-refresher/data/interview_viz')\n",
    "    viz_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    visualize_attention_weights(\n",
    "        attention_weights,\n",
    "        tokens,\n",
    "        save_path=str(viz_path / 'attention_basic.png'),\n",
    "        title='Basic Attention Pattern'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_2_multi_head_attention():\n",
    "    \"\"\"\n",
    "    Example 2: Multi-head attention with multiple heads learning different patterns.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE 2: MULTI-HEAD ATTENTION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Configuration\n",
    "    batch_size = 1\n",
    "    seq_len = 6\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "\n",
    "    print(f\"\\nScenario: Multi-head attention with {num_heads} heads\")\n",
    "    print(f\"Model dimension: {d_model}\")\n",
    "    print(f\"Dimension per head: {d_model // num_heads}\")\n",
    "\n",
    "    # Create model\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # Create input (in practice, these are embeddings)\n",
    "    torch.manual_seed(42)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Forward pass (self-attention: Q=K=V)\n",
    "    output, attention_weights = mha(x, x, x, verbose=True)\n",
    "\n",
    "    # Visualize\n",
    "    tokens = ['I', 'love', 'machine', 'learning', 'models', '.']\n",
    "    viz_path = Path('/Users/zack/dev/ml-refresher/data/interview_viz')\n",
    "\n",
    "    # Visualize individual head\n",
    "    visualize_attention_weights(\n",
    "        attention_weights,\n",
    "        tokens,\n",
    "        head_idx=0,\n",
    "        save_path=str(viz_path / 'attention_multihead_head0.png'),\n",
    "        title='Multi-Head Attention'\n",
    "    )\n",
    "\n",
    "    # Visualize all heads\n",
    "    visualize_all_heads(\n",
    "        attention_weights,\n",
    "        tokens,\n",
    "        save_path=str(viz_path / 'attention_all_heads.png')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ac8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_3_causal_attention():\n",
    "    \"\"\"\n",
    "    Example 3: Causal attention with masking (like in GPT).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE 3: CAUSAL (AUTOREGRESSIVE) ATTENTION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    seq_len = 7\n",
    "    batch_size = 1\n",
    "    num_heads = 4\n",
    "    d_model = 256\n",
    "\n",
    "    print(f\"\\nScenario: Autoregressive language model (like GPT)\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "    print(f\"Key property: Each token can only attend to past and current tokens\")\n",
    "    print(f\"Purpose: Ensures the model can't 'cheat' by looking at future tokens\")\n",
    "\n",
    "    # Create causal mask\n",
    "    causal_mask = create_causal_mask(seq_len)\n",
    "    print(f\"\\nCausal mask shape: {causal_mask.shape}\")\n",
    "    print(\"Causal mask (1=attend, 0=mask):\")\n",
    "    print(causal_mask.numpy().astype(int))\n",
    "\n",
    "    # Reshape mask for multi-head attention\n",
    "    # Shape: (1, 1, seq_len, seq_len) for broadcasting\n",
    "    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Create model and input\n",
    "    torch.manual_seed(42)\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Forward pass with causal mask\n",
    "    output, attention_weights = mha(x, x, x, mask=causal_mask, verbose=False)\n",
    "\n",
    "    print(f\"\\nOutput shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "    # Visualize\n",
    "    tokens = ['Once', 'upon', 'a', 'time', 'there', 'was', '...']\n",
    "    viz_path = Path('/Users/zack/dev/ml-refresher/data/interview_viz')\n",
    "\n",
    "    # Visualize causal mask\n",
    "    visualize_causal_mask(\n",
    "        seq_len,\n",
    "        save_path=str(viz_path / 'causal_mask.png')\n",
    "    )\n",
    "\n",
    "    # Visualize attention with mask\n",
    "    visualize_attention_weights(\n",
    "        attention_weights,\n",
    "        tokens,\n",
    "        head_idx=0,\n",
    "        save_path=str(viz_path / 'attention_causal.png'),\n",
    "        title='Causal Attention Pattern'\n",
    "    )\n",
    "\n",
    "    # Show how masking affects attention\n",
    "    print(\"\\nEffect of Causal Masking:\")\n",
    "    print(\"Notice in the visualization:\")\n",
    "    print(\"  - Lower triangular pattern (tokens can only see past)\")\n",
    "    print(\"  - No attention to future positions (zeros above diagonal)\")\n",
    "    print(\"  - Essential for autoregressive generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_4_attention_patterns():\n",
    "    \"\"\"\n",
    "    Example 4: Demonstrate different attention patterns that emerge.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE 4: DIFFERENT ATTENTION PATTERNS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\nIn real transformers, different heads learn different patterns:\")\n",
    "    print(\"  - Syntactic patterns (e.g., attending to related words)\")\n",
    "    print(\"  - Positional patterns (e.g., attending to adjacent tokens)\")\n",
    "    print(\"  - Semantic patterns (e.g., attending to similar meanings)\")\n",
    "\n",
    "    seq_len = 8\n",
    "    tokens = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog']\n",
    "\n",
    "    # Create different attention patterns manually for illustration\n",
    "\n",
    "    # Pattern 1: Uniform attention (attends equally to all tokens)\n",
    "    uniform = torch.ones(seq_len, seq_len) / seq_len\n",
    "\n",
    "    # Pattern 2: Local attention (attends to nearby tokens)\n",
    "    local = torch.zeros(seq_len, seq_len)\n",
    "    for i in range(seq_len):\n",
    "        for j in range(max(0, i-2), min(seq_len, i+3)):\n",
    "            local[i, j] = 1.0\n",
    "    local = local / local.sum(dim=1, keepdim=True)\n",
    "\n",
    "    # Pattern 3: Self attention (each token attends mostly to itself)\n",
    "    self_attn = torch.eye(seq_len) * 0.7\n",
    "    self_attn = self_attn + (1 - torch.eye(seq_len)) * 0.3 / (seq_len - 1)\n",
    "\n",
    "    # Pattern 4: Focused attention (attends to specific important tokens)\n",
    "    focused = torch.zeros(seq_len, seq_len)\n",
    "    focused[:, 3] = 0.6  # All tokens focus on position 3 (fox)\n",
    "    focused[:, 4] = 0.3  # And position 4 (jumps)\n",
    "    for i in range(seq_len):\n",
    "        focused[i, i] += 0.1  # Small self-attention\n",
    "\n",
    "    patterns = [\n",
    "        (uniform, \"Uniform Attention\"),\n",
    "        (local, \"Local/Positional Attention\"),\n",
    "        (self_attn, \"Self Attention\"),\n",
    "        (focused, \"Focused Attention\")\n",
    "    ]\n",
    "\n",
    "    viz_path = Path('/Users/zack/dev/ml-refresher/data/interview_viz')\n",
    "\n",
    "    for idx, (pattern, name) in enumerate(patterns):\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Pattern shape: {pattern.shape}\")\n",
    "        print(f\"  Row sums (should be 1.0): {pattern.sum(dim=1)[0]:.6f}\")\n",
    "\n",
    "        visualize_attention_weights(\n",
    "            pattern,\n",
    "            tokens,\n",
    "            save_path=str(viz_path / f'attention_pattern_{idx}.png'),\n",
    "            title=name\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442a0c3",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SECTION 7: INTERVIEW KEY POINTS SUMMARY\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_interview_summary():\n",
    "    \"\"\"\n",
    "    Print a summary of key points for interview questions.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY INTERVIEW POINTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    summary = \"\"\"\n",
    "Q2: EXPLAIN ATTENTION MECHANISMS\n",
    "---------------------------------\n",
    "\u2713 Attention allows models to focus on different parts of input when producing output\n",
    "\u2713 Core idea: weighted combination of values based on query-key similarity\n",
    "\u2713 Formula: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k))V\n",
    "\u2713 Benefits: handles variable-length sequences, captures long-range dependencies\n",
    "\n",
    "Q22: HOW DOES MULTI-HEAD ATTENTION WORK?\n",
    "----------------------------------------\n",
    "\u2713 Performs attention multiple times in parallel with different learned projections\n",
    "\u2713 Each head learns to attend to different aspects of the input\n",
    "\u2713 Formula: MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O\n",
    "\u2713 Benefits: richer representations, different attention patterns, more expressive\n",
    "\n",
    "Q23: WHY IS SCALED DOT-PRODUCT ATTENTION IMPORTANT?\n",
    "---------------------------------------------------\n",
    "\u2713 Scaling by sqrt(d_k) prevents dot products from growing too large\n",
    "\u2713 Without scaling, softmax enters regions with tiny gradients (vanishing gradients)\n",
    "\u2713 Larger d_k \u2192 larger dot products \u2192 more peaked softmax \u2192 worse gradients\n",
    "\u2713 Scaling keeps gradients healthy and training stable\n",
    "\n",
    "Q24: WHAT IS CAUSAL MASKING?\n",
    "-----------------------------\n",
    "\u2713 Prevents attention to future positions in autoregressive models\n",
    "\u2713 Implemented by setting future positions to -inf before softmax\n",
    "\u2713 Creates lower triangular attention pattern\n",
    "\u2713 Essential for models like GPT that generate text left-to-right\n",
    "\u2713 Ensures model doesn't \"cheat\" during training\n",
    "\n",
    "Q32: WHAT ARE QUERY, KEY, VALUE?\n",
    "---------------------------------\n",
    "\u2713 Query (Q): \"What am I looking for?\" - represents current position\n",
    "\u2713 Key (K): \"What do I contain?\" - represents positions that could be attended to\n",
    "\u2713 Value (V): \"What information do I have?\" - actual content to retrieve\n",
    "\u2713 Attention score = similarity between query and key\n",
    "\u2713 Output = weighted sum of values based on attention scores\n",
    "\n",
    "COMMON FOLLOW-UP QUESTIONS:\n",
    "---------------------------\n",
    "\u2022 Why use softmax? \u2192 Ensures weights are non-negative and sum to 1\n",
    "\u2022 Self-attention vs cross-attention? \u2192 Self: Q=K=V, Cross: K,V from different source\n",
    "\u2022 Computational complexity? \u2192 O(n\u00b2d) where n=seq_len, d=model_dim\n",
    "\u2022 How to reduce complexity? \u2192 Sparse attention, linear attention, etc.\n",
    "\"\"\"\n",
    "\n",
    "    print(summary)\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9416f7",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "MAIN EXECUTION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d771850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run all demonstrations.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ATTENTION MECHANISMS - COMPREHENSIVE DEMO FOR LLM INTERVIEWS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nThis demo covers:\")\n",
    "    print(\"  1. Scaled dot-product attention from scratch\")\n",
    "    print(\"  2. Multi-head attention implementation\")\n",
    "    print(\"  3. Causal masking for autoregressive models\")\n",
    "    print(\"  4. Attention visualization and patterns\")\n",
    "    print(\"  5. Softmax normalization effects\")\n",
    "    print(\"\\nVisualizations will be saved to: /Users/zack/dev/ml-refresher/data/interview_viz/\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Run all examples\n",
    "    example_1_basic_attention()\n",
    "    example_2_multi_head_attention()\n",
    "    example_3_causal_attention()\n",
    "    example_4_attention_patterns()\n",
    "\n",
    "    # Demonstrate softmax effect\n",
    "    demonstrate_softmax_effect()\n",
    "\n",
    "    # Print interview summary\n",
    "    print_interview_summary()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEMO COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nNext steps for interview prep:\")\n",
    "    print(\"  1. Review the visualizations in data/interview_viz/\")\n",
    "    print(\"  2. Understand each step in the scaled_dot_product_attention function\")\n",
    "    print(\"  3. Be able to explain Q, K, V intuitively\")\n",
    "    print(\"  4. Practice explaining why scaling by sqrt(d_k) matters\")\n",
    "    print(\"  5. Understand when and why to use causal masking\")\n",
    "    print(\"\\nGood luck with your interviews! \ud83d\ude80\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}