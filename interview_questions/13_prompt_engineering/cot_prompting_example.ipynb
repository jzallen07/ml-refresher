{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Chain-of-Thought Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chain-of-Thought (CoT) Prompting for LLM Interviews\n",
    "\n",
    "This module demonstrates various prompt engineering techniques, particularly\n",
    "Chain-of-Thought prompting, which is crucial for improving LLM reasoning capabilities.\n",
    "\n",
    "Key Interview Topics (Q13, Q38):\n",
    "================================\n",
    "1. Standard vs Chain-of-Thought Prompting\n",
    "2. Zero-shot CoT (\"Let's think step by step\")\n",
    "3. Few-shot CoT with examples\n",
    "4. Self-consistency sampling\n",
    "5. Prompt template best practices\n",
    "6. Task-specific prompt patterns\n",
    "\n",
    "Core Concepts:\n",
    "=============\n",
    "- Chain-of-Thought: Prompting LLMs to show intermediate reasoning steps\n",
    "- Few-shot Learning: Providing examples in the prompt\n",
    "- Zero-shot: No examples, just instructions\n",
    "- Self-consistency: Multiple reasoning paths \u2192 majority vote\n",
    "- Prompt Engineering: Crafting effective inputs to guide LLM behavior\n",
    "\n",
    "Why CoT Works:\n",
    "=============\n",
    "1. Breaks down complex reasoning into manageable steps\n",
    "2. Makes the model's reasoning process explicit and verifiable\n",
    "3. Reduces errors by preventing \"jumping to conclusions\"\n",
    "4. Enables better performance on arithmetic, logic, and multi-step problems\n",
    "5. Allows humans to audit the reasoning process\n",
    "\n",
    "Research Background:\n",
    "===================\n",
    "- Wei et al. (2022): \"Chain-of-Thought Prompting Elicits Reasoning in LLMs\"\n",
    "- Kojima et al. (2022): \"Large Language Models are Zero-Shot Reasoners\"\n",
    "- Wang et al. (2022): \"Self-Consistency Improves Chain of Thought Reasoning\"\n",
    "\n",
    "Interview Talking Points:\n",
    "========================\n",
    "1. CoT improves performance on complex reasoning tasks (math, logic, common sense)\n",
    "2. Zero-shot CoT works surprisingly well with just \"Let's think step by step\"\n",
    "3. Few-shot CoT benefits from diverse, high-quality examples\n",
    "4. Self-consistency can improve accuracy by 10-20% over greedy decoding\n",
    "5. CoT adds latency/cost due to longer outputs (important production trade-off)\n",
    "6. Not all tasks benefit equally - simple tasks may not need CoT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8708de1c",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 1: Standard vs Chain-of-Thought Prompts\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae727d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptStyle(Enum):\n",
    "    \"\"\"Different prompting styles for comparison.\"\"\"\n",
    "    STANDARD = \"standard\"\n",
    "    ZERO_SHOT_COT = \"zero_shot_cot\"\n",
    "    FEW_SHOT_COT = \"few_shot_cot\"\n",
    "    SELF_CONSISTENCY = \"self_consistency\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_prompt_example():\n",
    "    \"\"\"\n",
    "    Standard prompting: Direct question \u2192 direct answer.\n",
    "\n",
    "    Limitations:\n",
    "    - No intermediate reasoning shown\n",
    "    - Model may make logical leaps\n",
    "    - Hard to debug when wrong\n",
    "    - Lower accuracy on complex problems\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"\n",
    "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "    expected_output = \"11 tennis balls\"\n",
    "\n",
    "    return prompt, expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104474ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought_prompt_example():\n",
    "    \"\"\"\n",
    "    Chain-of-Thought prompting: Show reasoning steps before answer.\n",
    "\n",
    "    Advantages:\n",
    "    - Explicit intermediate reasoning\n",
    "    - Easier to verify correctness\n",
    "    - Better performance on multi-step problems\n",
    "    - More interpretable results\n",
    "\n",
    "    Key Insight: The model learns to break down problems by seeing examples\n",
    "    that demonstrate step-by-step reasoning.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"\n",
    "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "\n",
    "Answer: Let me work through this step-by-step:\n",
    "1. Roger starts with 5 tennis balls\n",
    "2. He buys 2 cans of tennis balls\n",
    "3. Each can contains 3 tennis balls\n",
    "4. So he buys 2 \u00d7 3 = 6 tennis balls\n",
    "5. Total tennis balls = 5 + 6 = 11\n",
    "\n",
    "Therefore, Roger has 11 tennis balls now.\n",
    "\"\"\".strip()\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a079fc9",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 2: Zero-Shot Chain-of-Thought\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_cot_template(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Zero-shot CoT: Add \"Let's think step by step\" to trigger reasoning.\n",
    "\n",
    "    Discovery (Kojima et al., 2022):\n",
    "    Simply appending \"Let's think step by step\" dramatically improves\n",
    "    reasoning on many tasks WITHOUT any examples.\n",
    "\n",
    "    Why it works:\n",
    "    - The phrase triggers the model's learned reasoning patterns\n",
    "    - Encourages decomposition of complex problems\n",
    "    - No examples needed (reduces prompt length/cost)\n",
    "\n",
    "    When to use:\n",
    "    - Math word problems\n",
    "    - Logic puzzles\n",
    "    - Multi-hop reasoning\n",
    "    - Common sense reasoning\n",
    "\n",
    "    Interview Note: This is one of the most impactful prompt engineering\n",
    "    discoveries - simple but highly effective.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "{question}\n",
    "\n",
    "Let's think step by step:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98caa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_cot_examples():\n",
    "    \"\"\"Demonstrate zero-shot CoT on different problem types.\"\"\"\n",
    "\n",
    "    examples = {\n",
    "        \"arithmetic\": {\n",
    "            \"question\": \"If a shirt costs $15 and is on sale for 20% off, what is the final price?\",\n",
    "            \"prompt\": zero_shot_cot_template(\n",
    "                \"If a shirt costs $15 and is on sale for 20% off, what is the final price?\"\n",
    "            ),\n",
    "            \"reasoning\": \"\"\"\n",
    "1. Original price: $15\n",
    "2. Discount: 20% of $15 = 0.20 \u00d7 $15 = $3\n",
    "3. Final price: $15 - $3 = $12\n",
    "\n",
    "Answer: $12\n",
    "\"\"\"\n",
    "        },\n",
    "\n",
    "        \"logic\": {\n",
    "            \"question\": \"All roses are flowers. Some flowers fade quickly. Does it follow that some roses fade quickly?\",\n",
    "            \"prompt\": zero_shot_cot_template(\n",
    "                \"All roses are flowers. Some flowers fade quickly. Does it follow that some roses fade quickly?\"\n",
    "            ),\n",
    "            \"reasoning\": \"\"\"\n",
    "1. Given: All roses are flowers (roses \u2286 flowers)\n",
    "2. Given: Some flowers fade quickly (\u2203 flowers that fade quickly)\n",
    "3. Question: Do some roses fade quickly?\n",
    "4. The second statement tells us some flowers fade quickly, but doesn't specify which flowers\n",
    "5. Roses are flowers, but we don't know if the quickly-fading flowers include roses\n",
    "6. This is invalid logical inference\n",
    "\n",
    "Answer: No, it does not follow. The quickly-fading flowers might not include any roses.\n",
    "\"\"\"\n",
    "        },\n",
    "\n",
    "        \"common_sense\": {\n",
    "            \"question\": \"John put his laptop in his backpack and went to the library. When he arrived, his laptop was gone. What likely happened?\",\n",
    "            \"prompt\": zero_shot_cot_template(\n",
    "                \"John put his laptop in his backpack and went to the library. When he arrived, his laptop was gone. What likely happened?\"\n",
    "            ),\n",
    "            \"reasoning\": \"\"\"\n",
    "1. John put laptop in backpack (confirmed)\n",
    "2. Laptop disappeared between home and library\n",
    "3. Possible explanations:\n",
    "   - Backpack zipper was open \u2192 laptop fell out\n",
    "   - Laptop was stolen from backpack\n",
    "   - John forgot to close backpack properly\n",
    "   - Someone accessed his backpack during transit\n",
    "4. Most common scenario: open/insecure backpack allowing loss/theft\n",
    "\n",
    "Answer: The laptop likely fell out of an open or improperly closed backpack,\n",
    "or was stolen if the backpack was left unattended.\n",
    "\"\"\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809cb50",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 3: Few-Shot Chain-of-Thought\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CoTExample:\n",
    "    \"\"\"Structure for a Chain-of-Thought example.\"\"\"\n",
    "    question: str\n",
    "    reasoning: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a19463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_few_shot_cot_prompt(\n",
    "    examples: List[CoTExample],\n",
    "    test_question: str,\n",
    "    instruction: str = \"Answer the following questions with step-by-step reasoning.\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a few-shot CoT prompt with examples.\n",
    "\n",
    "    Best Practices:\n",
    "    ===============\n",
    "    1. Use 3-8 examples (more examples = better but more expensive)\n",
    "    2. Examples should be diverse (cover different problem types)\n",
    "    3. Show consistent reasoning format across examples\n",
    "    4. Include both simple and complex examples\n",
    "    5. Make sure reasoning is correct and clear\n",
    "\n",
    "    Structure:\n",
    "    ==========\n",
    "    [Instruction]\n",
    "    [Example 1: Question \u2192 Reasoning \u2192 Answer]\n",
    "    [Example 2: Question \u2192 Reasoning \u2192 Answer]\n",
    "    ...\n",
    "    [Test Question]\n",
    "\n",
    "    Interview Note: Quality of examples matters more than quantity.\n",
    "    Bad examples can hurt performance!\n",
    "    \"\"\"\n",
    "    prompt_parts = [instruction, \"\"]\n",
    "\n",
    "    # Add examples\n",
    "    for i, example in enumerate(examples, 1):\n",
    "        prompt_parts.append(f\"Question {i}: {example.question}\")\n",
    "        prompt_parts.append(f\"\\nReasoning: {example.reasoning}\")\n",
    "        prompt_parts.append(f\"\\nAnswer: {example.answer}\")\n",
    "        prompt_parts.append(\"\")  # Empty line between examples\n",
    "\n",
    "    # Add test question\n",
    "    prompt_parts.append(f\"Question {len(examples) + 1}: {test_question}\")\n",
    "    prompt_parts.append(\"\\nReasoning:\")\n",
    "\n",
    "    return \"\\n\".join(prompt_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_cot_math_examples() -> List[CoTExample]:\n",
    "    \"\"\"\n",
    "    High-quality few-shot examples for math word problems.\n",
    "\n",
    "    Example Selection Strategy:\n",
    "    - Mix of operations (addition, subtraction, multiplication, division)\n",
    "    - Different complexity levels\n",
    "    - Clear, consistent reasoning format\n",
    "    - Explicitly show calculations\n",
    "    \"\"\"\n",
    "    return [\n",
    "        CoTExample(\n",
    "            question=\"A cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\",\n",
    "            reasoning=\"\"\"\n",
    "Step 1: Start with initial amount: 23 apples\n",
    "Step 2: Subtract apples used: 23 - 20 = 3 apples remaining\n",
    "Step 3: Add newly bought apples: 3 + 6 = 9 apples\n",
    "\"\"\",\n",
    "            answer=\"9 apples\"\n",
    "        ),\n",
    "\n",
    "        CoTExample(\n",
    "            question=\"Sarah has 3 boxes of cookies. Each box contains 12 cookies. She gives away 8 cookies. How many does she have left?\",\n",
    "            reasoning=\"\"\"\n",
    "Step 1: Calculate total cookies: 3 boxes \u00d7 12 cookies/box = 36 cookies\n",
    "Step 2: Subtract cookies given away: 36 - 8 = 28 cookies\n",
    "\"\"\",\n",
    "            answer=\"28 cookies\"\n",
    "        ),\n",
    "\n",
    "        CoTExample(\n",
    "            question=\"A parking lot has 4 levels. Each level can hold 15 cars. If 47 cars are parked, how many spaces are free?\",\n",
    "            reasoning=\"\"\"\n",
    "Step 1: Calculate total capacity: 4 levels \u00d7 15 cars/level = 60 spaces\n",
    "Step 2: Calculate free spaces: 60 - 47 = 13 spaces\n",
    "\"\"\",\n",
    "            answer=\"13 free spaces\"\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26640bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_cot_logic_examples() -> List[CoTExample]:\n",
    "    \"\"\"Few-shot examples for logical reasoning tasks.\"\"\"\n",
    "    return [\n",
    "        CoTExample(\n",
    "            question=\"If all mammals are warm-blooded and all dogs are mammals, are all dogs warm-blooded?\",\n",
    "            reasoning=\"\"\"\n",
    "Step 1: Identify premises:\n",
    "  - P1: All mammals are warm-blooded (mammals \u2192 warm-blooded)\n",
    "  - P2: All dogs are mammals (dogs \u2192 mammals)\n",
    "Step 2: Apply transitive property:\n",
    "  - If dogs \u2192 mammals AND mammals \u2192 warm-blooded\n",
    "  - Then dogs \u2192 warm-blooded\n",
    "Step 3: This is valid deductive reasoning (modus ponens)\n",
    "\"\"\",\n",
    "            answer=\"Yes, all dogs are warm-blooded.\"\n",
    "        ),\n",
    "\n",
    "        CoTExample(\n",
    "            question=\"Some birds can fly. Penguins are birds. Can penguins fly?\",\n",
    "            reasoning=\"\"\"\n",
    "Step 1: Identify premises:\n",
    "  - P1: Some birds can fly (\u2203 birds that fly)\n",
    "  - P2: Penguins are birds (penguins \u2286 birds)\n",
    "Step 2: Analyze logical relationship:\n",
    "  - \"Some birds\" does not mean \"all birds\"\n",
    "  - We cannot conclude properties of specific birds from \"some birds\"\n",
    "Step 3: This is invalid reasoning (existential quantifier doesn't apply to all)\n",
    "\"\"\",\n",
    "            answer=\"No, we cannot conclude that penguins can fly. In fact, penguins cannot fly despite being birds.\"\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b205ea",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 4: Self-Consistency with CoT\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency_template(question: str, num_paths: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Self-consistency: Sample multiple reasoning paths and take majority vote.\n",
    "\n",
    "    Algorithm (Wang et al., 2022):\n",
    "    ==============================\n",
    "    1. Generate multiple CoT reasoning paths (using temperature > 0)\n",
    "    2. Extract final answer from each path\n",
    "    3. Take majority vote among answers\n",
    "    4. Return most common answer\n",
    "\n",
    "    Why it works:\n",
    "    =============\n",
    "    - Different reasoning paths may catch different errors\n",
    "    - Correct answers tend to be more consistent across paths\n",
    "    - Reduces impact of single-path errors\n",
    "    - Works best with diverse reasoning (higher temperature)\n",
    "\n",
    "    Performance Gains:\n",
    "    ==================\n",
    "    - Typically 5-15% accuracy improvement over single CoT\n",
    "    - Diminishing returns after 10-20 samples\n",
    "    - Trade-off: 5-10x more API calls/cost\n",
    "\n",
    "    Production Considerations:\n",
    "    =========================\n",
    "    - Use for high-stakes decisions where accuracy > cost\n",
    "    - Can parallelize API calls for lower latency\n",
    "    - Consider caching if same questions asked repeatedly\n",
    "\n",
    "    Interview Note: This is a classic bias-variance trade-off.\n",
    "    More samples reduce variance (improve robustness) at higher cost.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "I will solve this problem multiple times using different reasoning approaches,\n",
    "then determine the most consistent answer.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "Please provide {num_paths} different reasoning paths to solve this problem.\n",
    "Each path should:\n",
    "1. Show clear step-by-step reasoning\n",
    "2. Arrive at a final answer\n",
    "3. Use a potentially different approach or perspective\n",
    "\n",
    "After all paths, identify the most common answer.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ae974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency_explanation():\n",
    "    \"\"\"\n",
    "    Detailed explanation of self-consistency for interviews.\n",
    "    \"\"\"\n",
    "    explanation = \"\"\"\n",
    "=============================================================================\n",
    "SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT\n",
    "=============================================================================\n",
    "\n",
    "CONCEPT:\n",
    "--------\n",
    "Instead of using a single reasoning path (greedy decoding), generate multiple\n",
    "diverse reasoning paths and aggregate their answers via majority voting.\n",
    "\n",
    "ALGORITHM:\n",
    "----------\n",
    "```\n",
    "Input: Question Q, LLM M, Number of samples N\n",
    "Output: Final answer A\n",
    "\n",
    "1. For i = 1 to N:\n",
    "     Generate reasoning path R_i with high temperature (e.g., 0.7)\n",
    "     Extract answer A_i from R_i\n",
    "\n",
    "2. Count frequency of each unique answer\n",
    "3. Return most frequent answer (majority vote)\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "--------\n",
    "Question: \"If 3 cats catch 3 mice in 3 minutes, how many cats are needed\n",
    "           to catch 100 mice in 100 minutes?\"\n",
    "\n",
    "Path 1 (Correct):\n",
    "- Rate: 1 cat catches 1 mouse in 3 minutes\n",
    "- In 100 minutes: 1 cat catches 100/3 \u2248 33 mice\n",
    "- Need 3 cats to catch 100 mice in 100 minutes\n",
    "Answer: 3 cats\n",
    "\n",
    "Path 2 (Correct):\n",
    "- 3 cats, 3 mice, 3 minutes \u2192 rate is constant\n",
    "- Same setup, so same number of cats needed\n",
    "Answer: 3 cats\n",
    "\n",
    "Path 3 (Incorrect):\n",
    "- More mice need more cats: 100/3 \u2248 33 cats\n",
    "Answer: 33 cats\n",
    "\n",
    "Path 4 (Correct):\n",
    "- Each cat catches 1 mouse per 3 minutes\n",
    "- In 100 minutes, each cat catches ~33 mice\n",
    "- 3 cats catch ~100 mice\n",
    "Answer: 3 cats\n",
    "\n",
    "Path 5 (Correct):\n",
    "- Ratio analysis: same time ratio = same cat ratio\n",
    "Answer: 3 cats\n",
    "\n",
    "MAJORITY VOTE: 3 cats (appears 4 times) \u2192 Final Answer\n",
    "\n",
    "BENEFITS:\n",
    "---------\n",
    "\u2713 More robust to reasoning errors\n",
    "\u2713 Builds confidence in answers (high agreement = high confidence)\n",
    "\u2713 Can identify when model is uncertain (low agreement)\n",
    "\u2713 Often improves accuracy by 10-20%\n",
    "\n",
    "DRAWBACKS:\n",
    "----------\n",
    "\u2717 Higher computational cost (N times more expensive)\n",
    "\u2717 Higher latency (unless parallelized)\n",
    "\u2717 Not always necessary for simple questions\n",
    "\u2717 Requires post-processing to extract and compare answers\n",
    "\n",
    "WHEN TO USE:\n",
    "------------\n",
    "1. High-stakes decisions (medical, financial, legal)\n",
    "2. Complex reasoning tasks (math, logic, planning)\n",
    "3. When model confidence is important\n",
    "4. When you can afford higher cost/latency\n",
    "\n",
    "WHEN TO AVOID:\n",
    "--------------\n",
    "1. Simple questions with obvious answers\n",
    "2. Real-time applications with strict latency requirements\n",
    "3. Cost-sensitive applications\n",
    "4. Tasks where reasoning diversity doesn't help\n",
    "\n",
    "IMPLEMENTATION TIPS:\n",
    "-------------------\n",
    "1. Use temperature 0.7-0.8 for diversity (not too high or too low)\n",
    "2. Start with N=5, increase if needed (diminishing returns after 10-20)\n",
    "3. Parallelize API calls when possible\n",
    "4. Implement robust answer extraction (regex, parsing, etc.)\n",
    "5. Handle ties (could use confidence, answer length, etc.)\n",
    "\n",
    "INTERVIEW INSIGHT:\n",
    "-----------------\n",
    "Self-consistency is an ensemble method applied to reasoning. Like random\n",
    "forests in ML, it improves performance by aggregating diverse predictions.\n",
    "The key innovation is recognizing that LLMs can generate diverse reasoning\n",
    "paths, not just diverse final answers.\n",
    "\"\"\"\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ccd33",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 5: Task-Specific Prompt Templates\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed29c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplateLibrary:\n",
    "    \"\"\"\n",
    "    Collection of proven prompt templates for common tasks.\n",
    "\n",
    "    These templates encode best practices for different task types.\n",
    "    Can be customized for specific use cases.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def classification_template(\n",
    "        text: str,\n",
    "        labels: List[str],\n",
    "        description: str = \"\",\n",
    "        few_shot_examples: Optional[List[Tuple[str, str]]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Template for text classification tasks.\n",
    "\n",
    "        Best Practices:\n",
    "        - Clearly list all possible labels\n",
    "        - Provide label descriptions if ambiguous\n",
    "        - Use few-shot examples for better accuracy\n",
    "        - Ask for reasoning before classification (improves accuracy)\n",
    "        \"\"\"\n",
    "        prompt_parts = [\"Text Classification Task\"]\n",
    "\n",
    "        if description:\n",
    "            prompt_parts.append(f\"\\nTask Description: {description}\")\n",
    "\n",
    "        prompt_parts.append(f\"\\nPossible Labels: {', '.join(labels)}\")\n",
    "\n",
    "        # Add few-shot examples if provided\n",
    "        if few_shot_examples:\n",
    "            prompt_parts.append(\"\\nExamples:\")\n",
    "            for example_text, example_label in few_shot_examples:\n",
    "                prompt_parts.append(f\"\\nText: {example_text}\")\n",
    "                prompt_parts.append(f\"Label: {example_label}\")\n",
    "\n",
    "        # Add test case\n",
    "        prompt_parts.append(f\"\\nNow classify this text:\")\n",
    "        prompt_parts.append(f\"\\nText: {text}\")\n",
    "        prompt_parts.append(\"\\nFirst, provide brief reasoning for your classification.\")\n",
    "        prompt_parts.append(\"Then provide the final label.\")\n",
    "        prompt_parts.append(\"\\nReasoning:\")\n",
    "\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    @staticmethod\n",
    "    def summarization_template(\n",
    "        text: str,\n",
    "        max_length: Optional[int] = None,\n",
    "        style: str = \"concise\",\n",
    "        focus: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Template for text summarization.\n",
    "\n",
    "        Parameters:\n",
    "        - max_length: Maximum words/sentences in summary\n",
    "        - style: concise, detailed, bullet-points, etc.\n",
    "        - focus: Specific aspect to focus on\n",
    "        \"\"\"\n",
    "        prompt_parts = [\"Please summarize the following text.\"]\n",
    "\n",
    "        if max_length:\n",
    "            prompt_parts.append(f\"\\nMaximum length: {max_length} words\")\n",
    "\n",
    "        if style:\n",
    "            prompt_parts.append(f\"Style: {style}\")\n",
    "\n",
    "        if focus:\n",
    "            prompt_parts.append(f\"Focus particularly on: {focus}\")\n",
    "\n",
    "        prompt_parts.append(f\"\\nText to summarize:\\n{text}\")\n",
    "        prompt_parts.append(\"\\nSummary:\")\n",
    "\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    @staticmethod\n",
    "    def question_answering_template(\n",
    "        context: str,\n",
    "        question: str,\n",
    "        answer_style: str = \"concise\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Template for question answering with context.\n",
    "\n",
    "        Best Practices:\n",
    "        - Provide context before question\n",
    "        - Specify desired answer format\n",
    "        - Ask model to cite context (reduces hallucination)\n",
    "        - Request \"I don't know\" if answer not in context\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Instructions:\n",
    "- Only use information from the provided context\n",
    "- If the answer is not in the context, say \"I don't know\"\n",
    "- Provide a {answer_style} answer\n",
    "- Cite specific parts of the context if possible\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def extraction_template(\n",
    "        text: str,\n",
    "        entities_to_extract: List[str],\n",
    "        output_format: str = \"json\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Template for information extraction tasks.\n",
    "\n",
    "        Common use cases:\n",
    "        - Named entity recognition\n",
    "        - Structured data extraction\n",
    "        - Key information retrieval\n",
    "        \"\"\"\n",
    "        entities_str = \", \".join(entities_to_extract)\n",
    "\n",
    "        return f\"\"\"\n",
    "Extract the following information from the text below:\n",
    "{entities_str}\n",
    "\n",
    "Return the results in {output_format} format.\n",
    "If any information is not found, use null or \"Not found\" as the value.\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Extracted Information:\n",
    "\"\"\".strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def creative_generation_template(\n",
    "        task: str,\n",
    "        constraints: Optional[List[str]] = None,\n",
    "        tone: str = \"professional\",\n",
    "        examples: Optional[List[str]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Template for creative text generation.\n",
    "\n",
    "        Use cases:\n",
    "        - Writing assistance\n",
    "        - Content generation\n",
    "        - Brainstorming\n",
    "        \"\"\"\n",
    "        prompt_parts = [f\"Task: {task}\"]\n",
    "\n",
    "        if tone:\n",
    "            prompt_parts.append(f\"\\nTone: {tone}\")\n",
    "\n",
    "        if constraints:\n",
    "            prompt_parts.append(\"\\nConstraints:\")\n",
    "            for constraint in constraints:\n",
    "                prompt_parts.append(f\"- {constraint}\")\n",
    "\n",
    "        if examples:\n",
    "            prompt_parts.append(\"\\nStyle Examples:\")\n",
    "            for i, example in enumerate(examples, 1):\n",
    "                prompt_parts.append(f\"\\nExample {i}: {example}\")\n",
    "\n",
    "        prompt_parts.append(\"\\nGenerated Text:\")\n",
    "\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    @staticmethod\n",
    "    def reasoning_template(\n",
    "        problem: str,\n",
    "        reasoning_type: str = \"step-by-step\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Template for explicit reasoning tasks.\n",
    "\n",
    "        Reasoning types:\n",
    "        - step-by-step: Sequential reasoning\n",
    "        - pros-cons: Decision making\n",
    "        - cause-effect: Causal analysis\n",
    "        - compare-contrast: Comparison\n",
    "        \"\"\"\n",
    "        templates = {\n",
    "            \"step-by-step\": f\"\"\"\n",
    "Problem: {problem}\n",
    "\n",
    "Please solve this problem using step-by-step reasoning:\n",
    "1. Break down the problem into smaller parts\n",
    "2. Solve each part sequentially\n",
    "3. Show all intermediate steps\n",
    "4. Arrive at a final answer\n",
    "\n",
    "Solution:\n",
    "\"\"\",\n",
    "            \"pros-cons\": f\"\"\"\n",
    "Question: {problem}\n",
    "\n",
    "Analyze this by listing:\n",
    "1. Pros (advantages, benefits, positive aspects)\n",
    "2. Cons (disadvantages, drawbacks, negative aspects)\n",
    "3. Final recommendation based on the analysis\n",
    "\n",
    "Analysis:\n",
    "\"\"\",\n",
    "            \"cause-effect\": f\"\"\"\n",
    "Situation: {problem}\n",
    "\n",
    "Analyze the cause-and-effect relationships:\n",
    "1. Identify the main causes\n",
    "2. Explain the mechanisms\n",
    "3. Describe the effects/consequences\n",
    "4. Consider second-order effects\n",
    "\n",
    "Analysis:\n",
    "\"\"\",\n",
    "            \"compare-contrast\": f\"\"\"\n",
    "Items to compare: {problem}\n",
    "\n",
    "Provide a detailed comparison:\n",
    "1. Similarities between the items\n",
    "2. Key differences\n",
    "3. Relative advantages of each\n",
    "4. Conclusion or recommendation\n",
    "\n",
    "Comparison:\n",
    "\"\"\"\n",
    "        }\n",
    "\n",
    "        return templates.get(reasoning_type, templates[\"step-by-step\"]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfceef4",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 6: Advanced Prompt Engineering Techniques\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70091e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedPromptTechniques:\n",
    "    \"\"\"\n",
    "    Advanced techniques for prompt engineering.\n",
    "\n",
    "    These go beyond basic CoT and are useful for specialized scenarios.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def least_to_most_prompting(problem: str, subproblems: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Least-to-Most Prompting: Solve subproblems sequentially.\n",
    "\n",
    "        Idea: Break complex problem into simpler subproblems, solve in order.\n",
    "        Each subproblem's solution is used to solve the next.\n",
    "\n",
    "        When to use:\n",
    "        - Compositional problems\n",
    "        - Problems with clear dependency structure\n",
    "        - Mathematical proofs\n",
    "        \"\"\"\n",
    "        prompt_parts = [\n",
    "            \"I'll solve this problem by breaking it into simpler subproblems.\",\n",
    "            f\"\\nMain Problem: {problem}\",\n",
    "            \"\\nSubproblems to solve in order:\"\n",
    "        ]\n",
    "\n",
    "        for i, subproblem in enumerate(subproblems, 1):\n",
    "            prompt_parts.append(f\"{i}. {subproblem}\")\n",
    "\n",
    "        prompt_parts.append(\"\\nLet's solve these one by one:\")\n",
    "\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    @staticmethod\n",
    "    def program_aided_language_model(problem: str) -> str:\n",
    "        \"\"\"\n",
    "        Program-Aided Language Model (PAL): Generate code to solve problem.\n",
    "\n",
    "        Idea: LLM generates Python code, we execute code for final answer.\n",
    "        Combines language understanding with precise computation.\n",
    "\n",
    "        Benefits:\n",
    "        - Perfect arithmetic (no calculation errors)\n",
    "        - Complex computations possible\n",
    "        - Verifiable logic\n",
    "\n",
    "        When to use:\n",
    "        - Math problems\n",
    "        - Data analysis\n",
    "        - Algorithmic problems\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "Problem: {problem}\n",
    "\n",
    "Generate Python code to solve this problem. The code should:\n",
    "1. Define all necessary variables\n",
    "2. Perform the required calculations\n",
    "3. Print the final answer\n",
    "\n",
    "Requirements:\n",
    "- Use descriptive variable names\n",
    "- Add comments explaining each step\n",
    "- Ensure code is executable\n",
    "- Handle edge cases\n",
    "\n",
    "Python Code:\n",
    "```python\n",
    "\"\"\".strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def react_prompting(task: str) -> str:\n",
    "        \"\"\"\n",
    "        ReAct: Reasoning + Acting (for agents with tools).\n",
    "\n",
    "        Idea: Interleave reasoning, action, and observation steps.\n",
    "\n",
    "        Format:\n",
    "        Thought: [reasoning about what to do]\n",
    "        Action: [action to take, e.g., search Wikipedia]\n",
    "        Observation: [result of action]\n",
    "        ... (repeat)\n",
    "        Thought: I now know the final answer\n",
    "        Answer: [final answer]\n",
    "\n",
    "        When to use:\n",
    "        - Agent-based systems\n",
    "        - Multi-step tasks requiring external info\n",
    "        - Problems needing tool use\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "Task: {task}\n",
    "\n",
    "Solve this task using the Thought-Action-Observation framework:\n",
    "\n",
    "Thought: [Your reasoning about what to do next]\n",
    "Action: [Action to take - specify tool and input]\n",
    "Observation: [Result of the action - will be provided]\n",
    "\n",
    "Repeat this process until you have enough information, then provide:\n",
    "\n",
    "Thought: I now have enough information to answer\n",
    "Answer: [Your final answer]\n",
    "\n",
    "Begin:\n",
    "\n",
    "Thought:\n",
    "\"\"\".strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def tree_of_thoughts_prompt(problem: str, num_branches: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Tree of Thoughts: Explore multiple reasoning branches.\n",
    "\n",
    "        Idea: Generate multiple thought steps, evaluate them, explore best ones.\n",
    "        More systematic than self-consistency.\n",
    "\n",
    "        Process:\n",
    "        1. Generate multiple next-step thoughts\n",
    "        2. Evaluate each thought's promise\n",
    "        3. Explore best thought(s)\n",
    "        4. Repeat until solution found\n",
    "\n",
    "        When to use:\n",
    "        - Strategic problems (e.g., game playing)\n",
    "        - Creative tasks (e.g., writing)\n",
    "        - Problems with many possible approaches\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "Problem: {problem}\n",
    "\n",
    "Use Tree of Thoughts reasoning:\n",
    "\n",
    "Step 1: Generate {num_branches} different initial approaches\n",
    "For each approach, rate its promise (1-10)\n",
    "\n",
    "Approach 1: [description]\n",
    "Promise: [rating]\n",
    "\n",
    "Approach 2: [description]\n",
    "Promise: [rating]\n",
    "\n",
    "Approach 3: [description]\n",
    "Promise: [rating]\n",
    "\n",
    "Step 2: Expand the most promising approach(es)\n",
    "[Continue exploration]\n",
    "\n",
    "Step 3: If solution found, provide it. Otherwise, backtrack and try another branch.\n",
    "\n",
    "Begin:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ef73e",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 7: Prompt Engineering Best Practices\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37493e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptBestPractices:\n",
    "    \"\"\"\n",
    "    Compilation of prompt engineering best practices.\n",
    "    Essential knowledge for interviews.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_best_practices() -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Comprehensive list of prompt engineering best practices.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"Clarity\": [\n",
    "                \"Be specific and unambiguous\",\n",
    "                \"Use clear, simple language\",\n",
    "                \"Define technical terms if necessary\",\n",
    "                \"Avoid pronouns when unclear (use specific nouns)\",\n",
    "                \"Structure complex prompts with sections/bullets\"\n",
    "            ],\n",
    "\n",
    "            \"Context\": [\n",
    "                \"Provide sufficient background information\",\n",
    "                \"Include relevant constraints and requirements\",\n",
    "                \"Specify desired output format\",\n",
    "                \"Give examples of expected behavior\",\n",
    "                \"Set the role/persona if helpful ('You are an expert...')\"\n",
    "            ],\n",
    "\n",
    "            \"Few-Shot Examples\": [\n",
    "                \"Use 3-8 examples (more is not always better)\",\n",
    "                \"Ensure examples are correct and high-quality\",\n",
    "                \"Show diversity in examples\",\n",
    "                \"Use consistent formatting across examples\",\n",
    "                \"Put examples in order of increasing difficulty\"\n",
    "            ],\n",
    "\n",
    "            \"Output Control\": [\n",
    "                \"Specify desired length/detail level\",\n",
    "                \"Request specific format (JSON, markdown, etc.)\",\n",
    "                \"Ask for structured output when appropriate\",\n",
    "                \"Use delimiters for multi-part responses\",\n",
    "                \"Request confidence scores if needed\"\n",
    "            ],\n",
    "\n",
    "            \"Reasoning\": [\n",
    "                \"Ask for step-by-step reasoning on complex tasks\",\n",
    "                \"Use 'Let's think step by step' for zero-shot CoT\",\n",
    "                \"Request explanations before final answers\",\n",
    "                \"Ask model to verify its own answers\",\n",
    "                \"Break complex tasks into subtasks\"\n",
    "            ],\n",
    "\n",
    "            \"Safety & Reliability\": [\n",
    "                \"Ask model to say 'I don't know' when uncertain\",\n",
    "                \"Request citations for factual claims\",\n",
    "                \"Warn against hallucination explicitly\",\n",
    "                \"Test prompts on edge cases\",\n",
    "                \"Use temperature 0 for consistency, >0 for creativity\"\n",
    "            ],\n",
    "\n",
    "            \"Efficiency\": [\n",
    "                \"Front-load the most important information\",\n",
    "                \"Remove unnecessary verbosity\",\n",
    "                \"Reuse prompt templates when possible\",\n",
    "                \"Cache static prompt components\",\n",
    "                \"Consider cost vs. quality trade-offs\"\n",
    "            ],\n",
    "\n",
    "            \"Testing\": [\n",
    "                \"Test prompts on diverse inputs\",\n",
    "                \"Create evaluation sets\",\n",
    "                \"Measure accuracy, consistency, latency\",\n",
    "                \"A/B test prompt variations\",\n",
    "                \"Monitor performance in production\"\n",
    "            ],\n",
    "\n",
    "            \"Common Pitfalls to Avoid\": [\n",
    "                \"Being too vague or ambiguous\",\n",
    "                \"Overloading with too much information\",\n",
    "                \"Using contradictory instructions\",\n",
    "                \"Forgetting to specify output format\",\n",
    "                \"Not testing on edge cases\",\n",
    "                \"Assuming model has real-time knowledge\",\n",
    "                \"Expecting perfect consistency without temperature 0\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def prompt_evaluation_criteria() -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Criteria for evaluating prompt quality.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"Clarity\": \"Is the prompt unambiguous and easy to understand?\",\n",
    "            \"Completeness\": \"Does it provide all necessary information?\",\n",
    "            \"Specificity\": \"Is it specific enough to get desired output?\",\n",
    "            \"Efficiency\": \"Is it as concise as possible without losing quality?\",\n",
    "            \"Robustness\": \"Does it work on various inputs/edge cases?\",\n",
    "            \"Consistency\": \"Does it produce consistent outputs?\",\n",
    "            \"Safety\": \"Does it include appropriate guardrails?\",\n",
    "            \"Measurability\": \"Can output quality be measured objectively?\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577ce42",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 8: Demonstration and Examples\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27d32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_all_techniques():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of all prompt techniques.\n",
    "    Run this to see examples of each technique.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CHAIN-OF-THOUGHT PROMPTING DEMONSTRATION\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 1. Standard vs CoT\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"1. STANDARD PROMPTING (No Reasoning)\")\n",
    "    print(\"=\" * 80)\n",
    "    standard, expected = standard_prompt_example()\n",
    "    print(standard)\n",
    "    print(f\"\\n[Expected: {expected}]\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"2. CHAIN-OF-THOUGHT PROMPTING (With Reasoning)\")\n",
    "    print(\"=\" * 80)\n",
    "    cot = chain_of_thought_prompt_example()\n",
    "    print(cot)\n",
    "\n",
    "    # 2. Zero-Shot CoT\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"3. ZERO-SHOT CHAIN-OF-THOUGHT\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    examples = zero_shot_cot_examples()\n",
    "    for task_type, example_data in examples.items():\n",
    "        print(f\"\\n--- {task_type.upper()} EXAMPLE ---\")\n",
    "        print(f\"\\nPrompt:\\n{example_data['prompt']}\")\n",
    "        print(f\"\\nExpected Reasoning:\\n{example_data['reasoning']}\")\n",
    "\n",
    "    # 3. Few-Shot CoT\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"4. FEW-SHOT CHAIN-OF-THOUGHT\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    math_examples = few_shot_cot_math_examples()\n",
    "    test_question = \"A store had 25 bottles of juice. They sold 17 bottles and received a shipment of 30 more. How many bottles do they have now?\"\n",
    "\n",
    "    few_shot_prompt = build_few_shot_cot_prompt(\n",
    "        examples=math_examples,\n",
    "        test_question=test_question\n",
    "    )\n",
    "    print(few_shot_prompt)\n",
    "\n",
    "    # 4. Self-Consistency\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"5. SELF-CONSISTENCY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(self_consistency_explanation())\n",
    "\n",
    "    # 5. Task-Specific Templates\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"6. TASK-SPECIFIC TEMPLATES\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    library = PromptTemplateLibrary()\n",
    "\n",
    "    print(\"\\n--- CLASSIFICATION ---\")\n",
    "    classification_prompt = library.classification_template(\n",
    "        text=\"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.\",\n",
    "        labels=[\"positive\", \"negative\", \"neutral\"],\n",
    "        description=\"Classify the sentiment of movie reviews\"\n",
    "    )\n",
    "    print(classification_prompt)\n",
    "\n",
    "    print(\"\\n\\n--- QUESTION ANSWERING ---\")\n",
    "    qa_prompt = library.question_answering_template(\n",
    "        context=\"The Eiffel Tower was built in 1889 for the World's Fair. It stands 330 meters tall and was designed by Gustave Eiffel. Initially criticized, it has become a global cultural icon of France.\",\n",
    "        question=\"When was the Eiffel Tower built and who designed it?\"\n",
    "    )\n",
    "    print(qa_prompt)\n",
    "\n",
    "    print(\"\\n\\n--- EXTRACTION ---\")\n",
    "    extraction_prompt = library.extraction_template(\n",
    "        text=\"John Smith (john.smith@email.com) purchased a laptop for $1299 on May 15, 2024. Order #12345.\",\n",
    "        entities_to_extract=[\"customer_name\", \"email\", \"product\", \"price\", \"date\", \"order_number\"],\n",
    "        output_format=\"json\"\n",
    "    )\n",
    "    print(extraction_prompt)\n",
    "\n",
    "    # 6. Advanced Techniques\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"7. ADVANCED TECHNIQUES\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    advanced = AdvancedPromptTechniques()\n",
    "\n",
    "    print(\"\\n--- PROGRAM-AIDED LANGUAGE MODEL (PAL) ---\")\n",
    "    pal_prompt = advanced.program_aided_language_model(\n",
    "        \"Calculate the compound interest on $5000 at 5% annual rate for 3 years, compounded quarterly.\"\n",
    "    )\n",
    "    print(pal_prompt)\n",
    "\n",
    "    print(\"\\n\\n--- ReAct (REASONING + ACTING) ---\")\n",
    "    react_prompt = advanced.react_prompting(\n",
    "        \"What is the population of the capital city of the country where the Eiffel Tower is located?\"\n",
    "    )\n",
    "    print(react_prompt)\n",
    "\n",
    "    # 7. Best Practices Summary\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"8. BEST PRACTICES SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    best_practices = PromptBestPractices()\n",
    "    practices = best_practices.get_best_practices()\n",
    "\n",
    "    for category, tips in practices.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for tip in tips:\n",
    "            print(f\"  \u2022 {tip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interview_cheat_sheet():\n",
    "    \"\"\"\n",
    "    Quick reference for interview questions.\n",
    "    \"\"\"\n",
    "    cheat_sheet = \"\"\"\n",
    "================================================================================\n",
    "CHAIN-OF-THOUGHT PROMPTING - INTERVIEW CHEAT SHEET\n",
    "================================================================================\n",
    "\n",
    "Q: What is Chain-of-Thought prompting?\n",
    "A: A technique where you prompt LLMs to show intermediate reasoning steps\n",
    "   before providing final answers, improving performance on complex reasoning.\n",
    "\n",
    "Q: What are the main types of CoT?\n",
    "A: 1. Few-shot CoT: Provide examples with reasoning\n",
    "   2. Zero-shot CoT: Just add \"Let's think step by step\"\n",
    "   3. Self-consistency: Multiple reasoning paths + majority vote\n",
    "\n",
    "Q: When should you use CoT?\n",
    "A: \u2022 Math word problems\n",
    "   \u2022 Multi-step reasoning\n",
    "   \u2022 Logic puzzles\n",
    "   \u2022 Common sense reasoning\n",
    "   \u2022 Any task where intermediate steps matter\n",
    "\n",
    "Q: What are the trade-offs?\n",
    "A: Pros: Better accuracy, interpretable, debuggable\n",
    "   Cons: Longer outputs (higher cost/latency), not always needed for simple tasks\n",
    "\n",
    "Q: How does self-consistency work?\n",
    "A: Generate multiple diverse reasoning paths (temperature > 0), extract final\n",
    "   answer from each, take majority vote. Improves accuracy 10-20%.\n",
    "\n",
    "Q: What's the difference between few-shot and zero-shot CoT?\n",
    "A: Few-shot: Provide examples showing reasoning (better but longer prompts)\n",
    "   Zero-shot: Just add \"Let's think step by step\" (simple but effective)\n",
    "\n",
    "Q: How many examples should you use for few-shot CoT?\n",
    "A: 3-8 examples is typical. More isn't always better; quality matters more\n",
    "   than quantity.\n",
    "\n",
    "Q: How do you evaluate prompt quality?\n",
    "A: Test on: accuracy, consistency, edge cases, latency, cost\n",
    "   Use: eval sets, A/B testing, human review\n",
    "\n",
    "Q: What are common prompt engineering mistakes?\n",
    "A: \u2022 Too vague or ambiguous\n",
    "   \u2022 Conflicting instructions\n",
    "   \u2022 Not specifying output format\n",
    "   \u2022 Skipping edge case testing\n",
    "   \u2022 Forgetting about cost/latency\n",
    "\n",
    "Q: What's the most important prompt engineering insight?\n",
    "A: Clear, specific instructions with examples work best. Test and iterate.\n",
    "   Small prompt changes can have big impact on output quality.\n",
    "\n",
    "================================================================================\n",
    "KEY PAPERS TO MENTION:\n",
    "================================================================================\n",
    "1. Wei et al. (2022) - \"Chain-of-Thought Prompting Elicits Reasoning\"\n",
    "2. Kojima et al. (2022) - \"Large Language Models are Zero-Shot Reasoners\"\n",
    "3. Wang et al. (2022) - \"Self-Consistency Improves Chain of Thought\"\n",
    "4. Zhou et al. (2022) - \"Least-to-Most Prompting\"\n",
    "5. Yao et al. (2023) - \"Tree of Thoughts\"\n",
    "\n",
    "================================================================================\n",
    "PRODUCTION TIPS:\n",
    "================================================================================\n",
    "\u2022 Use temperature 0 for consistency, 0.7+ for creativity/diversity\n",
    "\u2022 Cache static prompt components\n",
    "\u2022 Monitor prompt performance in production\n",
    "\u2022 Version your prompts like code\n",
    "\u2022 Consider prompt length vs. quality trade-offs\n",
    "\u2022 Test with diverse inputs and edge cases\n",
    "\u2022 Implement fallbacks for API failures\n",
    "\u2022 Log prompts and outputs for debugging\n",
    "================================================================================\n",
    "\"\"\"\n",
    "    return cheat_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db34ba",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "MAIN EXECUTION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CHAIN-OF-THOUGHT PROMPTING - COMPREHENSIVE DEMONSTRATION\")\n",
    "    print(\"Educational Resource for LLM Interviews\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    # Run full demonstration\n",
    "    demonstrate_all_techniques()\n",
    "\n",
    "    # Print interview cheat sheet\n",
    "    print(\"\\n\\n\")\n",
    "    print(interview_cheat_sheet())\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DEMONSTRATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nThis file covers:\")\n",
    "    print(\"\u2713 Standard vs Chain-of-Thought prompting\")\n",
    "    print(\"\u2713 Zero-shot CoT ('Let's think step by step')\")\n",
    "    print(\"\u2713 Few-shot CoT with example builder\")\n",
    "    print(\"\u2713 Self-consistency with multiple reasoning paths\")\n",
    "    print(\"\u2713 Task-specific prompt templates\")\n",
    "    print(\"\u2713 Advanced techniques (PAL, ReAct, Tree of Thoughts)\")\n",
    "    print(\"\u2713 Best practices and common pitfalls\")\n",
    "    print(\"\u2713 Interview cheat sheet\")\n",
    "    print(\"\\nReview the code and output above for detailed explanations!\")\n",
    "    print(\"=\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}