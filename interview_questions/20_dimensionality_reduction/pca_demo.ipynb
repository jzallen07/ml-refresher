{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: PCA Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b787c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comprehensive PCA and Dimensionality Reduction Demo\n",
    "\n",
    "Interview Question Q28: \"How does PCA relate to feature extraction in machine learning?\"\n",
    "\n",
    "This demo covers:\n",
    "1. PCA from scratch implementation\n",
    "2. Visualizations of principal components\n",
    "3. Embedding dimensionality reduction\n",
    "4. Comparison with t-SNE and UMAP\n",
    "5. Practical denoising example\n",
    "\n",
    "Author: Educational Demo\n",
    "Date: 2025\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fde168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs, make_swiss_roll\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import UMAP\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "    print(\"\u2713 UMAP is available\")\n",
    "except ImportError:\n",
    "    UMAP_AVAILABLE = False\n",
    "    print(\"\u2717 UMAP not available (install with: pip install umap-learn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e378cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory for visualizations\n",
    "VIZ_DIR = \"/Users/zack/dev/ml-refresher/data/interview_viz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PCA AND DIMENSIONALITY REDUCTION DEMO\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f14b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 1: PCA FROM SCRATCH\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: PCA FROM SCRATCH IMPLEMENTATION\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96690ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW CONTEXT: What is PCA?\n",
    "---------------------------------\n",
    "PCA (Principal Component Analysis) is an unsupervised dimensionality reduction\n",
    "technique that:\n",
    "\n",
    "1. FINDS DIRECTIONS OF MAXIMUM VARIANCE\n",
    "   - These directions are called \"principal components\"\n",
    "   - First PC: direction of highest variance\n",
    "   - Second PC: direction of second highest variance (orthogonal to first)\n",
    "   - And so on...\n",
    "\n",
    "2. PROJECTS DATA onto these components\n",
    "   - Reduces dimensionality while preserving most information\n",
    "   - Removes correlations between features\n",
    "   - Can be used for:\n",
    "     * Visualization (reduce to 2D or 3D)\n",
    "     * Denoising (remove low-variance components)\n",
    "     * Feature extraction (use PCs as new features)\n",
    "     * Compression (keep only top k components)\n",
    "\n",
    "3. MATHEMATICAL FOUNDATION\n",
    "   - Linear transformation based on eigendecomposition\n",
    "   - Covariance matrix contains information about variance and correlations\n",
    "   - Eigenvectors = directions (principal components)\n",
    "   - Eigenvalues = magnitude of variance in each direction\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b6fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample 2D data for visualization\n",
    "np.random.seed(42)\n",
    "n_samples = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63389a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated 2D data\n",
    "mean = [0, 0]\n",
    "cov = [[3, 2.5],\n",
    "       [2.5, 3]]  # High correlation\n",
    "X_2d = np.random.multivariate_normal(mean, cov, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9cb6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOriginal Data Shape:\", X_2d.shape)\n",
    "print(\"First 5 samples:\")\n",
    "print(X_2d[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1964cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAFromScratch:\n",
    "    \"\"\"\n",
    "    PCA implementation from scratch to understand the mathematics.\n",
    "\n",
    "    Steps:\n",
    "    1. Center the data (subtract mean)\n",
    "    2. Compute covariance matrix\n",
    "    3. Perform eigendecomposition\n",
    "    4. Sort eigenvectors by eigenvalues (descending)\n",
    "    5. Select top k eigenvectors\n",
    "    6. Project data onto these eigenvectors\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components=2):\n",
    "        self.n_components = n_components\n",
    "        self.mean_ = None\n",
    "        self.components_ = None\n",
    "        self.explained_variance_ = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit PCA on data X\"\"\"\n",
    "        print(\"\\n--- PCA From Scratch: Step-by-step ---\")\n",
    "\n",
    "        # Step 1: Center the data\n",
    "        print(\"\\nStep 1: Center the data (subtract mean)\")\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "        print(f\"  Original mean: {self.mean_}\")\n",
    "        print(f\"  Centered mean: {np.mean(X_centered, axis=0)} (should be ~0)\")\n",
    "\n",
    "        # Step 2: Compute covariance matrix\n",
    "        print(\"\\nStep 2: Compute covariance matrix\")\n",
    "        # Cov = (X^T @ X) / (n - 1)\n",
    "        n_samples = X.shape[0]\n",
    "        cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)\n",
    "        print(f\"  Covariance matrix shape: {cov_matrix.shape}\")\n",
    "        print(f\"  Covariance matrix:\")\n",
    "        print(f\"  {cov_matrix}\")\n",
    "\n",
    "        # Step 3: Eigendecomposition\n",
    "        print(\"\\nStep 3: Eigendecomposition of covariance matrix\")\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "        print(f\"  Eigenvalues (variance in each direction): {eigenvalues}\")\n",
    "        print(f\"  Eigenvectors (principal component directions):\")\n",
    "        print(f\"  {eigenvectors}\")\n",
    "\n",
    "        # Step 4: Sort by eigenvalues (descending)\n",
    "        print(\"\\nStep 4: Sort eigenvectors by eigenvalues (descending)\")\n",
    "        idx = eigenvalues.argsort()[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        # Step 5: Select top k components\n",
    "        print(f\"\\nStep 5: Select top {self.n_components} components\")\n",
    "        self.components_ = eigenvectors[:, :self.n_components].T\n",
    "        self.explained_variance_ = eigenvalues[:self.n_components]\n",
    "\n",
    "        # Compute explained variance ratio\n",
    "        total_variance = np.sum(eigenvalues)\n",
    "        self.explained_variance_ratio_ = self.explained_variance_ / total_variance\n",
    "\n",
    "        print(f\"  Selected components shape: {self.components_.shape}\")\n",
    "        print(f\"  Explained variance: {self.explained_variance_}\")\n",
    "        print(f\"  Explained variance ratio: {self.explained_variance_ratio_}\")\n",
    "        print(f\"  Total explained: {np.sum(self.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Project data onto principal components\"\"\"\n",
    "        X_centered = X - self.mean_\n",
    "        # Project: X_projected = X_centered @ components^T\n",
    "        return X_centered @ self.components_.T\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def inverse_transform(self, X_transformed):\n",
    "        \"\"\"Reconstruct data from principal components\"\"\"\n",
    "        return X_transformed @ self.components_ + self.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our PCA implementation\n",
    "pca_scratch = PCAFromScratch(n_components=2)\n",
    "X_pca_scratch = pca_scratch.fit_transform(X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42288237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with sklearn's PCA\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"COMPARISON WITH SKLEARN'S PCA\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8427bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sklearn = PCA(n_components=2)\n",
    "X_pca_sklearn = pca_sklearn.fit_transform(X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a07b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOur implementation:\")\n",
    "print(f\"  Components:\\n{pca_scratch.components_}\")\n",
    "print(f\"  Explained variance ratio: {pca_scratch.explained_variance_ratio_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSklearn's implementation:\")\n",
    "print(f\"  Components:\\n{pca_sklearn.components_}\")\n",
    "print(f\"  Explained variance ratio: {pca_sklearn.explained_variance_ratio_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDifference in transformed data (should be close to 0):\")\n",
    "print(f\"  Max absolute difference: {np.max(np.abs(X_pca_scratch - X_pca_sklearn)):.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51db3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 2: VISUALIZATION OF PCA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: VISUALIZING PRINCIPAL COMPONENTS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a358f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed7c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Original data with principal components\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.5, c='steelblue', s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0873f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw principal components as arrows\n",
    "mean_point = pca_scratch.mean_\n",
    "for i, (component, variance) in enumerate(zip(pca_scratch.components_,\n",
    "                                               pca_scratch.explained_variance_)):\n",
    "    # Scale arrow by explained variance\n",
    "    arrow_scale = 3 * np.sqrt(variance)\n",
    "    ax.arrow(mean_point[0], mean_point[1],\n",
    "             component[0] * arrow_scale, component[1] * arrow_scale,\n",
    "             head_width=0.3, head_length=0.3, fc=f'C{i}', ec=f'C{i}',\n",
    "             linewidth=3, alpha=0.8,\n",
    "             label=f'PC{i+1} ({pca_scratch.explained_variance_ratio_[i]:.1%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f5c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(mean_point[0], mean_point[1], c='red', s=200, marker='X',\n",
    "           label='Mean', zorder=5, edgecolors='black', linewidths=2)\n",
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12)\n",
    "ax.set_title('Original Data with Principal Components', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Data projected onto PC1 only\n",
    "ax = axes[0, 1]\n",
    "pca_1d = PCA(n_components=1)\n",
    "X_pca_1d = pca_1d.fit_transform(X_2d)\n",
    "X_reconstructed_1d = pca_1d.inverse_transform(X_pca_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.3, c='lightgray', s=20, label='Original')\n",
    "ax.scatter(X_reconstructed_1d[:, 0], X_reconstructed_1d[:, 1],\n",
    "           alpha=0.6, c=X_pca_1d.ravel(), cmap='viridis', s=30,\n",
    "           label='Projected to PC1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e735cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw projection lines\n",
    "for i in range(0, n_samples, 10):  # Show every 10th point\n",
    "    ax.plot([X_2d[i, 0], X_reconstructed_1d[i, 0]],\n",
    "            [X_2d[i, 1], X_reconstructed_1d[i, 1]],\n",
    "            'k-', alpha=0.1, linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee24e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12)\n",
    "ax.set_title(f'Projection to 1D (PC1 explains {pca_1d.explained_variance_ratio_[0]:.1%})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Transformed data in PC space\n",
    "ax = axes[1, 0]\n",
    "scatter = ax.scatter(X_pca_scratch[:, 0], X_pca_scratch[:, 1],\n",
    "                    alpha=0.5, c='coral', s=30)\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.set_xlabel(f'PC1 ({pca_scratch.explained_variance_ratio_[0]:.1%} variance)',\n",
    "              fontsize=12)\n",
    "ax.set_ylabel(f'PC2 ({pca_scratch.explained_variance_ratio_[1]:.1%} variance)',\n",
    "              fontsize=12)\n",
    "ax.set_title('Data in Principal Component Space', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: 3D example\n",
    "ax = axes[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D data\n",
    "mean_3d = [0, 0, 0]\n",
    "cov_3d = [[3, 2.5, 1],\n",
    "          [2.5, 3, 1.5],\n",
    "          [1, 1.5, 1]]\n",
    "X_3d = np.random.multivariate_normal(mean_3d, cov_3d, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3d = PCA(n_components=3)\n",
    "pca_3d.fit(X_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa326d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show explained variance\n",
    "components = np.arange(1, 4)\n",
    "ax.bar(components - 0.2, pca_3d.explained_variance_ratio_, 0.4,\n",
    "       label='Individual', color='steelblue', alpha=0.8)\n",
    "ax.bar(components + 0.2, np.cumsum(pca_3d.explained_variance_ratio_), 0.4,\n",
    "       label='Cumulative', color='coral', alpha=0.8)\n",
    "ax.set_xlabel('Principal Component', fontsize=12)\n",
    "ax.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax.set_title('Explained Variance by Component (3D\u21923D)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(components)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(f'{VIZ_DIR}/14_pca_projection.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved visualization: {VIZ_DIR}/14_pca_projection.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3: EXPLAINED VARIANCE ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3: EXPLAINED VARIANCE ANALYSIS (SCREE PLOT)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52273006",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW CONTEXT: Choosing Number of Components\n",
    "-------------------------------------------------\n",
    "How many components should we keep?\n",
    "\n",
    "1. SCREE PLOT: Plot explained variance vs component number\n",
    "   - Look for \"elbow\" where curve flattens\n",
    "   - Shows diminishing returns\n",
    "\n",
    "2. CUMULATIVE VARIANCE: Keep components until reaching threshold\n",
    "   - Common thresholds: 80%, 90%, 95%, 99%\n",
    "   - Trade-off: information vs dimensionality\n",
    "\n",
    "3. DOMAIN KNOWLEDGE: Consider your use case\n",
    "   - Visualization: usually 2-3 components\n",
    "   - Feature extraction: depends on downstream task\n",
    "   - Denoising: remove low-variance (noisy) components\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4cf8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create high-dimensional data\n",
    "n_samples = 500\n",
    "n_features = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfbae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with decreasing variance structure\n",
    "X_high_dim = np.random.randn(n_samples, n_features)\n",
    "# Add structure: first few features have high variance\n",
    "for i in range(n_features):\n",
    "    variance_scale = np.exp(-i / 10)  # Exponential decay\n",
    "    X_high_dim[:, i] *= variance_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7549b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with all components\n",
    "pca_full = PCA(n_components=n_features)\n",
    "pca_full.fit(X_high_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nData shape: {X_high_dim.shape}\")\n",
    "print(f\"Number of components: {n_features}\")\n",
    "print(f\"\\nExplained variance ratio (first 10 components):\")\n",
    "for i in range(10):\n",
    "    print(f\"  PC{i+1}: {pca_full.explained_variance_ratio_[i]:.4f} \"\n",
    "          f\"(cumulative: {np.sum(pca_full.explained_variance_ratio_[:i+1]):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db81ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scree plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Scree plot (individual variance)\n",
    "ax = axes[0, 0]\n",
    "ax.plot(range(1, n_features + 1), pca_full.explained_variance_ratio_,\n",
    "        'bo-', linewidth=2, markersize=4)\n",
    "ax.set_xlabel('Principal Component', fontsize=12)\n",
    "ax.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax.set_title('Scree Plot: Individual Explained Variance', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, n_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight elbow region\n",
    "ax.axvspan(0, 10, alpha=0.1, color='green', label='High variance')\n",
    "ax.axvspan(10, 25, alpha=0.1, color='yellow', label='Medium variance')\n",
    "ax.axvspan(25, n_features, alpha=0.1, color='red', label='Low variance (noise)')\n",
    "ax.legend(fontsize=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Cumulative explained variance\n",
    "ax = axes[0, 1]\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "ax.plot(range(1, n_features + 1), cumulative_variance,\n",
    "        'ro-', linewidth=2, markersize=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add threshold lines\n",
    "thresholds = [0.80, 0.90, 0.95, 0.99]\n",
    "colors = ['green', 'blue', 'orange', 'red']\n",
    "for thresh, color in zip(thresholds, colors):\n",
    "    n_components_needed = np.argmax(cumulative_variance >= thresh) + 1\n",
    "    ax.axhline(y=thresh, color=color, linestyle='--', alpha=0.5,\n",
    "               label=f'{thresh:.0%}: {n_components_needed} components')\n",
    "    ax.plot(n_components_needed, thresh, 'o', color=color, markersize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xlabel('Number of Components', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "ax.set_title('Cumulative Explained Variance', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_xlim(0, n_features + 1)\n",
    "ax.set_ylim(0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ad17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Log scale (better for seeing tail)\n",
    "ax = axes[1, 0]\n",
    "ax.semilogy(range(1, n_features + 1), pca_full.explained_variance_ratio_,\n",
    "            'go-', linewidth=2, markersize=4)\n",
    "ax.set_xlabel('Principal Component', fontsize=12)\n",
    "ax.set_ylabel('Explained Variance Ratio (log scale)', fontsize=12)\n",
    "ax.set_title('Scree Plot (Log Scale) - Shows Tail Better', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "ax.set_xlim(0, n_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Comparison of dimensions\n",
    "ax = axes[1, 1]\n",
    "n_components_list = [5, 10, 15, 20, 30, 40, 50]\n",
    "variance_explained = [np.sum(pca_full.explained_variance_ratio_[:n])\n",
    "                      for n in n_components_list]\n",
    "compression_ratio = [n_features / n for n in n_components_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0637b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = ax.twinx()\n",
    "bars = ax.bar(range(len(n_components_list)), variance_explained,\n",
    "              alpha=0.7, color='steelblue', label='Variance Explained')\n",
    "line = ax2.plot(range(len(n_components_list)), compression_ratio,\n",
    "                'ro-', linewidth=2, markersize=8, label='Compression Ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75878911",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xlabel('Number of Components Kept', fontsize=12)\n",
    "ax.set_ylabel('Variance Explained', fontsize=12, color='steelblue')\n",
    "ax2.set_ylabel('Compression Ratio', fontsize=12, color='red')\n",
    "ax.set_title('Variance vs Compression Trade-off', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(len(n_components_list)))\n",
    "ax.set_xticklabels(n_components_list)\n",
    "ax.tick_params(axis='y', labelcolor='steelblue')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax.grid(True, alpha=0.3, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text annotations\n",
    "for i, (n, var, ratio) in enumerate(zip(n_components_list, variance_explained,\n",
    "                                         compression_ratio)):\n",
    "    ax.text(i, var + 0.02, f'{var:.1%}', ha='center', fontsize=8,\n",
    "            fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1844e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(f'{VIZ_DIR}/13_pca_explained_variance.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved visualization: {VIZ_DIR}/13_pca_explained_variance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e54fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 4: EMBEDDING VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 4: EMBEDDING DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b346047",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW CONTEXT: PCA for Embeddings\n",
    "--------------------------------------\n",
    "In NLP and ML, we often work with high-dimensional embeddings:\n",
    "- Word embeddings (Word2Vec, GloVe): 50-300 dimensions\n",
    "- Sentence embeddings (BERT, Sentence-BERT): 768-1024 dimensions\n",
    "- Image embeddings (ResNet, ViT): 512-2048 dimensions\n",
    "\n",
    "VISUALIZATION CHALLENGE:\n",
    "- Can't plot 768-dimensional space!\n",
    "- Need to reduce to 2D or 3D for visualization\n",
    "\n",
    "PCA FOR EMBEDDINGS:\n",
    "- Fast and scalable (linear transformation)\n",
    "- Preserves global structure (distances)\n",
    "- Good for overview of data distribution\n",
    "- BUT: May not capture non-linear patterns\n",
    "\n",
    "WHEN TO USE PCA vs t-SNE vs UMAP:\n",
    "- PCA: Quick exploration, large datasets, preserving distances\n",
    "- t-SNE: Finding clusters, local structure, final visualization\n",
    "- UMAP: Balance between PCA and t-SNE, faster than t-SNE\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366849ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate high-dimensional embeddings\n",
    "np.random.seed(42)\n",
    "n_samples = 600\n",
    "embedding_dim = 128  # Simulating smaller BERT-like embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic embeddings with cluster structure\n",
    "n_clusters = 5\n",
    "embeddings_list = []\n",
    "labels_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in range(n_clusters):\n",
    "    # Random center for each cluster\n",
    "    center = np.random.randn(embedding_dim) * 3\n",
    "    # Generate points around center\n",
    "    cluster_samples = n_samples // n_clusters\n",
    "    cluster_embeddings = center + np.random.randn(cluster_samples, embedding_dim) * 0.5\n",
    "\n",
    "    embeddings_list.append(cluster_embeddings)\n",
    "    labels_list.extend([cluster_id] * cluster_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.vstack(embeddings_list)\n",
    "labels = np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSimulated embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Samples per cluster: {n_samples // n_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize embeddings (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783875d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOriginal embedding statistics:\")\n",
    "print(f\"  Mean: {np.mean(embeddings, axis=0)[:5]}... (first 5 dims)\")\n",
    "print(f\"  Std: {np.std(embeddings, axis=0)[:5]}... (first 5 dims)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ec4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nScaled embedding statistics:\")\n",
    "print(f\"  Mean: {np.mean(embeddings_scaled, axis=0)[:5]}... (should be ~0)\")\n",
    "print(f\"  Std: {np.std(embeddings_scaled, axis=0)[:5]}... (should be ~1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "print(\"\\nApplying PCA to reduce from 128D to 2D...\")\n",
    "pca_embeddings = PCA(n_components=2, random_state=42)\n",
    "embeddings_pca = pca_embeddings.fit_transform(embeddings_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Explained variance ratio: {pca_embeddings.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {np.sum(pca_embeddings.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 5: COMPARISON WITH OTHER METHODS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 5: COMPARING DIMENSIONALITY REDUCTION METHODS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "METHOD COMPARISON:\n",
    "------------------\n",
    "1. PCA (Principal Component Analysis)\n",
    "   - Type: Linear, global structure preservation\n",
    "   - Speed: Very fast (O(min(n*d^2, d*n^2)))\n",
    "   - Best for: Quick exploration, large datasets, linear patterns\n",
    "   - Deterministic: Yes (same result every time)\n",
    "\n",
    "2. t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "   - Type: Non-linear, local structure preservation\n",
    "   - Speed: Slow (O(n^2))\n",
    "   - Best for: Finding clusters, final visualizations\n",
    "   - Deterministic: No (random initialization)\n",
    "   - Note: Distances between clusters not meaningful\n",
    "\n",
    "3. UMAP (Uniform Manifold Approximation and Projection)\n",
    "   - Type: Non-linear, balances local and global structure\n",
    "   - Speed: Fast (O(n log n))\n",
    "   - Best for: Large datasets, preserving both local and global structure\n",
    "   - Deterministic: No (but more stable than t-SNE)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE\n",
    "print(\"\\nApplying t-SNE (this may take a moment)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings_scaled)\n",
    "print(\"\u2713 t-SNE completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP if available\n",
    "if UMAP_AVAILABLE:\n",
    "    print(\"\\nApplying UMAP...\")\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15)\n",
    "    embeddings_umap = reducer.fit_transform(embeddings_scaled)\n",
    "    print(\"\u2713 UMAP completed\")\n",
    "else:\n",
    "    embeddings_umap = None\n",
    "    print(\"\\n\u2717 UMAP not available - skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot\n",
    "n_methods = 3 if UMAP_AVAILABLE else 2\n",
    "fig, axes = plt.subplots(1, n_methods, figsize=(6 * n_methods, 5))\n",
    "if n_methods == 2:\n",
    "    axes = [axes[0], axes[1], None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c8cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(embeddings_pca[:, 0], embeddings_pca[:, 1],\n",
    "                    c=labels, cmap='tab10', s=30, alpha=0.7)\n",
    "ax.set_xlabel(f'PC1 ({pca_embeddings.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "ax.set_ylabel(f'PC2 ({pca_embeddings.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "ax.set_title('PCA\\n(Linear, Global Structure)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4436c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE\n",
    "ax = axes[1]\n",
    "scatter = ax.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1],\n",
    "                    c=labels, cmap='tab10', s=30, alpha=0.7)\n",
    "ax.set_xlabel('t-SNE 1', fontsize=11)\n",
    "ax.set_ylabel('t-SNE 2', fontsize=11)\n",
    "ax.set_title('t-SNE\\n(Non-linear, Local Structure)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e997e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot UMAP if available\n",
    "if UMAP_AVAILABLE and axes[2] is not None:\n",
    "    ax = axes[2]\n",
    "    scatter = ax.scatter(embeddings_umap[:, 0], embeddings_umap[:, 1],\n",
    "                        c=labels, cmap='tab10', s=30, alpha=0.7)\n",
    "    ax.set_xlabel('UMAP 1', fontsize=11)\n",
    "    ax.set_ylabel('UMAP 2', fontsize=11)\n",
    "    ax.set_title('UMAP\\n(Non-linear, Balanced)', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add colorbar\n",
    "if n_methods == 3:\n",
    "    plt.colorbar(scatter, ax=axes, label='Cluster ID', fraction=0.02, pad=0.04)\n",
    "else:\n",
    "    plt.colorbar(scatter, ax=axes[:2], label='Cluster ID', fraction=0.02, pad=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(f'{VIZ_DIR}/15_dimensionality_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved visualization: {VIZ_DIR}/15_dimensionality_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 6: PRACTICAL EXAMPLE - DENOISING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 6: PRACTICAL APPLICATION - DENOISING WITH PCA\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7391d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW CONTEXT: PCA for Denoising\n",
    "-------------------------------------\n",
    "PCA can be used for denoising by:\n",
    "\n",
    "1. ASSUMPTION: Signal has high variance, noise has low variance\n",
    "   - First few PCs capture signal (high variance)\n",
    "   - Last PCs capture noise (low variance)\n",
    "\n",
    "2. PROCESS:\n",
    "   - Apply PCA to noisy data\n",
    "   - Keep only top k components (signal)\n",
    "   - Discard remaining components (noise)\n",
    "   - Reconstruct data from top k components\n",
    "\n",
    "3. APPLICATIONS:\n",
    "   - Image denoising\n",
    "   - Signal processing\n",
    "   - Feature preprocessing\n",
    "   - Data compression\n",
    "\n",
    "4. TRADE-OFF:\n",
    "   - More components = less noise removal but more detail preserved\n",
    "   - Fewer components = more noise removal but may lose signal\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c3d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean signal (2D spiral)\n",
    "n_points = 300\n",
    "t = np.linspace(0, 4 * np.pi, n_points)\n",
    "clean_signal = np.column_stack([\n",
    "    t * np.cos(t),\n",
    "    t * np.sin(t)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e833db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise\n",
    "noise_level = 0.5\n",
    "noise = np.random.randn(*clean_signal.shape) * noise_level\n",
    "noisy_signal = clean_signal + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f445df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSignal shape: {clean_signal.shape}\")\n",
    "print(f\"Noise level: {noise_level}\")\n",
    "print(f\"Signal-to-Noise Ratio (SNR): {10 * np.log10(np.var(clean_signal) / np.var(noise)):.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98617992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with different numbers of components\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clean signal\n",
    "ax = axes[0, 0]\n",
    "ax.plot(clean_signal[:, 0], clean_signal[:, 1], 'b-', linewidth=2, alpha=0.7)\n",
    "ax.scatter(clean_signal[:, 0], clean_signal[:, 1], c=range(n_points),\n",
    "          cmap='viridis', s=20, alpha=0.6)\n",
    "ax.set_xlabel('X', fontsize=11)\n",
    "ax.set_ylabel('Y', fontsize=11)\n",
    "ax.set_title('Clean Signal (Ground Truth)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac40de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot noisy signal\n",
    "ax = axes[0, 1]\n",
    "ax.plot(noisy_signal[:, 0], noisy_signal[:, 1], 'r-', linewidth=1, alpha=0.3)\n",
    "ax.scatter(noisy_signal[:, 0], noisy_signal[:, 1], c=range(n_points),\n",
    "          cmap='viridis', s=20, alpha=0.6)\n",
    "ax.set_xlabel('X', fontsize=11)\n",
    "ax.set_ylabel('Y', fontsize=11)\n",
    "ax.set_title('Noisy Signal', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoise with PCA (1 component)\n",
    "ax = axes[0, 2]\n",
    "pca_denoise_1 = PCA(n_components=1)\n",
    "signal_transformed_1 = pca_denoise_1.fit_transform(noisy_signal)\n",
    "signal_denoised_1 = pca_denoise_1.inverse_transform(signal_transformed_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(signal_denoised_1[:, 0], signal_denoised_1[:, 1], 'g-', linewidth=2, alpha=0.7)\n",
    "ax.scatter(signal_denoised_1[:, 0], signal_denoised_1[:, 1], c=range(n_points),\n",
    "          cmap='viridis', s=20, alpha=0.6)\n",
    "mse_1 = np.mean((clean_signal - signal_denoised_1) ** 2)\n",
    "ax.set_xlabel('X', fontsize=11)\n",
    "ax.set_ylabel('Y', fontsize=11)\n",
    "ax.set_title(f'Denoised (1 PC)\\nMSE: {mse_1:.3f}', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00771c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with original (overlay)\n",
    "ax = axes[1, 0]\n",
    "ax.plot(clean_signal[:, 0], clean_signal[:, 1], 'b-', linewidth=2,\n",
    "       alpha=0.5, label='Clean')\n",
    "ax.plot(noisy_signal[:, 0], noisy_signal[:, 1], 'r-', linewidth=1,\n",
    "       alpha=0.3, label='Noisy')\n",
    "ax.plot(signal_denoised_1[:, 0], signal_denoised_1[:, 1], 'g-', linewidth=2,\n",
    "       alpha=0.7, label='Denoised (1 PC)')\n",
    "ax.set_xlabel('X', fontsize=11)\n",
    "ax.set_ylabel('Y', fontsize=11)\n",
    "ax.set_title('Overlay Comparison', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d471b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "ax = axes[1, 1]\n",
    "error_noisy = np.linalg.norm(clean_signal - noisy_signal, axis=1)\n",
    "error_denoised = np.linalg.norm(clean_signal - signal_denoised_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22081384",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(range(n_points), error_noisy, 'r-', linewidth=1.5, alpha=0.7,\n",
    "       label=f'Noisy (mean: {np.mean(error_noisy):.3f})')\n",
    "ax.plot(range(n_points), error_denoised, 'g-', linewidth=1.5, alpha=0.7,\n",
    "       label=f'Denoised (mean: {np.mean(error_denoised):.3f})')\n",
    "ax.set_xlabel('Point Index', fontsize=11)\n",
    "ax.set_ylabel('Error (L2 distance)', fontsize=11)\n",
    "ax.set_title('Point-wise Reconstruction Error', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2019bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance\n",
    "ax = axes[1, 2]\n",
    "pca_full_denoise = PCA(n_components=2)\n",
    "pca_full_denoise.fit(noisy_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [1, 2]\n",
    "explained_var = pca_full_denoise.explained_variance_ratio_\n",
    "ax.bar(components, explained_var, color=['steelblue', 'lightcoral'], alpha=0.8)\n",
    "ax.set_xlabel('Principal Component', fontsize=11)\n",
    "ax.set_ylabel('Explained Variance Ratio', fontsize=11)\n",
    "ax.set_title('Variance in Noisy Signal', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(components)\n",
    "ax.set_xticklabels(['PC1\\n(Signal)', 'PC2\\n(Noise)'])\n",
    "ax.grid(True, alpha=0.3, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ace1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text annotations\n",
    "for i, var in enumerate(explained_var):\n",
    "    ax.text(components[i], var + 0.02, f'{var:.1%}',\n",
    "           ha='center', fontsize=10, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2218d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(f'{VIZ_DIR}/16_pca_denoising.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved visualization: {VIZ_DIR}/16_pca_denoising.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba08b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative comparison\n",
    "print(\"\\nDenoising Results:\")\n",
    "print(f\"  Noisy signal MSE: {np.mean((clean_signal - noisy_signal) ** 2):.4f}\")\n",
    "print(f\"  Denoised signal MSE: {mse_1:.4f}\")\n",
    "improvement = (1 - mse_1 / np.mean((clean_signal - noisy_signal) ** 2)) * 100\n",
    "print(f\"  Improvement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 7: ADVANCED CONCEPTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 7: ADVANCED PCA CONCEPTS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "ADVANCED TOPICS FOR INTERVIEWS:\n",
    "--------------------------------\n",
    "\n",
    "1. KERNEL PCA\n",
    "   - Extends PCA to non-linear relationships\n",
    "   - Uses kernel trick (similar to SVM)\n",
    "   - Can capture complex patterns\n",
    "   - More computationally expensive\n",
    "\n",
    "2. INCREMENTAL PCA\n",
    "   - Processes data in mini-batches\n",
    "   - Useful for large datasets that don't fit in memory\n",
    "   - sklearn: IncrementalPCA\n",
    "\n",
    "3. SPARSE PCA\n",
    "   - Produces sparse principal components\n",
    "   - Easier to interpret (many zeros)\n",
    "   - Trade-off: sparsity vs variance explained\n",
    "\n",
    "4. PROBABILISTIC PCA\n",
    "   - Probabilistic interpretation of PCA\n",
    "   - Can handle missing data\n",
    "   - Provides uncertainty estimates\n",
    "\n",
    "5. PCA vs AUTOENCODERS\n",
    "   - PCA: Linear compression (1 layer)\n",
    "   - Autoencoder: Non-linear compression (deep network)\n",
    "   - Autoencoder more flexible but requires more data/computation\n",
    "\n",
    "6. WHITENING\n",
    "   - PCA + scaling to unit variance\n",
    "   - Makes components uncorrelated AND unit variance\n",
    "   - Useful preprocessing for neural networks\n",
    "\n",
    "7. RELATIONSHIP TO SVD\n",
    "   - PCA via eigendecomposition of covariance matrix\n",
    "   - Can also compute via SVD of data matrix\n",
    "   - SVD more numerically stable\n",
    "   - sklearn uses randomized SVD for efficiency\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c34254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate SVD relationship\n",
    "print(\"\\nDemonstrating PCA-SVD Relationship:\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad738ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_demo = np.random.randn(100, 10)\n",
    "X_centered = X_demo - np.mean(X_demo, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b310c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: PCA via eigendecomposition (what we did earlier)\n",
    "cov_matrix = (X_centered.T @ X_centered) / (X_centered.shape[0] - 1)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "pca_components_eigen = eigenvectors[:, idx].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: PCA via SVD\n",
    "U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "pca_components_svd = Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f78443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: sklearn (uses randomized SVD)\n",
    "pca_sklearn_demo = PCA(n_components=10)\n",
    "pca_sklearn_demo.fit(X_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Components from eigendecomposition (first 3, first 5 values):\")\n",
    "print(pca_components_eigen[:3, :5])\n",
    "print(f\"\\nComponents from SVD (first 3, first 5 values):\")\n",
    "print(pca_components_svd[:3, :5])\n",
    "print(f\"\\nComponents from sklearn (first 3, first 5 values):\")\n",
    "print(pca_sklearn_demo.components_[:3, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7504fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nAll methods produce same components (up to sign):\")\n",
    "print(f\"Max difference (eigen vs SVD): {np.max(np.abs(np.abs(pca_components_eigen) - np.abs(pca_components_svd))):.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ddfe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY AND INTERVIEW TIPS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY: KEY INTERVIEW POINTS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "ESSENTIAL CONCEPTS TO REMEMBER:\n",
    "--------------------------------\n",
    "\n",
    "1. WHAT IS PCA?\n",
    "   \u2713 Unsupervised dimensionality reduction\n",
    "   \u2713 Finds directions of maximum variance\n",
    "   \u2713 Linear transformation (orthogonal projection)\n",
    "   \u2713 Based on eigendecomposition of covariance matrix\n",
    "\n",
    "2. WHY USE PCA?\n",
    "   \u2713 Dimensionality reduction (compress data)\n",
    "   \u2713 Visualization (reduce to 2D/3D)\n",
    "   \u2713 Feature extraction (new features are PCs)\n",
    "   \u2713 Denoising (remove low-variance components)\n",
    "   \u2713 Remove multicollinearity (decorrelate features)\n",
    "\n",
    "3. HOW DOES IT WORK?\n",
    "   \u2713 Center data (subtract mean)\n",
    "   \u2713 Compute covariance matrix\n",
    "   \u2713 Find eigenvectors/eigenvalues\n",
    "   \u2713 Sort by eigenvalues (descending)\n",
    "   \u2713 Project data onto top k eigenvectors\n",
    "\n",
    "4. CHOOSING NUMBER OF COMPONENTS\n",
    "   \u2713 Scree plot (look for elbow)\n",
    "   \u2713 Cumulative variance threshold (e.g., 95%)\n",
    "   \u2713 Domain knowledge/task requirements\n",
    "   \u2713 Cross-validation for downstream tasks\n",
    "\n",
    "5. LIMITATIONS\n",
    "   \u2713 Only captures linear relationships\n",
    "   \u2713 Assumes high variance = important\n",
    "   \u2713 Sensitive to scaling (always standardize!)\n",
    "   \u2713 Interpretability: PCs are combinations of original features\n",
    "\n",
    "6. ALTERNATIVES\n",
    "   \u2713 t-SNE: Better for visualization, captures non-linear patterns\n",
    "   \u2713 UMAP: Faster than t-SNE, preserves global structure\n",
    "   \u2713 Autoencoders: Non-linear, learnable compression\n",
    "   \u2713 Feature selection: Keep subset of original features\n",
    "\n",
    "7. PRACTICAL TIPS\n",
    "   \u2713 Always standardize features before PCA\n",
    "   \u2713 Check explained variance ratio\n",
    "   \u2713 Use PCA for exploration, not always for final model\n",
    "   \u2713 Consider computational cost vs accuracy trade-off\n",
    "\n",
    "8. COMMON INTERVIEW QUESTIONS\n",
    "   Q: \"What's the difference between PCA and LDA?\"\n",
    "   A: PCA is unsupervised (maximizes variance), LDA is supervised\n",
    "      (maximizes class separation)\n",
    "\n",
    "   Q: \"Why do we center the data in PCA?\"\n",
    "   A: PCA finds directions of maximum variance from origin. Centering\n",
    "      ensures we measure variance around the data's actual mean.\n",
    "\n",
    "   Q: \"Can PCA handle missing values?\"\n",
    "   A: Standard PCA cannot. Need to impute first, or use probabilistic\n",
    "      PCA / matrix completion methods.\n",
    "\n",
    "   Q: \"Is PCA sensitive to outliers?\"\n",
    "   A: Yes! Outliers can dominate variance. Consider robust PCA or\n",
    "      outlier removal.\n",
    "\n",
    "9. CODE INTERVIEW TIPS\n",
    "   \u2713 Know how to implement PCA from scratch (covariance + eigen)\n",
    "   \u2713 Understand relationship between PCA and SVD\n",
    "   \u2713 Can explain each step mathematically\n",
    "   \u2713 Know sklearn API: fit(), transform(), fit_transform()\n",
    "   \u2713 Can interpret explained_variance_ratio_\n",
    "\n",
    "10. MATHEMATICAL CONCEPTS\n",
    "    \u2713 Covariance matrix: measures feature correlations\n",
    "    \u2713 Eigenvectors: directions of principal components\n",
    "    \u2713 Eigenvalues: variance along each component\n",
    "    \u2713 Orthogonality: PCs are uncorrelated\n",
    "    \u2713 Linear algebra: PCA is matrix factorization\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1aa0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEMO COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nGenerated visualizations:\")\n",
    "print(f\"  1. {VIZ_DIR}/13_pca_explained_variance.png\")\n",
    "print(f\"  2. {VIZ_DIR}/14_pca_projection.png\")\n",
    "print(f\"  3. {VIZ_DIR}/15_dimensionality_comparison.png\")\n",
    "print(f\"  4. {VIZ_DIR}/16_pca_denoising.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNext steps for learning:\")\n",
    "print(\"  \u2022 Implement PCA from scratch on different datasets\")\n",
    "print(\"  \u2022 Try kernel PCA for non-linear patterns\")\n",
    "print(\"  \u2022 Compare PCA with autoencoders\")\n",
    "print(\"  \u2022 Apply PCA in a real project\")\n",
    "print(\"  \u2022 Study SVD and its relationship to PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a278dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}