{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Training Objectives Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training Objectives for LLM Interview Preparation\n",
    "=================================================\n",
    "\n",
    "This demo covers key training objectives used in modern LLMs:\n",
    "1. Masked Language Modeling (MLM) - BERT-style\n",
    "2. Causal Language Modeling (CLM) - GPT-style\n",
    "3. Next Sentence Prediction (NSP)\n",
    "\n",
    "Key Interview Questions:\n",
    "- Q7: How does masked language modeling work?\n",
    "- Q9: What's the difference between MLM and causal LM?\n",
    "- Q11: Why do we use different masking strategies?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af983fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for visualizations\n",
    "output_dir = Path(\"/Users/zack/dev/ml-refresher/data/interview_viz\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LLM TRAINING OBJECTIVES DEMO\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3515239d",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 1: VOCABULARY AND TOKENIZATION SETUP\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e470a8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    Simple tokenizer for demonstration purposes.\n",
    "\n",
    "    Interview Tip: Real LLMs use subword tokenizers (BPE, WordPiece, SentencePiece)\n",
    "    but this simplified version helps understand the core concepts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Special tokens used in LLM training\n",
    "        self.special_tokens = {\n",
    "            '[PAD]': 0,   # Padding token\n",
    "            '[UNK]': 1,   # Unknown token\n",
    "            '[CLS]': 2,   # Classification token (BERT)\n",
    "            '[SEP]': 3,   # Separator token\n",
    "            '[MASK]': 4,  # Mask token for MLM\n",
    "        }\n",
    "\n",
    "        # Simple vocabulary (in practice, this would be 30k-50k tokens)\n",
    "        self.vocab = {\n",
    "            **self.special_tokens,\n",
    "            'the': 5, 'a': 6, 'is': 7, 'was': 8, 'are': 9,\n",
    "            'cat': 10, 'dog': 11, 'sat': 12, 'mat': 13, 'on': 14,\n",
    "            'quick': 15, 'brown': 16, 'fox': 17, 'jumps': 18, 'over': 19,\n",
    "            'lazy': 20, 'language': 21, 'model': 22, 'learning': 23,\n",
    "            'machine': 24, 'deep': 25, 'neural': 26, 'network': 27,\n",
    "            'transformer': 28, 'attention': 29, 'bert': 30, 'gpt': 31,\n",
    "        }\n",
    "\n",
    "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text to token IDs\"\"\"\n",
    "        tokens = text.lower().split()\n",
    "        return [self.vocab.get(token, self.vocab['[UNK]']) for token in tokens]\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Convert token IDs back to text\"\"\"\n",
    "        return ' '.join([self.id_to_token.get(id, '[UNK]') for id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5125069",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921ceda",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOKENIZER SETUP\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {list(tokenizer.special_tokens.keys())}\")\n",
    "print(f\"\\nExample encoding:\")\n",
    "example_text = \"the cat sat on the mat\"\n",
    "encoded = tokenizer.encode(example_text)\n",
    "print(f\"Text: '{example_text}'\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: '{tokenizer.decode(encoded)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa838c69",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 2: MASKED LANGUAGE MODELING (MLM) - BERT STYLE\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85411b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MASKED LANGUAGE MODELING (MLM)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "MLM is the training objective used by BERT and similar bidirectional models.\n",
    "\n",
    "KEY CONCEPTS:\n",
    "1. Random tokens are masked in the input\n",
    "2. Model must predict the original token\n",
    "3. Uses bidirectional context (can see both left and right)\n",
    "4. Masking strategy (BERT paper):\n",
    "   - 80% of time: Replace with [MASK]\n",
    "   - 10% of time: Replace with random token\n",
    "   - 10% of time: Keep unchanged\n",
    "\n",
    "WHY THIS STRATEGY?\n",
    "- 80% [MASK]: Main training signal\n",
    "- 10% random: Prevents model from relying on [MASK] token\n",
    "- 10% unchanged: Encourages model to learn representations for all tokens\n",
    "\n",
    "INTERVIEW TIP: Be able to explain why we don't mask 100% with [MASK]!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataProcessor:\n",
    "    \"\"\"\n",
    "    Processes data for Masked Language Modeling.\n",
    "\n",
    "    This is a critical component for BERT-style models.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: SimpleTokenizer, mask_prob: float = 0.15):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokenizer: Tokenizer instance\n",
    "            mask_prob: Probability of masking each token (BERT uses 15%)\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token_id = tokenizer.vocab['[MASK]']\n",
    "        self.pad_token_id = tokenizer.vocab['[PAD]']\n",
    "        self.special_token_ids = set(tokenizer.special_tokens.values())\n",
    "\n",
    "    def create_mlm_batch(\n",
    "        self,\n",
    "        text: str,\n",
    "        verbose: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Create a batch for MLM training.\n",
    "\n",
    "        Returns:\n",
    "            input_ids: Token IDs with masking applied\n",
    "            labels: Original token IDs (only for masked positions)\n",
    "            attention_mask: 1 for real tokens, 0 for padding\n",
    "        \"\"\"\n",
    "        # Encode text\n",
    "        tokens = tokenizer.encode(text)\n",
    "        original_tokens = tokens.copy()\n",
    "\n",
    "        # Create labels (-100 is ignored by PyTorch loss functions)\n",
    "        labels = [-100] * len(tokens)\n",
    "\n",
    "        # Apply masking\n",
    "        masked_positions = []\n",
    "        for i, token_id in enumerate(tokens):\n",
    "            # Don't mask special tokens\n",
    "            if token_id in self.special_token_ids:\n",
    "                continue\n",
    "\n",
    "            # Randomly decide if this token should be masked\n",
    "            if random.random() < self.mask_prob:\n",
    "                masked_positions.append(i)\n",
    "                labels[i] = token_id  # Store original token for loss calculation\n",
    "\n",
    "                prob = random.random()\n",
    "                if prob < 0.8:\n",
    "                    # 80% of time: replace with [MASK]\n",
    "                    tokens[i] = self.mask_token_id\n",
    "                elif prob < 0.9:\n",
    "                    # 10% of time: replace with random token\n",
    "                    tokens[i] = random.randint(5, self.tokenizer.vocab_size - 1)\n",
    "                # else: 10% of time: keep unchanged (do nothing)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nOriginal text: '{text}'\")\n",
    "            print(f\"Original tokens: {original_tokens}\")\n",
    "            print(f\"Original decoded: '{tokenizer.decode(original_tokens)}'\")\n",
    "            print(f\"\\nMasked positions: {masked_positions} ({len(masked_positions)}/{len(tokens)} = {len(masked_positions)/len(tokens)*100:.1f}%)\")\n",
    "            print(f\"Input tokens: {tokens}\")\n",
    "            print(f\"Input decoded: '{tokenizer.decode(tokens)}'\")\n",
    "            print(f\"\\nLabels (only masked positions have values != -100):\")\n",
    "            for i, (label, orig) in enumerate(zip(labels, original_tokens)):\n",
    "                if label != -100:\n",
    "                    print(f\"  Position {i}: Predict '{tokenizer.id_to_token[label]}' (ID: {label})\")\n",
    "\n",
    "        # Convert to tensors\n",
    "        input_ids = torch.tensor([tokens])\n",
    "        labels_tensor = torch.tensor([labels])\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        return input_ids, labels_tensor, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate MLM with examples\n",
    "mlm_processor = MLMDataProcessor(tokenizer, mask_prob=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"EXAMPLE 1: Simple sentence\")\n",
    "print(\"-\" * 80)\n",
    "example1 = \"the quick brown fox jumps over the lazy dog\"\n",
    "input_ids1, labels1, mask1 = mlm_processor.create_mlm_batch(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"EXAMPLE 2: Technical sentence\")\n",
    "print(\"-\" * 80)\n",
    "example2 = \"the transformer model uses attention mechanism\"\n",
    "input_ids2, labels2, mask2 = mlm_processor.create_mlm_batch(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBERTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified BERT model for demonstration.\n",
    "\n",
    "    In a real BERT model:\n",
    "    - 12-24 transformer layers\n",
    "    - 768-1024 hidden dimensions\n",
    "    - Multi-head attention\n",
    "    - Layer normalization, residual connections, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=4,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # MLM head: projects hidden states back to vocabulary\n",
    "        self.mlm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Forward pass for MLM.\n",
    "\n",
    "        Returns logits for each position in vocabulary space.\n",
    "        \"\"\"\n",
    "        # Embed tokens\n",
    "        embeddings = self.embeddings(input_ids)\n",
    "\n",
    "        # Apply transformer (bidirectional - can see entire context)\n",
    "        hidden_states = self.transformer(embeddings)\n",
    "\n",
    "        # Project to vocabulary for prediction\n",
    "        logits = self.mlm_head(hidden_states)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278049f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and compute loss\n",
    "bert_model = SimpleBERTModel(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3155b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MLM LOSS COMPUTATION\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78484a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "logits = bert_model(input_ids1)\n",
    "print(f\"Input shape: {input_ids1.shape}\")\n",
    "print(f\"Logits shape: {logits.shape} (batch_size, seq_len, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss (only on masked positions due to labels=-100)\n",
    "loss_fn = nn.CrossEntropyLoss()  # Ignores -100 labels by default\n",
    "loss = loss_fn(logits.view(-1, tokenizer.vocab_size), labels1.view(-1))\n",
    "print(f\"\\nMLM Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad2cfd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "LOSS COMPUTATION DETAILS:\n",
    "- Loss is computed ONLY on masked positions\n",
    "- Non-masked positions have label = -100 (ignored)\n",
    "- This is key: model learns to predict masked tokens using bidirectional context\n",
    "- At inference: no masking, model outputs representations for downstream tasks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64726369",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 3: CAUSAL LANGUAGE MODELING (CLM) - GPT STYLE\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4559ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CAUSAL LANGUAGE MODELING (CLM)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "CLM is the training objective used by GPT and similar autoregressive models.\n",
    "\n",
    "KEY CONCEPTS:\n",
    "1. Predict the next token given previous tokens\n",
    "2. Uses causal (autoregressive) attention - can only see left context\n",
    "3. Each position predicts the next token\n",
    "4. No masking needed in input (masking in attention mechanism)\n",
    "\n",
    "DIFFERENCES FROM MLM:\n",
    "- MLM: Bidirectional, mask some tokens, predict masked ones\n",
    "- CLM: Unidirectional, no masking in input, predict next token at each position\n",
    "\n",
    "INTERVIEW TIP: CLM is simpler but very effective! Powers GPT-3, GPT-4, etc.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create causal attention mask.\n",
    "\n",
    "    Position i can only attend to positions <= i.\n",
    "    This prevents the model from \"cheating\" by looking at future tokens.\n",
    "\n",
    "    Returns:\n",
    "        mask: Shape (seq_len, seq_len), True where attention is allowed\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize causal mask\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0062766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CAUSAL ATTENTION MASK VISUALIZATION\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\"\"\n",
    "In causal attention:\n",
    "- Each position can attend to itself and previous positions\n",
    "- Cannot attend to future positions (upper triangle is masked)\n",
    "- This enforces left-to-right, autoregressive generation\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    causal_mask.numpy(),\n",
    "    cmap='RdYlGn',\n",
    "    cbar_kws={'label': 'Can Attend'},\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    xticklabels=[f'Pos {i}' for i in range(seq_len)],\n",
    "    yticklabels=[f'Pos {i}' for i in range(seq_len)]\n",
    ")\n",
    "plt.title('Causal Attention Mask\\n(1 = can attend, 0 = masked)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Key Position (attending TO)', fontsize=12)\n",
    "plt.ylabel('Query Position (attending FROM)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'causal_attention_mask.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\u2713 Saved: {output_dir / 'causal_attention_mask.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ec286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLMDataProcessor:\n",
    "    \"\"\"\n",
    "    Processes data for Causal Language Modeling.\n",
    "\n",
    "    Much simpler than MLM - no masking needed in input!\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: SimpleTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def create_clm_batch(\n",
    "        self,\n",
    "        text: str,\n",
    "        verbose: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Create a batch for CLM training.\n",
    "\n",
    "        Input:  [token_1, token_2, token_3, token_4]\n",
    "        Labels: [token_2, token_3, token_4, token_5]\n",
    "\n",
    "        Each position predicts the next token.\n",
    "        \"\"\"\n",
    "        # Encode text\n",
    "        tokens = tokenizer.encode(text)\n",
    "\n",
    "        # Input: all tokens except last\n",
    "        input_ids = tokens[:-1]\n",
    "\n",
    "        # Labels: all tokens except first (shifted by 1)\n",
    "        labels = tokens[1:]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nOriginal text: '{text}'\")\n",
    "            print(f\"All tokens: {tokens}\")\n",
    "            print(f\"Decoded: '{tokenizer.decode(tokens)}'\")\n",
    "            print(f\"\\nCLM Training Setup:\")\n",
    "            print(f\"Input IDs:  {input_ids}\")\n",
    "            print(f\"Labels:     {labels}\")\n",
    "            print(f\"\\nPrediction targets at each position:\")\n",
    "            for i, (inp, label) in enumerate(zip(input_ids, labels)):\n",
    "                inp_word = tokenizer.id_to_token[inp]\n",
    "                label_word = tokenizer.id_to_token[label]\n",
    "                print(f\"  Position {i}: Given '{inp_word}' (and all previous), predict '{label_word}'\")\n",
    "\n",
    "        return torch.tensor([input_ids]), torch.tensor([labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate CLM\n",
    "clm_processor = CLMDataProcessor(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc75aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CLM EXAMPLE 1\")\n",
    "print(\"-\" * 80)\n",
    "input_ids_clm1, labels_clm1 = clm_processor.create_clm_batch(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CLM EXAMPLE 2\")\n",
    "print(\"-\" * 80)\n",
    "input_ids_clm2, labels_clm2 = clm_processor.create_clm_batch(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified GPT model for demonstration.\n",
    "\n",
    "    Key difference from BERT: Uses causal (masked) attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_dim)\n",
    "\n",
    "        # Causal transformer decoder layer\n",
    "        self.transformer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=4,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # LM head: projects hidden states to vocabulary\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Forward pass for CLM.\n",
    "\n",
    "        Uses causal attention mask to prevent seeing future tokens.\n",
    "        \"\"\"\n",
    "        seq_len = input_ids.size(1)\n",
    "\n",
    "        # Embed tokens\n",
    "        embeddings = self.embeddings(input_ids)\n",
    "\n",
    "        # Create causal mask\n",
    "        causal_mask = ~create_causal_mask(seq_len)  # PyTorch uses True for masked positions\n",
    "\n",
    "        # Apply transformer with causal attention\n",
    "        hidden_states = self.transformer(\n",
    "            embeddings,\n",
    "            embeddings,\n",
    "            tgt_mask=causal_mask\n",
    "        )\n",
    "\n",
    "        # Project to vocabulary\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GPT model and compute loss\n",
    "gpt_model = SimpleGPTModel(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477041b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CLM LOSS COMPUTATION\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "logits_clm = gpt_model(input_ids_clm1)\n",
    "print(f\"Input shape: {input_ids_clm1.shape}\")\n",
    "print(f\"Logits shape: {logits_clm.shape} (batch_size, seq_len, vocab_size)\")\n",
    "print(f\"Labels shape: {labels_clm1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a568fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss (on ALL positions - each predicts next token)\n",
    "loss_clm = loss_fn(logits_clm.view(-1, tokenizer.vocab_size), labels_clm1.view(-1))\n",
    "print(f\"\\nCLM Loss: {loss_clm.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2252308",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "LOSS COMPUTATION DETAILS:\n",
    "- Loss is computed on ALL positions (unlike MLM)\n",
    "- Each position predicts the next token\n",
    "- Model learns from every token in the sequence\n",
    "- At inference: generate token by token, feeding output back as input\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f469aaa",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 4: SIDE-BY-SIDE COMPARISON - MLM VS CLM\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50843be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MLM VS CLM: SIDE-BY-SIDE COMPARISON\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec492ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_text = \"the cat sat on the mat\"\n",
    "comparison_tokens = tokenizer.encode(comparison_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b34077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM processing\n",
    "mlm_input_ids, mlm_labels, _ = mlm_processor.create_mlm_batch(comparison_text, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93265853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLM processing\n",
    "clm_input_ids, clm_labels = clm_processor.create_clm_batch(comparison_text, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53956468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM visualization\n",
    "ax1 = axes[0]\n",
    "tokens_display = [tokenizer.id_to_token[tid] for tid in comparison_tokens]\n",
    "mlm_input_display = [tokenizer.id_to_token[tid] for tid in mlm_input_ids[0].tolist()]\n",
    "mlm_mask = (mlm_labels[0] != -100).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d824ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_mlm = ['lightcoral' if masked else 'lightgreen' for masked in mlm_mask]\n",
    "y_pos = np.arange(len(tokens_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1.barh(y_pos, [1] * len(tokens_display), color=colors_mlm, alpha=0.6)\n",
    "for i, (orig, masked, is_masked) in enumerate(zip(tokens_display, mlm_input_display, mlm_mask)):\n",
    "    label = f\"{orig}\\n\u2192 {masked}\" if is_masked else orig\n",
    "    ax1.text(0.5, i, label, ha='center', va='center', fontsize=11, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078af90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels([f\"Pos {i}\" for i in range(len(tokens_display))])\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_title('Masked Language Modeling (MLM) - BERT Style\\nRed = Masked positions (predict these), Green = Context',\n",
    "              fontsize=13, fontweight='bold', pad=20)\n",
    "ax1.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLM visualization\n",
    "ax2 = axes[1]\n",
    "clm_input_display = [tokenizer.id_to_token[tid] for tid in clm_input_ids[0].tolist()]\n",
    "clm_label_display = [tokenizer.id_to_token[tid] for tid in clm_labels[0].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67cd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_clm = ['lightblue'] * len(clm_input_display)\n",
    "y_pos_clm = np.arange(len(clm_input_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.barh(y_pos_clm, [1] * len(clm_input_display), color=colors_clm, alpha=0.6)\n",
    "for i, (inp, target) in enumerate(zip(clm_input_display, clm_label_display)):\n",
    "    label = f\"{inp}\\n\u2192 {target}\"\n",
    "    ax2.text(0.5, i, label, ha='center', va='center', fontsize=11, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.set_yticks(y_pos_clm)\n",
    "ax2.set_yticklabels([f\"Pos {i}\" for i in range(len(clm_input_display))])\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_title('Causal Language Modeling (CLM) - GPT Style\\nBlue = Predict next token at each position',\n",
    "              fontsize=13, fontweight='bold', pad=20)\n",
    "ax2.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ffb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'mlm_vs_clm_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n\u2713 Saved comparison: {output_dir / 'mlm_vs_clm_comparison.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24452498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison table\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"COMPARISON TABLE\")\n",
    "print(\"-\" * 80)\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Direction',\n",
    "        'Masking',\n",
    "        'Loss Computed On',\n",
    "        'Training Efficiency',\n",
    "        'Best For',\n",
    "        'Examples',\n",
    "        'Attention Pattern',\n",
    "        'Generation'\n",
    "    ],\n",
    "    'MLM (BERT)': [\n",
    "        'Bidirectional',\n",
    "        '15% of tokens masked',\n",
    "        'Only masked positions',\n",
    "        'Lower (only ~15% tokens)',\n",
    "        'Understanding, classification',\n",
    "        'BERT, RoBERTa, ALBERT',\n",
    "        'Full attention',\n",
    "        'Not naturally generative'\n",
    "    ],\n",
    "    'CLM (GPT)': [\n",
    "        'Unidirectional (left-to-right)',\n",
    "        'No masking in input',\n",
    "        'All positions (predict next)',\n",
    "        'Higher (uses all tokens)',\n",
    "        'Generation, completion',\n",
    "        'GPT-2, GPT-3, GPT-4',\n",
    "        'Causal (triangular) mask',\n",
    "        'Natural for generation'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939562b9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for i in range(len(comparison_data['Aspect'])):\n",
    "    print(f\"\\n{comparison_data['Aspect'][i]}:\")\n",
    "    print(f\"  MLM: {comparison_data['MLM (BERT)'][i]}\")\n",
    "    print(f\"  CLM: {comparison_data['CLM (GPT)'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb978fa9",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 5: NEXT SENTENCE PREDICTION (NSP)\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NEXT SENTENCE PREDICTION (NSP)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "NSP is an additional training objective used in original BERT.\n",
    "\n",
    "KEY CONCEPTS:\n",
    "1. Given two sentences A and B, predict if B follows A in original text\n",
    "2. Binary classification: IsNext (1) or NotNext (0)\n",
    "3. Uses [CLS] token representation for classification\n",
    "4. 50% of time B actually follows A, 50% it's a random sentence\n",
    "\n",
    "PURPOSE:\n",
    "- Learn sentence-level relationships\n",
    "- Useful for tasks like QA where understanding sentence pairs matters\n",
    "\n",
    "CONTROVERSY:\n",
    "- Later work (RoBERTa) showed NSP might not be necessary\n",
    "- Removing it didn't hurt and sometimes helped performance\n",
    "- Modern models often skip NSP\n",
    "\n",
    "INTERVIEW TIP: Know that NSP exists but is less important than MLM/CLM!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2972ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSPDataProcessor:\n",
    "    \"\"\"\n",
    "    Processes data for Next Sentence Prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: SimpleTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cls_token_id = tokenizer.vocab['[CLS]']\n",
    "        self.sep_token_id = tokenizer.vocab['[SEP]']\n",
    "\n",
    "    def create_nsp_pair(\n",
    "        self,\n",
    "        sentence_a: str,\n",
    "        sentence_b: str,\n",
    "        is_next: bool\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Create NSP training example.\n",
    "\n",
    "        Format: [CLS] sentence_a [SEP] sentence_b [SEP]\n",
    "        Label: 1 if is_next, 0 otherwise\n",
    "        \"\"\"\n",
    "        # Encode sentences\n",
    "        tokens_a = tokenizer.encode(sentence_a)\n",
    "        tokens_b = tokenizer.encode(sentence_b)\n",
    "\n",
    "        # Combine with special tokens\n",
    "        tokens = [self.cls_token_id] + tokens_a + [self.sep_token_id] + tokens_b + [self.sep_token_id]\n",
    "\n",
    "        label = 1 if is_next else 0\n",
    "\n",
    "        return torch.tensor([tokens]), torch.tensor([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76afb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NSP examples\n",
    "nsp_processor = NSPDataProcessor(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380aeffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"NSP EXAMPLE 1: Positive pair (IsNext)\")\n",
    "print(\"-\" * 80)\n",
    "sent_a1 = \"the cat sat on the mat\"\n",
    "sent_b1 = \"the dog was lazy\"  # Could be next sentence\n",
    "nsp_input1, nsp_label1 = nsp_processor.create_nsp_pair(sent_a1, sent_b1, is_next=True)\n",
    "print(f\"Sentence A: '{sent_a1}'\")\n",
    "print(f\"Sentence B: '{sent_b1}'\")\n",
    "print(f\"Combined tokens: {nsp_input1[0].tolist()}\")\n",
    "print(f\"Decoded: '{tokenizer.decode(nsp_input1[0].tolist())}'\")\n",
    "print(f\"Label: {nsp_label1.item()} (IsNext)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"NSP EXAMPLE 2: Negative pair (NotNext)\")\n",
    "print(\"-\" * 80)\n",
    "sent_a2 = \"the cat sat on the mat\"\n",
    "sent_b2 = \"transformer model uses attention\"  # Random, unrelated\n",
    "nsp_input2, nsp_label2 = nsp_processor.create_nsp_pair(sent_a2, sent_b2, is_next=False)\n",
    "print(f\"Sentence A: '{sent_a2}'\")\n",
    "print(f\"Sentence B: '{sent_b2}'\")\n",
    "print(f\"Combined tokens: {nsp_input2[0].tolist()}\")\n",
    "print(f\"Decoded: '{tokenizer.decode(nsp_input2[0].tolist())}'\")\n",
    "print(f\"Label: {nsp_label2.item()} (NotNext)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTWithNSP(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model with NSP head.\n",
    "\n",
    "    Uses [CLS] token representation for sentence pair classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=4,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # MLM head\n",
    "        self.mlm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # NSP head (binary classification)\n",
    "        self.nsp_head = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embeddings(input_ids)\n",
    "        hidden_states = self.transformer(embeddings)\n",
    "\n",
    "        # MLM predictions for all positions\n",
    "        mlm_logits = self.mlm_head(hidden_states)\n",
    "\n",
    "        # NSP prediction from [CLS] token (position 0)\n",
    "        cls_hidden = hidden_states[:, 0, :]\n",
    "        nsp_logits = self.nsp_head(cls_hidden)\n",
    "\n",
    "        return mlm_logits, nsp_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae95424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate NSP\n",
    "bert_nsp_model = BERTWithNSP(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"NSP LOSS COMPUTATION\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289722f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_logits, nsp_logits = bert_nsp_model(nsp_input1)\n",
    "print(f\"Input shape: {nsp_input1.shape}\")\n",
    "print(f\"MLM logits shape: {mlm_logits.shape}\")\n",
    "print(f\"NSP logits shape: {nsp_logits.shape} (batch_size, 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSP loss\n",
    "nsp_loss = loss_fn(nsp_logits, nsp_label1)\n",
    "print(f\"\\nNSP Loss: {nsp_loss.item():.4f}\")\n",
    "print(f\"NSP Prediction: {torch.argmax(nsp_logits, dim=1).item()}\")\n",
    "print(f\"NSP Label: {nsp_label1.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709601c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "COMBINED TRAINING:\n",
    "- Original BERT trained with BOTH MLM and NSP simultaneously\n",
    "- Total loss = MLM loss + NSP loss\n",
    "- Multi-task learning helps create better representations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb278ddc",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 6: MASKING PATTERNS VISUALIZATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MASKING PATTERNS VISUALIZATION\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create longer example for visualization\n",
    "long_text = \"the quick brown fox jumps over the lazy dog the cat sat on the mat\"\n",
    "long_tokens = tokenizer.encode(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MLM masking multiple times to show randomness\n",
    "num_examples = 5\n",
    "masking_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb088b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_examples):\n",
    "    input_ids, labels, _ = mlm_processor.create_mlm_batch(long_text, verbose=False)\n",
    "    masked = (labels[0] != -100).numpy()\n",
    "    masking_results.append(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ddf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize masking patterns\n",
    "fig, axes = plt.subplots(num_examples, 1, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (ax, masked) in enumerate(zip(axes, masking_results)):\n",
    "    # Create color map: red for masked, green for unmasked\n",
    "    colors = ['red' if m else 'green' for m in masked]\n",
    "\n",
    "    # Bar plot\n",
    "    ax.bar(range(len(masked)), [1] * len(masked), color=colors, alpha=0.6, edgecolor='black')\n",
    "\n",
    "    # Add token labels\n",
    "    for j, token_id in enumerate(long_tokens):\n",
    "        token = tokenizer.id_to_token[token_id]\n",
    "        ax.text(j, 0.5, token, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax.set_xlim(-0.5, len(masked) - 0.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(range(len(masked)))\n",
    "    ax.set_xticklabels([f\"{i}\" for i in range(len(masked))], fontsize=8)\n",
    "    ax.set_title(f'Masking Pattern {i+1} ({masked.sum()}/{len(masked)} tokens masked = {masked.sum()/len(masked)*100:.1f}%)',\n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Token Position' if i == num_examples - 1 else '', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f8f39",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.suptitle('MLM Masking Patterns (15% probability per token)\\nRed = Masked, Green = Unmasked',\n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'masking_patterns.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\u2713 Saved masking patterns: {output_dir / 'masking_patterns.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59831ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 80-10-10 rule\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"80-10-10 MASKING STRATEGY BREAKDOWN\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ecc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate many maskings to show distribution\n",
    "mask_types = {'[MASK]': 0, 'random': 0, 'unchanged': 0}\n",
    "num_simulations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_simulations):\n",
    "    prob = random.random()\n",
    "    if prob < 0.8:\n",
    "        mask_types['[MASK]'] += 1\n",
    "    elif prob < 0.9:\n",
    "        mask_types['random'] += 1\n",
    "    else:\n",
    "        mask_types['unchanged'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors_strategy = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "bars = ax.bar(mask_types.keys(), mask_types.values(), color=colors_strategy, alpha=0.7, edgecolor='black', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bar, (key, value) in zip(bars, mask_types.items()):\n",
    "    height = bar.get_height()\n",
    "    percentage = (value / num_simulations) * 100\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{value}\\n({percentage:.1f}%)',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_ylabel('Count (out of 1000)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Masking Strategy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('BERT Masking Strategy Distribution\\n80% [MASK], 10% Random Token, 10% Unchanged',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_ylim(0, max(mask_types.values()) * 1.2)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0606fa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'masking_strategy_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\u2713 Saved masking strategy: {output_dir / 'masking_strategy_distribution.png'}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00186bd",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "PART 7: INTERVIEW PREP SUMMARY\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERVIEW PREPARATION SUMMARY\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_qa = {\n",
    "    \"Q7: How does masked language modeling work?\": \"\"\"\n",
    "    Answer:\n",
    "    - Randomly mask 15% of tokens in the input\n",
    "    - Model must predict original token using bidirectional context\n",
    "    - Masking strategy: 80% [MASK], 10% random token, 10% unchanged\n",
    "    - Loss computed only on masked positions\n",
    "    - Used by BERT, RoBERTa, ALBERT\n",
    "    - Good for understanding tasks (classification, NER, etc.)\n",
    "    \"\"\",\n",
    "\n",
    "    \"Q9: What's the difference between MLM and Causal LM?\": \"\"\"\n",
    "    Answer:\n",
    "    MLM (BERT-style):\n",
    "    - Bidirectional: sees full context\n",
    "    - Masks random tokens, predicts them\n",
    "    - Loss on ~15% of tokens only\n",
    "    - Better for understanding tasks\n",
    "\n",
    "    Causal LM (GPT-style):\n",
    "    - Unidirectional: left-to-right only\n",
    "    - Predicts next token at each position\n",
    "    - Loss on all tokens\n",
    "    - Better for generation tasks\n",
    "    - Uses causal attention mask\n",
    "\n",
    "    Key insight: MLM sees future tokens (bidirectional) while CLM doesn't (causal)\n",
    "    \"\"\",\n",
    "\n",
    "    \"Q11: Why do we use different masking strategies (80-10-10)?\": \"\"\"\n",
    "    Answer:\n",
    "    - 80% [MASK]: Main training signal, model learns to predict masked tokens\n",
    "    - 10% random token: Prevents overfitting to [MASK] token, model can't rely on it\n",
    "    - 10% unchanged: Forces model to learn representations for all tokens, not just masked ones\n",
    "\n",
    "    Without this strategy:\n",
    "    - Model might only learn to predict when it sees [MASK]\n",
    "    - At inference, there's no [MASK] token, causing train/test mismatch\n",
    "    - This strategy makes training more robust\n",
    "\n",
    "    Trade-off: More complex but better generalization\n",
    "    \"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, answer in interview_qa.items():\n",
    "    print(f\"\\n{question}\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY TAKEAWAYS FOR INTERVIEWS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. MLM (BERT):\n",
    "   - Bidirectional, mask random tokens\n",
    "   - Good for understanding and classification\n",
    "   - 80-10-10 masking strategy is crucial\n",
    "   - Loss only on masked positions\n",
    "\n",
    "2. CLM (GPT):\n",
    "   - Unidirectional, predict next token\n",
    "   - Good for generation\n",
    "   - Simpler but very effective\n",
    "   - Loss on all positions\n",
    "   - Uses causal attention mask\n",
    "\n",
    "3. NSP (BERT):\n",
    "   - Additional objective for sentence pairs\n",
    "   - Binary classification: IsNext or NotNext\n",
    "   - Later shown to be less important\n",
    "   - Many modern models skip it\n",
    "\n",
    "4. Attention Patterns:\n",
    "   - MLM: Full attention matrix (bidirectional)\n",
    "   - CLM: Triangular attention matrix (causal)\n",
    "   - This is the fundamental architectural difference\n",
    "\n",
    "5. Interview Red Flags to Avoid:\n",
    "   \u2717 \"BERT uses masking in attention\" (No! It's bidirectional)\n",
    "   \u2717 \"GPT can see future tokens\" (No! It's causal)\n",
    "   \u2717 \"All masked tokens use [MASK]\" (No! 80-10-10 rule)\n",
    "   \u2717 \"NSP is essential\" (No! It's optional)\n",
    "\n",
    "6. Good Interview Answers:\n",
    "   \u2713 Explain bidirectional vs causal attention\n",
    "   \u2713 Describe the 80-10-10 masking strategy\n",
    "   \u2713 Know when to use MLM vs CLM\n",
    "   \u2713 Understand the loss computation differences\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e31937",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEMO COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAll visualizations saved to: {output_dir}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. causal_attention_mask.png - Shows causal masking pattern\")\n",
    "print(\"  2. mlm_vs_clm_comparison.png - Side-by-side comparison\")\n",
    "print(\"  3. masking_patterns.png - Multiple masking examples\")\n",
    "print(\"  4. masking_strategy_distribution.png - 80-10-10 breakdown\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}