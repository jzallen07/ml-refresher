{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: DataLoaders - Code Exercise\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6dde4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "================================================================================\n",
    "LESSON 09: DATALOADERS & BATCHING IN PYTORCH\n",
    "================================================================================\n",
    "\n",
    "Topics Covered:\n",
    "1. Creating custom Dataset classes\n",
    "2. Understanding DataLoader parameters (batch_size, shuffle, num_workers, etc.)\n",
    "3. Data splitting (train/validation/test) with random_split\n",
    "4. Iterating through batches efficiently\n",
    "5. Effect of shuffling on data order\n",
    "6. Comparing batch sizes and their impact\n",
    "7. Multi-worker data loading for performance\n",
    "8. Custom collate functions for variable-size data\n",
    "9. Complete train/val/test DataLoader setup pattern\n",
    "\n",
    "Learning Objectives:\n",
    "- Master the DataLoader API for efficient batch processing\n",
    "- Understand how to split datasets properly\n",
    "- Learn performance optimization with num_workers\n",
    "- Create production-ready data loading pipelines\n",
    "- Debug and time data loading operations\n",
    "\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LESSON 09: DATALOADERS & BATCHING\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Number of CPU cores available: {os.cpu_count()}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85afc87d",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 1: CREATING A CUSTOM DATASET CLASS\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 1: CREATING A CUSTOM DATASET CLASS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"In PyTorch, the Dataset class is an abstract class representing a dataset.\")\n",
    "print(\"To create a custom dataset, we need to implement three methods:\")\n",
    "print(\"  1. __init__()  - Initialize the dataset (load file paths, labels, etc.)\")\n",
    "print(\"  2. __len__()   - Return the total number of samples\")\n",
    "print(\"  3. __getitem__() - Return a single sample given an index\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "class OxfordFlowersDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Oxford Flowers 102 dataset.\n",
    "\n",
    "    This class demonstrates how to create a PyTorch Dataset that:\n",
    "    - Loads images from disk on-the-fly (memory efficient)\n",
    "    - Applies transforms to preprocess images\n",
    "    - Returns (image, label) pairs\n",
    "\n",
    "    Args:\n",
    "        image_dir (str or Path): Directory containing .jpg images\n",
    "        labels (list): List of integer labels (one per image)\n",
    "        transform (callable, optional): Transform to apply to images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        This method is called once when you create the dataset object.\n",
    "        It should load metadata (file paths, labels) but NOT load all images\n",
    "        into memory - that would be too memory intensive!\n",
    "        \"\"\"\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get all image paths and sort them for consistency\n",
    "        self.image_paths = sorted(list(self.image_dir.glob(\"*.jpg\")))\n",
    "\n",
    "        # Make sure we have the right number of labels\n",
    "        assert len(self.image_paths) == len(self.labels), \\\n",
    "            f\"Number of images ({len(self.image_paths)}) != number of labels ({len(self.labels)})\"\n",
    "\n",
    "        print(f\"  Dataset initialized with {len(self.image_paths)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "\n",
    "        This is used by DataLoader to know how many iterations to do per epoch.\n",
    "        \"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return a single sample from the dataset.\n",
    "\n",
    "        This method is called by DataLoader to fetch individual samples.\n",
    "        It's called many times, so it should be efficient!\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve (0 to len(dataset)-1)\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, label) where image is a PIL Image or tensor\n",
    "        \"\"\"\n",
    "        # Load the image from disk\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure RGB format\n",
    "\n",
    "        # Get the corresponding label\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class SyntheticDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Fallback synthetic dataset for demonstration when real data isn't available.\n",
    "\n",
    "    This creates random images and labels on-the-fly.\n",
    "    Useful for testing and learning without requiring downloads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_samples=1000, image_size=(224, 224), num_classes=10, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize synthetic dataset.\n",
    "\n",
    "        Args:\n",
    "            num_samples: Total number of synthetic samples to generate\n",
    "            image_size: Size of each image (height, width)\n",
    "            num_classes: Number of different classes\n",
    "            transform: Optional transform to apply\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.transform = transform\n",
    "\n",
    "        # Pre-generate labels (but not images - those are generated on-the-fly)\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,)).tolist()\n",
    "\n",
    "        print(f\"  Synthetic dataset initialized with {num_samples} samples\")\n",
    "        print(f\"  Image size: {image_size}, Classes: {num_classes}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generate a random image and return with its label.\n",
    "\n",
    "        In practice, you'd load real data here. But for learning/testing,\n",
    "        synthetic data is perfectly fine!\n",
    "        \"\"\"\n",
    "        # Generate a random image (values between 0 and 255)\n",
    "        # Shape: (height, width, channels)\n",
    "        img_array = np.random.randint(0, 256,\n",
    "                                      (*self.image_size, 3),\n",
    "                                      dtype=np.uint8)\n",
    "        image = Image.fromarray(img_array)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "print(\"Custom Dataset classes defined!\")\n",
    "print()\n",
    "print(\"Key points about PyTorch Datasets:\")\n",
    "print(\"  \u2022 Dataset is responsible for loading individual samples\")\n",
    "print(\"  \u2022 It does NOT handle batching - that's the DataLoader's job\")\n",
    "print(\"  \u2022 __getitem__() is called once per sample by the DataLoader\")\n",
    "print(\"  \u2022 Keep __getitem__() efficient - it's called thousands of times!\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd6cf5",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 2: LOADING THE DATASET\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef573e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 2: LOADING THE DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Define paths\n",
    "data_dir = Path(\"/Users/zack/dev/ml-refresher/data/oxford_flowers\")\n",
    "image_dir = data_dir / \"jpg\"\n",
    "\n",
    "print(f\"Looking for Oxford Flowers dataset at: {image_dir}\")\n",
    "print()\n",
    "\n",
    "# Define transforms (from Lesson 08)\n",
    "# These are standard transforms for ImageNet-style models\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),              # Resize shortest edge to 256\n",
    "    transforms.CenterCrop(224),          # Crop center 224x224\n",
    "    transforms.ToTensor(),               # Convert to tensor [0, 1]\n",
    "    transforms.Normalize(                # Normalize with ImageNet stats\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"Transform pipeline defined:\")\n",
    "print(\"  1. Resize to 256 pixels (shortest edge)\")\n",
    "print(\"  2. Center crop to 224x224\")\n",
    "print(\"  3. Convert to tensor\")\n",
    "print(\"  4. Normalize with ImageNet statistics\")\n",
    "print()\n",
    "\n",
    "# Try to load Oxford Flowers dataset\n",
    "if image_dir.exists() and len(list(image_dir.glob(\"*.jpg\"))) > 0:\n",
    "    print(\"\u2713 Oxford Flowers dataset found!\")\n",
    "\n",
    "    # Get all image paths\n",
    "    image_paths = sorted(list(image_dir.glob(\"*.jpg\")))\n",
    "    num_images = len(image_paths)\n",
    "    print(f\"  Found {num_images} images\")\n",
    "\n",
    "    # Load labels from .mat file\n",
    "    try:\n",
    "        from scipy.io import loadmat\n",
    "        labels_file = data_dir / \"imagelabels.mat\"\n",
    "        mat_data = loadmat(labels_file)\n",
    "        labels = mat_data['labels'][0].tolist()  # Labels are 1-indexed (1-102)\n",
    "        labels = [l - 1 for l in labels]  # Convert to 0-indexed (0-101)\n",
    "\n",
    "        print(f\"  Loaded {len(labels)} labels (102 flower classes)\")\n",
    "        print(f\"  Label range: {min(labels)} to {max(labels)}\")\n",
    "    except:\n",
    "        print(\"  ! Could not load labels file, generating synthetic labels...\")\n",
    "        labels = [i % 102 for i in range(num_images)]\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = OxfordFlowersDataset(image_dir, labels, transform=transform)\n",
    "\n",
    "else:\n",
    "    print(\"! Oxford Flowers dataset not found.\")\n",
    "    print(\"  Using synthetic dataset for demonstration...\")\n",
    "    print()\n",
    "\n",
    "    # Create synthetic dataset\n",
    "    dataset = SyntheticDataset(\n",
    "        num_samples=1000,\n",
    "        image_size=(224, 224),\n",
    "        num_classes=102,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "print()\n",
    "print(f\"Dataset ready! Total samples: {len(dataset)}\")\n",
    "print()\n",
    "\n",
    "# Test dataset by loading one sample\n",
    "print(\"-\" * 60)\n",
    "print(\"Testing dataset by loading a single sample...\")\n",
    "sample_image, sample_label = dataset[0]\n",
    "print(f\"  Sample image shape: {sample_image.shape}\")\n",
    "print(f\"  Sample image dtype: {sample_image.dtype}\")\n",
    "print(f\"  Sample image range: [{sample_image.min():.3f}, {sample_image.max():.3f}]\")\n",
    "print(f\"  Sample label: {sample_label} (class ID)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779ca6e",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 3: DATA SPLITTING (TRAIN/VAL/TEST)\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10aaf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 3: DATA SPLITTING (TRAIN/VAL/TEST)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Before training, we need to split our dataset into:\")\n",
    "print(\"  \u2022 Training set   (70%) - Used to train the model\")\n",
    "print(\"  \u2022 Validation set (15%) - Used to tune hyperparameters\")\n",
    "print(\"  \u2022 Test set       (15%) - Used for final evaluation\")\n",
    "print()\n",
    "print(\"Why split?\")\n",
    "print(\"  \u2022 Training set: Model learns patterns from this data\")\n",
    "print(\"  \u2022 Validation set: Used to check if model generalizes (prevents overfitting)\")\n",
    "print(\"  \u2022 Test set: Final unbiased evaluation (model never sees this during training!)\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calculate split sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.70 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size  # Ensure all samples are used\n",
    "\n",
    "print(f\"Total dataset size: {total_size}\")\n",
    "print(f\"  Train size: {train_size} ({train_size/total_size*100:.1f}%)\")\n",
    "print(f\"  Val size:   {val_size} ({val_size/total_size*100:.1f}%)\")\n",
    "print(f\"  Test size:  {test_size} ({test_size/total_size*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Use random_split with a generator for reproducibility\n",
    "print(\"Using torch.utils.data.random_split() to split dataset...\")\n",
    "print(\"  Setting generator seed for reproducibility (seed=42)\")\n",
    "\n",
    "# Create a generator with a fixed seed\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=generator  # This ensures same split every time!\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"\u2713 Dataset split complete!\")\n",
    "print(f\"  Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"  Val dataset:   {len(val_dataset)} samples\")\n",
    "print(f\"  Test dataset:  {len(test_dataset)} samples\")\n",
    "print()\n",
    "print(\"IMPORTANT: The split is reproducible because we used a seeded generator.\")\n",
    "print(\"           Running this code again will produce the same split!\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc101d44",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 4: CREATING DATALOADERS\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86399c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 4: CREATING DATALOADERS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"The DataLoader wraps a Dataset and provides:\")\n",
    "print(\"  \u2022 Batching: Groups samples into batches\")\n",
    "print(\"  \u2022 Shuffling: Randomizes sample order (important for training!)\")\n",
    "print(\"  \u2022 Parallel loading: Uses multiple workers for faster loading\")\n",
    "print(\"  \u2022 Memory pinning: Speeds up GPU transfer\")\n",
    "print()\n",
    "print(\"Key DataLoader parameters:\")\n",
    "print(\"  \u2022 batch_size: Number of samples per batch\")\n",
    "print(\"  \u2022 shuffle: Whether to shuffle data each epoch\")\n",
    "print(\"  \u2022 num_workers: Number of subprocesses for data loading\")\n",
    "print(\"  \u2022 pin_memory: Pin memory for faster GPU transfer\")\n",
    "print(\"  \u2022 drop_last: Drop last incomplete batch if True\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create basic DataLoader\n",
    "batch_size = 32\n",
    "print(f\"Creating DataLoader with batch_size={batch_size}...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,        # Shuffle training data each epoch\n",
    "    num_workers=0,       # 0 means load in main process (we'll compare later)\n",
    "    pin_memory=True,     # Speeds up GPU transfer\n",
    "    drop_last=False      # Keep last batch even if smaller\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Train DataLoader created!\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Number of batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Shuffle: True\")\n",
    "print(f\"  Num workers: 0 (main process only)\")\n",
    "print()\n",
    "\n",
    "# Create validation and test loaders (no shuffling for evaluation!)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,       # Don't shuffle validation data\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,       # Don't shuffle test data\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Val DataLoader created!\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Number of batches: {len(val_loader)}\")\n",
    "print(f\"  Shuffle: False (we want consistent evaluation)\")\n",
    "print()\n",
    "\n",
    "print(f\"\u2713 Test DataLoader created!\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Number of batches: {len(test_loader)}\")\n",
    "print(f\"  Shuffle: False\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e4e8a",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 5: ITERATING THROUGH BATCHES\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c2356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 5: ITERATING THROUGH BATCHES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Let's iterate through the DataLoader to see how batching works...\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Time the iteration\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Iterating through first 5 batches of training data:\")\n",
    "print()\n",
    "\n",
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    if batch_idx >= 5:  # Only show first 5 batches\n",
    "        break\n",
    "\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(f\"  Images shape: {images.shape}\")\n",
    "    print(f\"    \u2022 Batch size: {images.shape[0]}\")\n",
    "    print(f\"    \u2022 Channels: {images.shape[1]} (RGB)\")\n",
    "    print(f\"    \u2022 Height: {images.shape[2]} pixels\")\n",
    "    print(f\"    \u2022 Width: {images.shape[3]} pixels\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    print(f\"  Labels: {labels[:10].tolist()}\")  # Show first 10 labels\n",
    "    print(f\"  Image dtype: {images.dtype}\")\n",
    "    print(f\"  Label dtype: {labels.dtype}\")\n",
    "    print(f\"  Memory: {images.element_size() * images.nelement() / 1024 / 1024:.2f} MB\")\n",
    "    print()\n",
    "\n",
    "iteration_time = time.time() - start_time\n",
    "print(f\"Time to iterate through 5 batches: {iteration_time:.4f} seconds\")\n",
    "print()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"  \u2022 Images are batched into shape [batch_size, channels, height, width]\")\n",
    "print(\"  \u2022 Labels are batched into shape [batch_size]\")\n",
    "print(\"  \u2022 DataLoader automatically handles batching - no manual work needed!\")\n",
    "print(\"  \u2022 Each iteration of the for loop gives you one batch\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0c90f",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 6: EFFECT OF SHUFFLING\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 6: EFFECT OF SHUFFLING\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Shuffling is crucial for training neural networks!\")\n",
    "print(\"It prevents the model from learning the order of data.\")\n",
    "print()\n",
    "print(\"Let's compare shuffle=True vs shuffle=False...\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create two loaders: one with shuffle, one without\n",
    "shuffle_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "no_shuffle_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"WITH SHUFFLE=TRUE:\")\n",
    "print(\"  Running two epochs to see if label order changes...\")\n",
    "print()\n",
    "\n",
    "# First epoch with shuffle\n",
    "labels_epoch1 = []\n",
    "for batch_idx, (_, labels) in enumerate(shuffle_loader):\n",
    "    if batch_idx >= 3:  # First 3 batches\n",
    "        break\n",
    "    labels_epoch1.extend(labels[:8].tolist())  # Take first 8 labels from each\n",
    "\n",
    "print(f\"Epoch 1 labels (first 24): {labels_epoch1}\")\n",
    "\n",
    "# Second epoch with shuffle\n",
    "labels_epoch2 = []\n",
    "for batch_idx, (_, labels) in enumerate(shuffle_loader):\n",
    "    if batch_idx >= 3:\n",
    "        break\n",
    "    labels_epoch2.extend(labels[:8].tolist())\n",
    "\n",
    "print(f\"Epoch 2 labels (first 24): {labels_epoch2}\")\n",
    "print()\n",
    "print(f\"Are they the same? {labels_epoch1 == labels_epoch2}\")\n",
    "print(\"  \u21b3 Labels are in DIFFERENT order each epoch (good for training!)\")\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"WITHOUT SHUFFLE (shuffle=False):\")\n",
    "print(\"  Running two epochs to see if label order stays the same...\")\n",
    "print()\n",
    "\n",
    "# First epoch without shuffle\n",
    "labels_epoch1_no_shuffle = []\n",
    "for batch_idx, (_, labels) in enumerate(no_shuffle_loader):\n",
    "    if batch_idx >= 3:\n",
    "        break\n",
    "    labels_epoch1_no_shuffle.extend(labels[:8].tolist())\n",
    "\n",
    "print(f\"Epoch 1 labels (first 24): {labels_epoch1_no_shuffle}\")\n",
    "\n",
    "# Second epoch without shuffle\n",
    "labels_epoch2_no_shuffle = []\n",
    "for batch_idx, (_, labels) in enumerate(no_shuffle_loader):\n",
    "    if batch_idx >= 3:\n",
    "        break\n",
    "    labels_epoch2_no_shuffle.extend(labels[:8].tolist())\n",
    "\n",
    "print(f\"Epoch 2 labels (first 24): {labels_epoch2_no_shuffle}\")\n",
    "print()\n",
    "print(f\"Are they the same? {labels_epoch1_no_shuffle == labels_epoch2_no_shuffle}\")\n",
    "print(\"  \u21b3 Labels are in SAME order each epoch (bad for training!)\")\n",
    "print()\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(\"  \u2022 Training: ALWAYS use shuffle=True\")\n",
    "print(\"  \u2022 Validation/Test: Use shuffle=False for consistent evaluation\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca50161",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 7: COMPARING BATCH SIZES\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b581d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 7: COMPARING BATCH SIZES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Batch size is a crucial hyperparameter that affects:\")\n",
    "print(\"  \u2022 Training speed: Larger batches = fewer iterations\")\n",
    "print(\"  \u2022 Memory usage: Larger batches = more GPU memory needed\")\n",
    "print(\"  \u2022 Gradient quality: Smaller batches = noisier gradients\")\n",
    "print(\"  \u2022 Generalization: Often smaller batches generalize better!\")\n",
    "print()\n",
    "print(\"Let's compare different batch sizes...\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "results = []\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"Testing batch_size={bs}...\")\n",
    "\n",
    "    # Create loader\n",
    "    loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=bs,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    num_batches = len(loader)\n",
    "\n",
    "    # Time one epoch\n",
    "    start = time.time()\n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        # Simulate some processing\n",
    "        _ = images.mean()  # Small computation\n",
    "\n",
    "        if batch_idx >= 20:  # Only process 20 batches for timing\n",
    "            break\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    time_per_batch = elapsed / min(20, num_batches)\n",
    "\n",
    "    # Calculate memory usage for one batch\n",
    "    sample_images, _ = next(iter(loader))\n",
    "    memory_mb = sample_images.element_size() * sample_images.nelement() / 1024 / 1024\n",
    "\n",
    "    results.append({\n",
    "        'batch_size': bs,\n",
    "        'num_batches': num_batches,\n",
    "        'time_per_batch': time_per_batch,\n",
    "        'memory_mb': memory_mb\n",
    "    })\n",
    "\n",
    "    print(f\"  Batches per epoch: {num_batches}\")\n",
    "    print(f\"  Time per batch: {time_per_batch:.4f} seconds\")\n",
    "    print(f\"  Memory per batch: {memory_mb:.2f} MB\")\n",
    "    print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"COMPARISON SUMMARY:\")\n",
    "print()\n",
    "print(f\"{'Batch Size':<12} {'Batches':<10} {'Time/Batch':<15} {'Memory':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for r in results:\n",
    "    print(f\"{r['batch_size']:<12} {r['num_batches']:<10} \"\n",
    "          f\"{r['time_per_batch']:.4f}s{'':<8} {r['memory_mb']:.2f} MB\")\n",
    "print()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"  \u2022 Larger batches \u2192 Fewer batches per epoch\")\n",
    "print(\"  \u2022 Larger batches \u2192 More memory usage\")\n",
    "print(\"  \u2022 Larger batches \u2192 Often slightly faster per batch (better GPU utilization)\")\n",
    "print(\"  \u2022 Common choices: 16, 32, 64, 128, 256 (powers of 2)\")\n",
    "print()\n",
    "\n",
    "print(\"How to choose batch size?\")\n",
    "print(\"  1. Start with 32 or 64\")\n",
    "print(\"  2. Increase until you run out of GPU memory\")\n",
    "print(\"  3. If training is unstable, try smaller batches\")\n",
    "print(\"  4. Use batch size that's a power of 2 (hardware optimization)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b75af",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 8: MULTI-WORKER DATA LOADING\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf675168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 8: MULTI-WORKER DATA LOADING\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"num_workers controls how many subprocesses are used for data loading.\")\n",
    "print(\"This can significantly speed up training!\")\n",
    "print()\n",
    "print(\"How it works:\")\n",
    "print(\"  \u2022 num_workers=0: Load data in main process (simple but slower)\")\n",
    "print(\"  \u2022 num_workers=2: Use 2 worker processes to load data in parallel\")\n",
    "print(\"  \u2022 num_workers=4: Use 4 worker processes (even faster!)\")\n",
    "print()\n",
    "print(\"Benefit: While GPU trains on current batch, workers load the next batch\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(f\"Available CPU cores: {os.cpu_count()}\")\n",
    "print()\n",
    "\n",
    "# Compare different num_workers settings\n",
    "# Note: On macOS/Windows, use num_workers=0 to avoid multiprocessing issues\n",
    "# On Linux, you can safely use num_workers > 0\n",
    "worker_counts = [0]  # Only test 0 to avoid multiprocessing complications\n",
    "worker_results = []\n",
    "\n",
    "print(\"Note: Testing with num_workers=0 only to avoid multiprocessing issues.\")\n",
    "print(\"      In production on Linux, you can use num_workers=2-4 for speedup.\")\n",
    "print()\n",
    "\n",
    "for num_workers in worker_counts:\n",
    "    print(f\"Testing num_workers={num_workers}...\")\n",
    "\n",
    "    # Create loader\n",
    "    loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Time loading 30 batches\n",
    "    start = time.time()\n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        # Simulate processing\n",
    "        _ = images.mean()\n",
    "\n",
    "        if batch_idx >= 29:  # Load 30 batches\n",
    "            break\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    batches_per_second = 30 / elapsed\n",
    "\n",
    "    worker_results.append({\n",
    "        'num_workers': num_workers,\n",
    "        'total_time': elapsed,\n",
    "        'batches_per_second': batches_per_second\n",
    "    })\n",
    "\n",
    "    print(f\"  Time for 30 batches: {elapsed:.3f}s\")\n",
    "    print(f\"  Batches per second: {batches_per_second:.2f}\")\n",
    "    print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"WORKER COMPARISON:\")\n",
    "print()\n",
    "print(f\"{'Workers':<10} {'Total Time':<15} {'Batches/sec':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for r in worker_results:\n",
    "    print(f\"{r['num_workers']:<10} {r['total_time']:.3f}s{'':<9} \"\n",
    "          f\"{r['batches_per_second']:.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"Guidelines for num_workers:\")\n",
    "print(\"  \u2022 On macOS/Windows: Use num_workers=0 (multiprocessing has issues)\")\n",
    "print(\"  \u2022 On Linux: Start with num_workers=2 or 4 for speedup\")\n",
    "print(\"  \u2022 Don't use more workers than CPU cores\")\n",
    "print(\"  \u2022 Too many workers can actually slow things down!\")\n",
    "print(\"  \u2022 Typical speedup: 2-3x faster with 4 workers on Linux\")\n",
    "print()\n",
    "\n",
    "print(\"IMPORTANT: In practice, the speedup is most noticeable when:\")\n",
    "print(\"  \u2022 Loading from slow storage (HDD vs SSD)\")\n",
    "print(\"  \u2022 Applying heavy data augmentation\")\n",
    "print(\"  \u2022 Working with large images or complex preprocessing\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e991ac9",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 9: COMPLETE TRAIN/VAL/TEST SETUP PATTERN\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21641e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 9: COMPLETE TRAIN/VAL/TEST SETUP PATTERN\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Here's a production-ready pattern for setting up DataLoaders:\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Define different transforms for training vs evaluation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),           # Random crop for augmentation\n",
    "    transforms.RandomHorizontalFlip(),    # Random flip for augmentation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Color augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),           # Center crop (deterministic)\n",
    "    # No random augmentation for evaluation!\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Step 1: Define separate transforms for training and evaluation\")\n",
    "print(\"  Training transform: Random crop, flip, color jitter (augmentation)\")\n",
    "print(\"  Eval transform: Center crop only (no randomness)\")\n",
    "print()\n",
    "\n",
    "# For this demo, we'll use our existing split datasets\n",
    "# In practice, you'd create new datasets with different transforms\n",
    "print(\"Step 2: Create datasets with appropriate transforms\")\n",
    "print(\"  (In practice, you'd recreate datasets with train_transform/eval_transform)\")\n",
    "print()\n",
    "\n",
    "print(\"Step 3: Create DataLoaders with appropriate settings\")\n",
    "print()\n",
    "\n",
    "# Training loader settings\n",
    "final_train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,           # Shuffle training data\n",
    "    num_workers=0,          # Use 0 for macOS/Windows, 2-4 for Linux\n",
    "    pin_memory=True,        # Pin memory for GPU\n",
    "    drop_last=True          # Drop last incomplete batch\n",
    ")\n",
    "\n",
    "print(\"Training DataLoader:\")\n",
    "print(f\"  batch_size=64 (larger for efficiency)\")\n",
    "print(f\"  shuffle=True (randomize order)\")\n",
    "print(f\"  num_workers=0 (use 2-4 on Linux for parallel loading)\")\n",
    "print(f\"  pin_memory=True (faster GPU transfer)\")\n",
    "print(f\"  drop_last=True (consistent batch sizes)\")\n",
    "print(f\"  \u2192 {len(final_train_loader)} batches per epoch\")\n",
    "print()\n",
    "\n",
    "# Validation loader settings\n",
    "final_val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,         # Can use larger batch for evaluation\n",
    "    shuffle=False,          # Don't shuffle\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=False         # Keep all validation samples\n",
    ")\n",
    "\n",
    "print(\"Validation DataLoader:\")\n",
    "print(f\"  batch_size=128 (larger since no gradients)\")\n",
    "print(f\"  shuffle=False (consistent evaluation)\")\n",
    "print(f\"  num_workers=0 (use 2-4 on Linux)\")\n",
    "print(f\"  pin_memory=True\")\n",
    "print(f\"  drop_last=False (evaluate all samples)\")\n",
    "print(f\"  \u2192 {len(final_val_loader)} batches\")\n",
    "print()\n",
    "\n",
    "# Test loader settings (same as validation)\n",
    "final_test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(\"Test DataLoader:\")\n",
    "print(f\"  batch_size=128\")\n",
    "print(f\"  shuffle=False\")\n",
    "print(f\"  num_workers=0 (use 2-4 on Linux)\")\n",
    "print(f\"  pin_memory=True\")\n",
    "print(f\"  drop_last=False\")\n",
    "print(f\"  \u2192 {len(final_test_loader)} batches\")\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Complete training loop structure:\")\n",
    "print()\n",
    "print(\"for epoch in range(num_epochs):\")\n",
    "print(\"    # Training phase\")\n",
    "print(\"    model.train()\")\n",
    "print(\"    for images, labels in final_train_loader:\")\n",
    "print(\"        # Forward pass, backward pass, optimizer step\")\n",
    "print(\"        ...\")\n",
    "print()\n",
    "print(\"    # Validation phase\")\n",
    "print(\"    model.eval()\")\n",
    "print(\"    with torch.no_grad():\")\n",
    "print(\"        for images, labels in final_val_loader:\")\n",
    "print(\"            # Evaluate model\")\n",
    "print(\"            ...\")\n",
    "print()\n",
    "print(\"# Final evaluation on test set\")\n",
    "print(\"model.eval()\")\n",
    "print(\"with torch.no_grad():\")\n",
    "print(\"    for images, labels in final_test_loader:\")\n",
    "print(\"        # Final test evaluation\")\n",
    "print(\"        ...\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185336d9",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 10: CUSTOM COLLATE FUNCTION\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 10: CUSTOM COLLATE FUNCTION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Sometimes you need custom batching logic. Examples:\")\n",
    "print(\"  \u2022 Variable-length sequences (text, time series)\")\n",
    "print(\"  \u2022 Images of different sizes\")\n",
    "print(\"  \u2022 Complex data structures\")\n",
    "print()\n",
    "print(\"The collate_fn parameter lets you customize how samples are batched.\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that handles variable-size data.\n",
    "\n",
    "    Args:\n",
    "        batch: List of (image, label) tuples from dataset\n",
    "\n",
    "    Returns:\n",
    "        Batched images and labels (with custom logic)\n",
    "    \"\"\"\n",
    "    # Separate images and labels\n",
    "    images, labels = zip(*batch)\n",
    "\n",
    "    # Example: Filter out None values (if dataset can return None)\n",
    "    images = [img for img in images if img is not None]\n",
    "    labels = [lbl for lbl in labels if lbl is not None]\n",
    "\n",
    "    # Stack images into a batch (assumes same size)\n",
    "    images_batch = torch.stack(images, dim=0)\n",
    "\n",
    "    # Convert labels to tensor\n",
    "    labels_batch = torch.tensor(labels)\n",
    "\n",
    "    # You could add custom logic here:\n",
    "    # - Pad sequences to same length\n",
    "    # - Resize images to same size\n",
    "    # - Add metadata\n",
    "    # - Apply per-batch augmentation\n",
    "\n",
    "    return images_batch, labels_batch\n",
    "\n",
    "\n",
    "# Create loader with custom collate function\n",
    "custom_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn,  # Use custom batching\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Custom collate function defined!\")\n",
    "print()\n",
    "print(\"Testing custom collate function...\")\n",
    "images, labels = next(iter(custom_loader))\n",
    "print(f\"  Batch shape: {images.shape}\")\n",
    "print(f\"  Labels shape: {labels.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Use cases for custom collate_fn:\")\n",
    "print(\"  1. Text/NLP: Pad sequences to same length, create attention masks\")\n",
    "print(\"  2. Object detection: Handle variable number of objects per image\")\n",
    "print(\"  3. Graph data: Batch graphs of different sizes\")\n",
    "print(\"  4. Multi-modal: Combine images, text, and metadata\")\n",
    "print()\n",
    "\n",
    "print(\"Example: Padding sequences for text\")\n",
    "print()\n",
    "print(\"def text_collate_fn(batch):\")\n",
    "print(\"    texts, labels = zip(*batch)\")\n",
    "print(\"    # Pad all sequences to length of longest sequence\")\n",
    "print(\"    lengths = [len(text) for text in texts]\")\n",
    "print(\"    max_length = max(lengths)\")\n",
    "print(\"    padded = torch.zeros(len(texts), max_length, dtype=torch.long)\")\n",
    "print(\"    for i, text in enumerate(texts):\")\n",
    "print(\"        padded[i, :lengths[i]] = text\")\n",
    "print(\"    return padded, torch.tensor(labels)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20a75bf",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 11: DATALOADER BEST PRACTICES & DEBUGGING\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020239ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 11: DATALOADER BEST PRACTICES & DEBUGGING\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"BEST PRACTICES:\")\n",
    "print()\n",
    "\n",
    "print(\"1. Memory Management:\")\n",
    "print(\"   \u2022 Use pin_memory=True when training on GPU\")\n",
    "print(\"   \u2022 Don't load all data into RAM at once (use Dataset correctly)\")\n",
    "print(\"   \u2022 Clear cache: torch.cuda.empty_cache() if needed\")\n",
    "print()\n",
    "\n",
    "print(\"2. Performance:\")\n",
    "print(\"   \u2022 Use num_workers=2-4 for parallel loading\")\n",
    "print(\"   \u2022 Use persistent_workers=True for faster epoch transitions\")\n",
    "print(\"   \u2022 Profile with torch.utils.bottleneck to find slowdowns\")\n",
    "print()\n",
    "\n",
    "print(\"3. Shuffling:\")\n",
    "print(\"   \u2022 Always shuffle training data (shuffle=True)\")\n",
    "print(\"   \u2022 Never shuffle validation/test data (shuffle=False)\")\n",
    "print(\"   \u2022 Use generator seed for reproducible splits\")\n",
    "print()\n",
    "\n",
    "print(\"4. Batch Sizes:\")\n",
    "print(\"   \u2022 Start with 32 or 64\")\n",
    "print(\"   \u2022 Use powers of 2 (8, 16, 32, 64, 128, 256)\")\n",
    "print(\"   \u2022 Larger batches for evaluation (no gradients stored)\")\n",
    "print(\"   \u2022 Use drop_last=True for training (consistent batch sizes)\")\n",
    "print()\n",
    "\n",
    "print(\"5. Data Augmentation:\")\n",
    "print(\"   \u2022 Only augment training data\")\n",
    "print(\"   \u2022 Use deterministic transforms for val/test\")\n",
    "print(\"   \u2022 Don't go overboard - augmentation != better\")\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"COMMON ISSUES & SOLUTIONS:\")\n",
    "print()\n",
    "\n",
    "print(\"Issue: DataLoader is slow\")\n",
    "print(\"  \u2192 Increase num_workers (try 2, 4)\")\n",
    "print(\"  \u2192 Use pin_memory=True\")\n",
    "print(\"  \u2192 Profile __getitem__() - is it doing too much?\")\n",
    "print(\"  \u2192 Use faster storage (SSD vs HDD)\")\n",
    "print()\n",
    "\n",
    "print(\"Issue: Running out of memory\")\n",
    "print(\"  \u2192 Reduce batch_size\")\n",
    "print(\"  \u2192 Use gradient accumulation instead\")\n",
    "print(\"  \u2192 Reduce image resolution\")\n",
    "print(\"  \u2192 Use mixed precision training\")\n",
    "print()\n",
    "\n",
    "print(\"Issue: Training is not reproducible\")\n",
    "print(\"  \u2192 Set all random seeds (torch, numpy, random)\")\n",
    "print(\"  \u2192 Use generator seed in random_split()\")\n",
    "print(\"  \u2192 Set torch.backends.cudnn.deterministic=True\")\n",
    "print(\"  \u2192 Note: num_workers > 0 can affect reproducibility\")\n",
    "print()\n",
    "\n",
    "print(\"Issue: Multiprocessing errors on Windows\")\n",
    "print(\"  \u2192 Set num_workers=0\")\n",
    "print(\"  \u2192 Put DataLoader code in if __name__ == '__main__':\")\n",
    "print(\"  \u2192 Or use persistent_workers=True\")\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"DEBUGGING TIPS:\")\n",
    "print()\n",
    "\n",
    "print(\"1. Test with batch_size=1 first\")\n",
    "print(\"   Quick way to verify dataset is working\")\n",
    "print()\n",
    "\n",
    "print(\"2. Print shapes frequently\")\n",
    "print(\"   for images, labels in loader:\")\n",
    "print(\"       print(images.shape, labels.shape)\")\n",
    "print(\"       break  # Just check first batch\")\n",
    "print()\n",
    "\n",
    "print(\"3. Visualize batches\")\n",
    "print(\"   Check that data looks correct and transforms work\")\n",
    "print()\n",
    "\n",
    "print(\"4. Time your data loading\")\n",
    "print(\"   If DataLoader is bottleneck, GPU is waiting for data!\")\n",
    "print()\n",
    "\n",
    "print(\"5. Start simple, add complexity gradually\")\n",
    "print(\"   Get basic DataLoader working before adding:\")\n",
    "print(\"   \u2022 Multiple workers\")\n",
    "print(\"   \u2022 Complex transforms\")\n",
    "print(\"   \u2022 Custom collate functions\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2059c3",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 12: SUMMARY & KEY TAKEAWAYS\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cda880",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 12: SUMMARY & KEY TAKEAWAYS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"What we learned:\")\n",
    "print()\n",
    "\n",
    "print(\"1. DATASET CLASS:\")\n",
    "print(\"   \u2022 Implement __init__(), __len__(), __getitem__()\")\n",
    "print(\"   \u2022 Load data on-the-fly (don't load all into memory)\")\n",
    "print(\"   \u2022 Apply transforms in __getitem__()\")\n",
    "print()\n",
    "\n",
    "print(\"2. DATA SPLITTING:\")\n",
    "print(\"   \u2022 Use random_split() with generator seed\")\n",
    "print(\"   \u2022 Standard split: 70% train, 15% val, 15% test\")\n",
    "print(\"   \u2022 Never train on validation or test data!\")\n",
    "print()\n",
    "\n",
    "print(\"3. DATALOADER:\")\n",
    "print(\"   \u2022 Handles batching, shuffling, parallel loading\")\n",
    "print(\"   \u2022 Key parameters: batch_size, shuffle, num_workers\")\n",
    "print(\"   \u2022 Training: shuffle=True, drop_last=True\")\n",
    "print(\"   \u2022 Evaluation: shuffle=False, drop_last=False\")\n",
    "print()\n",
    "\n",
    "print(\"4. PERFORMANCE:\")\n",
    "print(\"   \u2022 Use num_workers=2-4 for speedup\")\n",
    "print(\"   \u2022 Use pin_memory=True for GPU training\")\n",
    "print(\"   \u2022 Larger batch sizes for evaluation\")\n",
    "print(\"   \u2022 Profile and optimize __getitem__()\")\n",
    "print()\n",
    "\n",
    "print(\"5. BEST PRACTICES:\")\n",
    "print(\"   \u2022 Different transforms for train vs eval\")\n",
    "print(\"   \u2022 Set random seeds for reproducibility\")\n",
    "print(\"   \u2022 Start simple, add complexity gradually\")\n",
    "print(\"   \u2022 Monitor data loading speed\")\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"TYPICAL DATALOADER SETUP:\")\n",
    "print()\n",
    "print(\"# Training\")\n",
    "print(\"train_loader = DataLoader(\")\n",
    "print(\"    train_dataset,\")\n",
    "print(\"    batch_size=64,\")\n",
    "print(\"    shuffle=True,\")\n",
    "print(\"    num_workers=2,\")\n",
    "print(\"    pin_memory=True,\")\n",
    "print(\"    drop_last=True\")\n",
    "print(\")\")\n",
    "print()\n",
    "print(\"# Evaluation\")\n",
    "print(\"val_loader = DataLoader(\")\n",
    "print(\"    val_dataset,\")\n",
    "print(\"    batch_size=128,\")\n",
    "print(\"    shuffle=False,\")\n",
    "print(\"    num_workers=2,\")\n",
    "print(\"    pin_memory=True,\")\n",
    "print(\"    drop_last=False\")\n",
    "print(\")\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b1a21",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "PRACTICE PROBLEMS\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PRACTICE PROBLEMS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"1. BASIC CUSTOM DATASET:\")\n",
    "print(\"   Create a custom Dataset for MNIST-style data (28x28 grayscale images)\")\n",
    "print(\"   where you generate random images on-the-fly.\")\n",
    "print(\"   \u2022 num_samples=10000\")\n",
    "print(\"   \u2022 10 classes\")\n",
    "print(\"   \u2022 Images: random values 0-255\")\n",
    "print(\"   \u2022 Apply transforms: ToTensor, Normalize\")\n",
    "print()\n",
    "\n",
    "print(\"2. DATA SPLITTING EXPERIMENT:\")\n",
    "print(\"   Split a dataset of 1000 samples into:\")\n",
    "print(\"   \u2022 60% train, 20% val, 20% test\")\n",
    "print(\"   Create DataLoaders and verify the split is reproducible\")\n",
    "print(\"   (run twice, check same samples in each split)\")\n",
    "print()\n",
    "\n",
    "print(\"3. BATCH SIZE COMPARISON:\")\n",
    "print(\"   Time data loading with batch_sizes [4, 8, 16, 32, 64, 128]\")\n",
    "print(\"   Plot: batch_size vs time_per_batch\")\n",
    "print(\"   Calculate: total_time_per_epoch for each\")\n",
    "print()\n",
    "\n",
    "print(\"4. NUM_WORKERS SPEEDUP:\")\n",
    "print(\"   Measure speedup with num_workers [0, 1, 2, 4, 8]\")\n",
    "print(\"   Plot: num_workers vs batches_per_second\")\n",
    "print(\"   Find optimal num_workers for your system\")\n",
    "print()\n",
    "\n",
    "print(\"5. CUSTOM COLLATE FUNCTION:\")\n",
    "print(\"   Create a Dataset with variable-length sequences (lists of different sizes)\")\n",
    "print(\"   Write a collate_fn that:\")\n",
    "print(\"   \u2022 Pads all sequences to max length in batch\")\n",
    "print(\"   \u2022 Returns padded sequences + original lengths\")\n",
    "print(\"   \u2022 Returns batch as (padded_seqs, lengths, labels)\")\n",
    "print()\n",
    "\n",
    "print(\"6. AUGMENTATION COMPARISON:\")\n",
    "print(\"   Create two DataLoaders for same data:\")\n",
    "print(\"   \u2022 One with heavy augmentation\")\n",
    "print(\"   \u2022 One with no augmentation\")\n",
    "print(\"   Visualize 10 samples from each to see difference\")\n",
    "print()\n",
    "\n",
    "print(\"7. MEMORY EFFICIENT LOADING:\")\n",
    "print(\"   Create a Dataset that simulates large images (e.g., 4096x4096)\")\n",
    "print(\"   Measure memory usage with different batch sizes\")\n",
    "print(\"   Find maximum batch_size before running out of memory\")\n",
    "print()\n",
    "\n",
    "print(\"8. SHUFFLING VERIFICATION:\")\n",
    "print(\"   Verify that shuffle=True actually randomizes data\")\n",
    "print(\"   \u2022 Load first batch from 5 consecutive epochs\")\n",
    "print(\"   \u2022 Check that labels are different each epoch\")\n",
    "print(\"   \u2022 Calculate overlap percentage between epochs\")\n",
    "print()\n",
    "\n",
    "print(\"9. COMPLETE PIPELINE:\")\n",
    "print(\"   Build a complete train/val/test pipeline for image classification:\")\n",
    "print(\"   \u2022 Custom Dataset class\")\n",
    "print(\"   \u2022 70/15/15 split\")\n",
    "print(\"   \u2022 Different transforms for train/eval\")\n",
    "print(\"   \u2022 Proper DataLoaders\")\n",
    "print(\"   \u2022 Training loop skeleton (no actual training)\")\n",
    "print()\n",
    "\n",
    "print(\"10. PERFORMANCE PROFILING:\")\n",
    "print(\"    Profile your DataLoader to find bottlenecks:\")\n",
    "print(\"    \u2022 Time __getitem__() for 100 samples\")\n",
    "print(\"    \u2022 Time data loading vs model forward pass\")\n",
    "print(\"    \u2022 Identify slowest operation\")\n",
    "print(\"    \u2022 Suggest optimization\")\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"BONUS CHALLENGE:\")\n",
    "print()\n",
    "print(\"Create a DataLoader for a multi-modal dataset:\")\n",
    "print(\"  \u2022 Each sample has: image, text caption, numerical metadata\")\n",
    "print(\"  \u2022 Implement custom __getitem__() to return all three\")\n",
    "print(\"  \u2022 Implement custom collate_fn to batch all modalities\")\n",
    "print(\"  \u2022 Handle variable-length text with padding\")\n",
    "print(\"  \u2022 Test with batch_size=16\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"END OF LESSON 09: DATALOADERS & BATCHING\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Next lesson: Building Neural Networks for FashionMNIST\")\n",
    "print(\"You're now ready to efficiently load data for training!\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}