{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Gradients - Code Exercise\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "LESSON 5: PyTorch Tensor Derivatives and Gradients\n",
    "================================================================================\n",
    "\n",
    "Source: Based on TK's article on PyTorch tensor gradients\n",
    "https://www.freecodecamp.org/news/pytorch-tutorial-for-deep-learning/\n",
    "\n",
    "This lesson covers:\n",
    "- What gradients are and why they're essential for deep learning\n",
    "- Creating tensors that track gradients (requires_grad=True)\n",
    "- Computing gradients using backward()\n",
    "- Understanding the computational graph\n",
    "- Multi-variable gradient computation\n",
    "- Gradient accumulation and zeroing\n",
    "- Using torch.no_grad() for inference\n",
    "\n",
    "Author: Machine Learning Refresher Course\n",
    "Date: 2025-12-02\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3095741",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PYTORCH TENSOR GRADIENTS AND DERIVATIVES\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f85a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: INTRODUCTION TO GRADIENTS\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 1: WHY GRADIENTS MATTER IN MACHINE LEARNING\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075e92a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Gradients are the foundation of how neural networks learn!\")\n",
    "print()\n",
    "print(\"Key Concepts:\")\n",
    "print(\"  \u2022 A gradient is the derivative of a function with respect to its inputs\")\n",
    "print(\"  \u2022 In ML, we use gradients to optimize our model's parameters\")\n",
    "print(\"  \u2022 Gradients tell us HOW to adjust parameters to minimize loss\")\n",
    "print()\n",
    "print(\"The Learning Process:\")\n",
    "print(\"  1. Forward pass: Make predictions using current parameters\")\n",
    "print(\"  2. Compute loss: Measure how wrong the predictions are\")\n",
    "print(\"  3. Backward pass: Compute gradients (how loss changes w.r.t. parameters)\")\n",
    "print(\"  4. Update parameters: Move in the direction that reduces loss\")\n",
    "print(\"  5. Repeat until the model converges\")\n",
    "print()\n",
    "print(\"PyTorch's autograd system automatically computes these gradients!\")\n",
    "print(\"This is called 'automatic differentiation' or 'autodiff'\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: CREATING TENSORS WITH requires_grad=True\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 2: ENABLING GRADIENT TRACKING\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To compute gradients, we need to tell PyTorch which tensors to track.\")\n",
    "print(\"We do this by setting requires_grad=True when creating a tensor.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor WITHOUT gradient tracking (default behavior)\n",
    "a = torch.tensor(3.0)\n",
    "print(f\"Tensor without gradient tracking: {a}\")\n",
    "print(f\"  requires_grad: {a.requires_grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor WITH gradient tracking\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "print(f\"Tensor with gradient tracking: {b}\")\n",
    "print(f\"  requires_grad: {b.requires_grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce9dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Why do we need requires_grad=True?\")\n",
    "print(\"  \u2022 It tells PyTorch to build a computational graph\")\n",
    "print(\"  \u2022 The graph tracks all operations performed on this tensor\")\n",
    "print(\"  \u2022 This allows automatic gradient computation via backpropagation\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade7395",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# You can also enable gradient tracking on existing tensors\n",
    "c = torch.tensor(5.0)\n",
    "c.requires_grad_(True)  # Note: underscore means in-place operation\n",
    "print(f\"Enabled gradient tracking on existing tensor: {c}\")\n",
    "print(f\"  requires_grad: {c.requires_grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: COMPUTING GRADIENTS WITH backward() - TK'S CORE EXAMPLE\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 3: COMPUTING GRADIENTS - THE CORE EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d178fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Let's work through TK's example step by step:\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TK's exact example from the article\n",
    "print(\"Step 1: Create a tensor with requires_grad=True\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d829bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 2: Build a relationship - let's compute y = x\u00b2\")\n",
    "y = x ** 2\n",
    "print(f\"y = x\u00b2 = {y}\")\n",
    "print(f\"  Notice the grad_fn: {y.grad_fn}\")\n",
    "print(\"  This shows that y was created by a power operation (PowBackward0)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mathematical Background:\")\n",
    "print(\"  \u2022 We have: y = x\u00b2\")\n",
    "print(\"  \u2022 The derivative is: dy/dx = 2x\")\n",
    "print(\"  \u2022 At x = 2.0, the derivative is: 2 \u00d7 2.0 = 4.0\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 3: Compute the gradient using backward()\")\n",
    "y.backward()\n",
    "print(\"Called y.backward() - this computes dy/dx\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 4: Access the computed gradient\")\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"What just happened?\")\n",
    "print(\"  \u2022 PyTorch computed dy/dx = 2x\")\n",
    "print(\"  \u2022 It evaluated this at x = 2.0\")\n",
    "print(\"  \u2022 Result: 2 \u00d7 2.0 = 4.0\")\n",
    "print(\"  \u2022 The gradient is stored in x.grad\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a5920",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"This gradient tells us: if we increase x by a tiny amount,\")\n",
    "print(\"y will increase by approximately 4 times that amount.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: UNDERSTANDING THE COMPUTATIONAL GRAPH\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 4: THE COMPUTATIONAL GRAPH AND grad_fn\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch builds a 'computational graph' to track operations.\")\n",
    "print(\"Each operation creates a node with a grad_fn (gradient function).\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e771d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex computation graph\n",
    "x1 = torch.tensor(3.0, requires_grad=True)\n",
    "x2 = torch.tensor(4.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input tensors:\")\n",
    "print(f\"  x1 = {x1}\")\n",
    "print(f\"  x2 = {x2}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a58223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a computation: z = x1\u00b2 + x2\u00b3\n",
    "y1 = x1 ** 2\n",
    "y2 = x2 ** 3\n",
    "z = y1 + y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17683e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Computations:\")\n",
    "print(f\"  y1 = x1\u00b2 = {y1}\")\n",
    "print(f\"    grad_fn: {y1.grad_fn}\")\n",
    "print(f\"  y2 = x2\u00b3 = {y2}\")\n",
    "print(f\"    grad_fn: {y2.grad_fn}\")\n",
    "print(f\"  z = y1 + y2 = {z}\")\n",
    "print(f\"    grad_fn: {z.grad_fn}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The computational graph looks like this:\")\n",
    "print()\n",
    "print(\"     x1 (3.0)          x2 (4.0)\")\n",
    "print(\"        |                 |\")\n",
    "print(\"    [** 2]            [** 3]\")\n",
    "print(\"        |                 |\")\n",
    "print(\"     y1 (9.0)          y2 (64.0)\")\n",
    "print(\"        |                 |\")\n",
    "print(\"        +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\")\n",
    "print(\"                 |\")\n",
    "print(\"              z (73.0)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d918e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9910816",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Computed gradients:\")\n",
    "print(f\"  dz/dx1 = {x1.grad}  (derivative of x\u00b2 at x=3 is 2\u00d73 = 6)\")\n",
    "print(f\"  dz/dx2 = {x2.grad}  (derivative of x\u00b3 at x=4 is 3\u00d716 = 48)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96426444",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Mathematical verification:\")\n",
    "print(f\"  z = x1\u00b2 + x2\u00b3\")\n",
    "print(f\"  dz/dx1 = 2\u00d7x1 = 2\u00d73 = 6.0 \u2713\")\n",
    "print(f\"  dz/dx2 = 3\u00d7x2\u00b2 = 3\u00d716 = 48.0 \u2713\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: MULTI-VARIABLE GRADIENTS\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 5: GRADIENTS WITH MULTIPLE VARIABLES\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b506ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real neural networks have millions of parameters.\")\n",
    "print(\"Let's see how PyTorch computes gradients for multiple variables.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Linear function z = 3a + 2b\n",
    "a = torch.tensor(5.0, requires_grad=True)\n",
    "b = torch.tensor(7.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ed86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Variables:\")\n",
    "print(f\"  a = {a.item()}\")\n",
    "print(f\"  b = {b.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5858567",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 3 * a + 2 * b\n",
    "print(f\"Function: z = 3a + 2b\")\n",
    "print(f\"  z = 3\u00d7{a.item()} + 2\u00d7{b.item()} = {z.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ab5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradients:\")\n",
    "print(f\"  dz/da = {a.grad.item()} (expected: 3, because coefficient of a is 3)\")\n",
    "print(f\"  dz/db = {b.grad.item()} (expected: 2, because coefficient of b is 2)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23209cdd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Interpretation:\")\n",
    "print(\"  \u2022 If we increase 'a' by 1, z increases by 3\")\n",
    "print(\"  \u2022 If we increase 'b' by 1, z increases by 2\")\n",
    "print(\"  \u2022 This tells us 'a' has more impact on z than 'b'\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: GRADIENT ACCUMULATION AND ZEROING\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 6: GRADIENT ACCUMULATION (AND WHY WE ZERO GRADIENTS)\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270fb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\u26a0\ufe0f  IMPORTANT: PyTorch ACCUMULATES gradients by default!\")\n",
    "print(\"This means calling backward() multiple times ADDS to existing gradients.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d59d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient accumulation\n",
    "x = torch.tensor(2.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be69219",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First computation:\")\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f\"  y1 = x\u00b2 = {y1.item()}\")\n",
    "print(f\"  After first backward(): x.grad = {x.grad.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Second computation (WITHOUT zeroing gradients):\")\n",
    "y2 = x ** 3\n",
    "y2.backward()\n",
    "print(f\"  y2 = x\u00b3 = {y2.item()}\")\n",
    "print(f\"  After second backward(): x.grad = {x.grad.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cfdda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notice that x.grad = 4 + 12 = 16!\")\n",
    "print(\"  \u2022 First backward: dy1/dx = 2x = 4\")\n",
    "print(\"  \u2022 Second backward: dy2/dx = 3x\u00b2 = 12\")\n",
    "print(\"  \u2022 Total accumulated: 4 + 12 = 16\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c495d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is usually NOT what we want in training!\")\n",
    "print(\"Solution: Zero the gradients before each backward pass.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate proper gradient zeroing\n",
    "x = torch.tensor(2.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad1775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proper approach:\")\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f\"  After first backward(): x.grad = {x.grad.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0965b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  Zero the gradient:\")\n",
    "x.grad.zero_()  # Zero the gradient before next computation\n",
    "print(f\"  After zero_(): x.grad = {x.grad.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = x ** 3\n",
    "y2.backward()\n",
    "print(f\"  After second backward(): x.grad = {x.grad.item()}\")\n",
    "print(\"  \u2713 Now we have the correct gradient for y2!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1014fe5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"In PyTorch training loops, you'll often see:\")\n",
    "print(\"  optimizer.zero_grad()  # Zero gradients\")\n",
    "print(\"  loss.backward()        # Compute gradients\")\n",
    "print(\"  optimizer.step()       # Update parameters\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: torch.no_grad() CONTEXT\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 7: DISABLING GRADIENT COMPUTATION WITH torch.no_grad()\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sometimes we don't need gradients (e.g., during inference/testing).\")\n",
    "print(\"Using torch.no_grad() saves memory and speeds up computation.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc501ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WITH gradient tracking:\")\n",
    "y1 = x ** 2\n",
    "print(f\"  y1 = {y1}\")\n",
    "print(f\"  y1.requires_grad = {y1.requires_grad}\")\n",
    "print(f\"  y1.grad_fn = {y1.grad_fn}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07db478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WITHOUT gradient tracking (using torch.no_grad()):\")\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 2\n",
    "    print(f\"  y2 = {y2}\")\n",
    "    print(f\"  y2.requires_grad = {y2.requires_grad}\")\n",
    "    print(f\"  y2.grad_fn = {y2.grad_fn}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce5e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"When to use torch.no_grad():\")\n",
    "print(\"  \u2713 During model evaluation/testing\")\n",
    "print(\"  \u2713 When making predictions on new data\")\n",
    "print(\"  \u2713 When computing metrics that don't need gradients\")\n",
    "print(\"  \u2717 During training (we need gradients to learn!)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2732301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate memory savings\n",
    "x = torch.randn(1000, 1000, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55394301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memory comparison (1000\u00d71000 tensor):\")\n",
    "y_with_grad = x ** 2 + x ** 3\n",
    "print(f\"  With gradient tracking: grad_fn exists: {y_with_grad.grad_fn is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb299fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_without_grad = x ** 2 + x ** 3\n",
    "    print(f\"  Without gradient tracking: grad_fn exists: {y_without_grad.grad_fn is not None}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2287b2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"The version without gradients uses less memory and computes faster!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20617b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: PRACTICE PROBLEMS\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 8: PRACTICE PROBLEMS\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec03592",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Now it's your turn! Try solving these problems.\")\n",
    "print(\"Uncomment the code and fill in the blanks.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f10f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 80)\n",
    "print(\"PROBLEM 1: Basic Gradient Computation\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "print(\"Given: f(x) = 3x\u00b2 + 2x + 1\")\n",
    "print(\"Find: df/dx at x = 4.0\")\n",
    "print()\n",
    "print(\"Expected: df/dx = 6x + 2 = 6(4) + 2 = 26\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "f = 3 * x**2 + 2 * x + 1\n",
    "f.backward()\n",
    "print(f\"Solution: df/dx = {x.grad.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8162ccf3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Uncomment to practice:\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "f = # TODO: Implement f(x) = 3x\u00b2 + 2x + 1\n",
    "# TODO: Compute the gradient\n",
    "print(f\"Your answer: df/dx = {x.grad.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c72d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 80)\n",
    "print(\"PROBLEM 2: Multi-Variable Gradients\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "print(\"Given: g(x, y) = x\u00b2y + y\u00b3\")\n",
    "print(\"Find: dg/dx and dg/dy at x = 2.0, y = 3.0\")\n",
    "print()\n",
    "print(\"Expected:\")\n",
    "print(\"  dg/dx = 2xy = 2(2)(3) = 12\")\n",
    "print(\"  dg/dy = x\u00b2 + 3y\u00b2 = 4 + 27 = 31\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "g = x**2 * y + y**3\n",
    "g.backward()\n",
    "print(f\"Solution: dg/dx = {x.grad.item()}, dg/dy = {y.grad.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e50091",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Uncomment to practice:\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "g = # TODO: Implement g(x, y) = x\u00b2y + y\u00b3\n",
    "# TODO: Compute gradients\n",
    "print(f\"Your answer: dg/dx = {x.grad.item()}, dg/dy = {y.grad.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7275ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 80)\n",
    "print(\"PROBLEM 3: Chain Rule in Action\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "print(\"Given: h(x) = (2x + 3)\u00b2\")\n",
    "print(\"Find: dh/dx at x = 1.0\")\n",
    "print()\n",
    "print(\"Expected:\")\n",
    "print(\"  Let u = 2x + 3, then h = u\u00b2\")\n",
    "print(\"  dh/dx = dh/du \u00d7 du/dx = 2u \u00d7 2 = 4u = 4(2x + 3)\")\n",
    "print(\"  At x = 1: dh/dx = 4(2(1) + 3) = 4(5) = 20\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdacbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "h = (2 * x + 3) ** 2\n",
    "h.backward()\n",
    "print(f\"Solution: dh/dx = {x.grad.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f4ba9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Uncomment to practice:\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "h = # TODO: Implement h(x) = (2x + 3)\u00b2\n",
    "# TODO: Compute the gradient\n",
    "print(f\"Your answer: dh/dx = {x.grad.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ae2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY AND KEY TAKEAWAYS\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"LESSON SUMMARY: KEY TAKEAWAYS\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83c\udfaf Key Concepts Covered:\")\n",
    "print()\n",
    "print(\"1. GRADIENTS are the foundation of neural network learning\")\n",
    "print(\"   \u2022 They tell us how to adjust parameters to reduce loss\")\n",
    "print()\n",
    "print(\"2. requires_grad=True enables gradient tracking\")\n",
    "print(\"   \u2022 PyTorch builds a computational graph of operations\")\n",
    "print()\n",
    "print(\"3. .backward() computes gradients automatically\")\n",
    "print(\"   \u2022 Uses automatic differentiation (autograd)\")\n",
    "print(\"   \u2022 Gradients stored in .grad attribute\")\n",
    "print()\n",
    "print(\"4. GRADIENT ACCUMULATION: PyTorch adds gradients by default\")\n",
    "print(\"   \u2022 Always zero gradients in training loops\")\n",
    "print(\"   \u2022 Use .zero_() or optimizer.zero_grad()\")\n",
    "print()\n",
    "print(\"5. torch.no_grad() for inference\")\n",
    "print(\"   \u2022 Disables gradient tracking\")\n",
    "print(\"   \u2022 Saves memory and speeds up computation\")\n",
    "print()\n",
    "print(\"6. The COMPUTATIONAL GRAPH tracks all operations\")\n",
    "print(\"   \u2022 Each operation has a grad_fn\")\n",
    "print(\"   \u2022 Enables backpropagation through complex models\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b55b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"\u2022 Experiment with different mathematical functions\")\n",
    "print(\"\u2022 Try computing gradients of complex expressions\")\n",
    "print(\"\u2022 Practice interpreting what gradients mean in context\")\n",
    "print(\"\u2022 Move on to using gradients in actual neural network training!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46236512",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"END OF LESSON 5: TENSOR GRADIENTS AND DERIVATIVES\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}