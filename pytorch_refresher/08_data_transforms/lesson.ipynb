{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Data Transforms - Code Exercise\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b56b27",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "================================================================================\n",
    "LESSON 08: DATA TRANSFORMS IN PYTORCH\n",
    "================================================================================\n",
    "\n",
    "Topics Covered:\n",
    "1. Understanding torchvision.transforms\n",
    "2. Basic transforms (Resize, Crop, ToTensor, Normalize)\n",
    "3. Data augmentation transforms (Flip, Rotate, ColorJitter, etc.)\n",
    "4. Composing transforms together\n",
    "5. Training vs Validation transforms\n",
    "6. Debugging transforms step-by-step\n",
    "\n",
    "Learning Objectives:\n",
    "- Understand how transforms preprocess images for neural networks\n",
    "- Learn the difference between deterministic preprocessing and augmentation\n",
    "- Master the art of composing transform pipelines\n",
    "- Debug and visualize what transforms do to your data\n",
    "\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1693296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LESSON 08: DATA TRANSFORMS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision available: {True}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33732de8",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 1: LOADING A SAMPLE IMAGE\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8465ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 1: LOADING A SAMPLE IMAGE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Define paths\n",
    "data_dir = Path(\"/Users/zack/dev/ml-refresher/data/oxford_flowers\")\n",
    "image_dir = data_dir / \"jpg\"\n",
    "output_dir = data_dir\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Looking for images in: {image_dir}\")\n",
    "print()\n",
    "\n",
    "# Try to load an image from the Oxford Flowers dataset\n",
    "if image_dir.exists() and list(image_dir.glob(\"*.jpg\")):\n",
    "    # Get a sample image (use the first one)\n",
    "    sample_image_path = list(image_dir.glob(\"*.jpg\"))[0]\n",
    "    print(f\"\u2713 Found Oxford Flowers dataset!\")\n",
    "    print(f\"  Loading sample image: {sample_image_path.name}\")\n",
    "\n",
    "    # Load the image using PIL\n",
    "    original_image = Image.open(sample_image_path)\n",
    "\n",
    "else:\n",
    "    print(\"! Oxford Flowers dataset not found. Creating synthetic image...\")\n",
    "\n",
    "    # Create a synthetic colorful image (RGB gradient)\n",
    "    # This creates a nice gradient from red to blue\n",
    "    width, height = 500, 500\n",
    "    img_array = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Create a radial gradient pattern with colors\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # Calculate distance from center\n",
    "            center_x, center_y = width // 2, height // 2\n",
    "            dist = np.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
    "            max_dist = np.sqrt(center_x**2 + center_y**2)\n",
    "\n",
    "            # Create RGB values based on position\n",
    "            img_array[y, x, 0] = int(255 * (x / width))  # Red increases left to right\n",
    "            img_array[y, x, 1] = int(255 * (y / height))  # Green increases top to bottom\n",
    "            img_array[y, x, 2] = int(255 * (1 - dist / max_dist))  # Blue decreases from center\n",
    "\n",
    "    original_image = Image.fromarray(img_array)\n",
    "    print(\"  Created synthetic 500x500 RGB image with gradient pattern\")\n",
    "\n",
    "print()\n",
    "print(f\"Image format: {original_image.format or 'N/A'}\")\n",
    "print(f\"Image mode: {original_image.mode}\")  # Should be RGB\n",
    "print(f\"Image size: {original_image.size}\")  # (width, height)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"KEY CONCEPT: PIL Images vs Tensors\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "PIL (Python Imaging Library) Images:\n",
    "  - Size format: (width, height)\n",
    "  - Pixel access: image[x, y]\n",
    "  - Value range: 0-255 (uint8)\n",
    "  - Color order: RGB\n",
    "  - Used by: Human viewing, file I/O\n",
    "\n",
    "PyTorch Tensors:\n",
    "  - Shape format: (Channels, Height, Width) = (C, H, W)\n",
    "  - Indexing: tensor[c, h, w]\n",
    "  - Value range: 0.0-1.0 (float32)\n",
    "  - Color order: RGB\n",
    "  - Used by: Neural networks, GPU computation\n",
    "\n",
    "Transforms bridge these two worlds!\n",
    "\"\"\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a50b2",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 2: BASIC TRANSFORMS - RESIZE\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e468499",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 2: BASIC TRANSFORMS - RESIZE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Transform: transforms.Resize()\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Purpose: Resize images to a specific size\n",
    "Why it's needed:\n",
    "  - Neural networks expect fixed input sizes\n",
    "  - Batch processing requires uniform dimensions\n",
    "  - Control computational cost\n",
    "\n",
    "Usage:\n",
    "  - Resize(size): if size is int, resize shorter edge to that size\n",
    "  - Resize((height, width)): resize to exact dimensions\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "# Create resize transform\n",
    "resize_transform = transforms.Resize((224, 224))  # Standard ImageNet size\n",
    "\n",
    "print(\"Applying: transforms.Resize((224, 224))\")\n",
    "print(f\"Original size: {original_image.size} (width \u00d7 height)\")\n",
    "\n",
    "resized_image = resize_transform(original_image)\n",
    "\n",
    "print(f\"Resized size:  {resized_image.size} (width \u00d7 height)\")\n",
    "print()\n",
    "\n",
    "# Visualize the resize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title(f\"Original\\nSize: {original_image.size[0]}\u00d7{original_image.size[1]}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(resized_image)\n",
    "axes[1].set_title(f\"Resized\\nSize: {resized_image.size[0]}\u00d7{resized_image.size[1]}\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(\"Transform: Resize\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_01_resize.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n",
    "\n",
    "print(\"\ud83d\udca1 TIP: Different resize options:\")\n",
    "print(\"   - Resize(256): Resize shortest edge to 256, keep aspect ratio\")\n",
    "print(\"   - Resize((224, 224)): Exact size, may distort aspect ratio\")\n",
    "print(\"   - Resize((224, 224), antialias=True): Better quality (newer PyTorch)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e627d98",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 3: BASIC TRANSFORMS - CENTER CROP\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ec63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 3: BASIC TRANSFORMS - CENTER CROP\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Transform: transforms.CenterCrop()\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Purpose: Crop the center portion of an image\n",
    "Why it's useful:\n",
    "  - Focus on the central subject\n",
    "  - Remove borders/edges\n",
    "  - Often combined with Resize\n",
    "\n",
    "Common pattern:\n",
    "  1. Resize to slightly larger (e.g., 256)\n",
    "  2. CenterCrop to target size (e.g., 224)\n",
    "  This gives better quality than direct resize!\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "# First resize to larger, then center crop\n",
    "resize_256 = transforms.Resize((256, 256))\n",
    "center_crop = transforms.CenterCrop((224, 224))\n",
    "\n",
    "print(\"Two-step process:\")\n",
    "print(\"  Step 1: Resize to (256, 256)\")\n",
    "image_256 = resize_256(original_image)\n",
    "print(f\"    \u2192 Size: {image_256.size}\")\n",
    "\n",
    "print(\"  Step 2: CenterCrop to (224, 224)\")\n",
    "cropped_image = center_crop(image_256)\n",
    "print(f\"    \u2192 Size: {cropped_image.size}\")\n",
    "print()\n",
    "\n",
    "# Visualize the crop\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title(f\"Original\\n{original_image.size[0]}\u00d7{original_image.size[1]}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(image_256)\n",
    "axes[1].set_title(f\"Resized\\n{image_256.size[0]}\u00d7{image_256.size[1]}\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(cropped_image)\n",
    "axes[2].set_title(f\"Center Cropped\\n{cropped_image.size[0]}\u00d7{cropped_image.size[1]}\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Transform: Resize \u2192 Center Crop\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_02_center_crop.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b0fbf3",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 4: BASIC TRANSFORMS - TO TENSOR\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 4: BASIC TRANSFORMS - TO TENSOR\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Transform: transforms.ToTensor()\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Purpose: Convert PIL Image to PyTorch Tensor\n",
    "This is THE MOST IMPORTANT transform!\n",
    "\n",
    "What it does:\n",
    "  1. Converts PIL Image (H, W, C) to Tensor (C, H, W)\n",
    "  2. Converts uint8 [0, 255] to float32 [0.0, 1.0]\n",
    "  3. Prepares data for neural network input\n",
    "\n",
    "The conversion formula: tensor = pil_image / 255.0\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "# Use our resized image for this demo\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "print(\"Before ToTensor():\")\n",
    "print(f\"  Type: {type(resized_image)}\")\n",
    "print(f\"  Mode: {resized_image.mode}\")\n",
    "print(f\"  Size: {resized_image.size} (W, H)\")\n",
    "print(f\"  Pixel value at (100, 100): {resized_image.getpixel((100, 100))}\")\n",
    "print(f\"    \u2192 This is a tuple of (R, G, B) values in range [0, 255]\")\n",
    "print()\n",
    "\n",
    "print(\"Applying: transforms.ToTensor()\")\n",
    "tensor_image = to_tensor(resized_image)\n",
    "\n",
    "print()\n",
    "print(\"After ToTensor():\")\n",
    "print(f\"  Type: {type(tensor_image)}\")\n",
    "print(f\"  Shape: {tensor_image.shape} (C, H, W)\")\n",
    "print(f\"  Dtype: {tensor_image.dtype}\")\n",
    "print(f\"  Device: {tensor_image.device}\")\n",
    "print(f\"  Min value: {tensor_image.min().item():.4f}\")\n",
    "print(f\"  Max value: {tensor_image.max().item():.4f}\")\n",
    "print(f\"  Mean value: {tensor_image.mean().item():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Value at position [channel=0, height=100, width=100]:\")\n",
    "print(f\"  {tensor_image[0, 100, 100].item():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Verifying the conversion:\")\n",
    "# Get the original PIL pixel value\n",
    "pil_pixel = resized_image.getpixel((100, 100))\n",
    "print(f\"  PIL pixel at (100, 100): R={pil_pixel[0]}, G={pil_pixel[1]}, B={pil_pixel[2]}\")\n",
    "\n",
    "# Calculate expected tensor values\n",
    "expected_r = pil_pixel[0] / 255.0\n",
    "expected_g = pil_pixel[1] / 255.0\n",
    "expected_b = pil_pixel[2] / 255.0\n",
    "\n",
    "print(f\"  Expected tensor values: R={expected_r:.4f}, G={expected_g:.4f}, B={expected_b:.4f}\")\n",
    "\n",
    "# Get actual tensor values\n",
    "actual_r = tensor_image[0, 100, 100].item()\n",
    "actual_g = tensor_image[1, 100, 100].item()\n",
    "actual_b = tensor_image[2, 100, 100].item()\n",
    "\n",
    "print(f\"  Actual tensor values:   R={actual_r:.4f}, G={actual_g:.4f}, B={actual_b:.4f}\")\n",
    "print(f\"  \u2713 Match: {np.allclose([expected_r, expected_g, expected_b], [actual_r, actual_g, actual_b])}\")\n",
    "print()\n",
    "\n",
    "# Visualize the tensor channels\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(resized_image)\n",
    "axes[0, 0].set_title(\"Original Image (RGB)\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Red channel\n",
    "axes[0, 1].imshow(tensor_image[0].numpy(), cmap='Reds')\n",
    "axes[0, 1].set_title(f\"Red Channel\\nRange: [{tensor_image[0].min():.3f}, {tensor_image[0].max():.3f}]\")\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Green channel\n",
    "axes[1, 0].imshow(tensor_image[1].numpy(), cmap='Greens')\n",
    "axes[1, 0].set_title(f\"Green Channel\\nRange: [{tensor_image[1].min():.3f}, {tensor_image[1].max():.3f}]\")\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Blue channel\n",
    "axes[1, 1].imshow(tensor_image[2].numpy(), cmap='Blues')\n",
    "axes[1, 1].set_title(f\"Blue Channel\\nRange: [{tensor_image[2].min():.3f}, {tensor_image[2].max():.3f}]\")\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.suptitle(\"Transform: ToTensor() - Channel Visualization\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_03_to_tensor.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28408a",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 5: BASIC TRANSFORMS - NORMALIZE\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d8c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 5: BASIC TRANSFORMS - NORMALIZE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Transform: transforms.Normalize()\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Purpose: Normalize tensor with mean and standard deviation\n",
    "Why it's crucial:\n",
    "  - Centers data around zero (easier to train)\n",
    "  - Scales data to similar ranges (faster convergence)\n",
    "  - Uses dataset statistics (ImageNet is common)\n",
    "\n",
    "Formula: output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\n",
    "Common ImageNet normalization:\n",
    "  mean = [0.485, 0.456, 0.406]  # RGB means\n",
    "  std  = [0.229, 0.224, 0.225]  # RGB standard deviations\n",
    "\n",
    "After normalization:\n",
    "  - Values typically in range [-2, 2]\n",
    "  - Mean \u2248 0, Std \u2248 1 for each channel\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "# ImageNet normalization statistics\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "normalize = transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "\n",
    "print(f\"Before Normalize:\")\n",
    "print(f\"  Shape: {tensor_image.shape}\")\n",
    "print(f\"  Per-channel statistics:\")\n",
    "for i, color in enumerate(['Red', 'Green', 'Blue']):\n",
    "    channel = tensor_image[i]\n",
    "    print(f\"    {color:5s}: mean={channel.mean():.4f}, std={channel.std():.4f}, \"\n",
    "          f\"range=[{channel.min():.4f}, {channel.max():.4f}]\")\n",
    "print()\n",
    "\n",
    "print(\"Applying: transforms.Normalize(mean=ImageNet_mean, std=ImageNet_std)\")\n",
    "normalized_tensor = normalize(tensor_image.clone())  # Clone to keep original\n",
    "\n",
    "print()\n",
    "print(f\"After Normalize:\")\n",
    "print(f\"  Shape: {normalized_tensor.shape}\")\n",
    "print(f\"  Per-channel statistics:\")\n",
    "for i, color in enumerate(['Red', 'Green', 'Blue']):\n",
    "    channel = normalized_tensor[i]\n",
    "    print(f\"    {color:5s}: mean={channel.mean():.4f}, std={channel.std():.4f}, \"\n",
    "          f\"range=[{channel.min():.4f}, {channel.max():.4f}]\")\n",
    "print()\n",
    "\n",
    "print(\"Understanding the math:\")\n",
    "print(\"  For each channel c and pixel (h, w):\")\n",
    "print(\"    normalized[c,h,w] = (tensor[c,h,w] - mean[c]) / std[c]\")\n",
    "print()\n",
    "print(\"  Example for Red channel at pixel (100, 100):\")\n",
    "original_val = tensor_image[0, 100, 100].item()\n",
    "normalized_val = normalized_tensor[0, 100, 100].item()\n",
    "expected_val = (original_val - imagenet_mean[0]) / imagenet_std[0]\n",
    "\n",
    "print(f\"    Original value:  {original_val:.4f}\")\n",
    "print(f\"    Mean (Red):      {imagenet_mean[0]:.4f}\")\n",
    "print(f\"    Std (Red):       {imagenet_std[0]:.4f}\")\n",
    "print(f\"    Calculation:     ({original_val:.4f} - {imagenet_mean[0]:.4f}) / {imagenet_std[0]:.4f}\")\n",
    "print(f\"    Expected result: {expected_val:.4f}\")\n",
    "print(f\"    Actual result:   {normalized_val:.4f}\")\n",
    "print(f\"    \u2713 Match: {np.isclose(expected_val, normalized_val)}\")\n",
    "print()\n",
    "\n",
    "# Visualize the normalization effect\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original tensor channels\n",
    "for i, (color, cmap) in enumerate(zip(['Red', 'Green', 'Blue'], ['Reds', 'Greens', 'Blues'])):\n",
    "    axes[0, i].imshow(tensor_image[i].numpy(), cmap=cmap, vmin=0, vmax=1)\n",
    "    axes[0, i].set_title(f\"Before: {color} Channel\\nRange: [0, 1]\")\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Normalized tensor channels (need to rescale for visualization)\n",
    "for i, (color, cmap) in enumerate(zip(['Red', 'Green', 'Blue'], ['Reds', 'Greens', 'Blues'])):\n",
    "    axes[1, i].imshow(normalized_tensor[i].numpy(), cmap=cmap)\n",
    "    axes[1, i].set_title(f\"After: {color} Channel\\n\"\n",
    "                         f\"Range: [{normalized_tensor[i].min():.2f}, {normalized_tensor[i].max():.2f}]\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Transform: Normalize() - Before and After\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_04_normalize.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n",
    "\n",
    "print(\"\ud83d\udca1 IMPORTANT: When to use ImageNet normalization?\")\n",
    "print(\"   - Using pre-trained models (ResNet, VGG, etc.): YES\")\n",
    "print(\"   - Training from scratch on ImageNet: YES\")\n",
    "print(\"   - Training from scratch on other data: Compute your own statistics!\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a21df",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 6: DATA AUGMENTATION - RANDOM HORIZONTAL FLIP\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38743560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 6: DATA AUGMENTATION - RANDOM HORIZONTAL FLIP\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Transform: transforms.RandomHorizontalFlip()\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Purpose: Randomly flip images horizontally (left \u2194 right)\n",
    "Why it's useful:\n",
    "  - Doubles effective dataset size\n",
    "  - Object orientation shouldn't matter (for most tasks)\n",
    "  - Cheap and effective augmentation\n",
    "\n",
    "Parameters:\n",
    "  - p: Probability of flipping (default=0.5)\n",
    "\n",
    "\u26a0\ufe0f  IMPORTANT: This is RANDOM! Each call may give different results.\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "random_flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "\n",
    "print(\"Applying RandomHorizontalFlip 5 times to the same image:\")\n",
    "print(\"(Notice: Some flipped, some not - it's random!)\")\n",
    "print()\n",
    "\n",
    "# Apply multiple times to show randomness\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(resized_image)\n",
    "axes[0, 0].set_title(\"Original Image\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Apply 5 times\n",
    "for idx in range(5):\n",
    "    row = (idx + 1) // 3\n",
    "    col = (idx + 1) % 3\n",
    "\n",
    "    flipped = random_flip(resized_image)\n",
    "    axes[row, col].imshow(flipped)\n",
    "\n",
    "    # Check if actually flipped by comparing with original\n",
    "    is_flipped = not np.array_equal(np.array(flipped), np.array(resized_image))\n",
    "    axes[row, col].set_title(f\"Attempt {idx+1}: {'FLIPPED' if is_flipped else 'NOT flipped'}\")\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle(\"Transform: RandomHorizontalFlip(p=0.5)\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_05_random_flip.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n",
    "\n",
    "print(\"\ud83d\udca1 When NOT to use horizontal flip:\")\n",
    "print(\"   - Text recognition (would flip letters!)\")\n",
    "print(\"   - Medical images with side labels (L/R matters)\")\n",
    "print(\"   - Asymmetric objects where orientation matters\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647cdc9c",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 7: DATA AUGMENTATION - RANDOM ROTATION\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6062adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 7: DATA AUGMENTATION - RANDOM ROTATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Transform: transforms.RandomRotation()\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Purpose: Randomly rotate images by a random angle\n",
    "Why it's useful:\n",
    "  - Object orientation varies in real world\n",
    "  - Makes model rotation-invariant\n",
    "  - Good for objects that appear at different angles\n",
    "\n",
    "Parameters:\n",
    "  - degrees: Range of degrees (int or tuple)\n",
    "    - degrees=30 means rotate between -30 and +30 degrees\n",
    "    - degrees=(0, 180) means rotate between 0 and 180 degrees\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "random_rotation = transforms.RandomRotation(degrees=30)\n",
    "\n",
    "print(\"Applying RandomRotation(degrees=30) 5 times:\")\n",
    "print(\"(Will rotate between -30\u00b0 and +30\u00b0 randomly)\")\n",
    "print()\n",
    "\n",
    "# Set seed for reproducibility in this demo\n",
    "torch.manual_seed(42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(resized_image)\n",
    "axes[0, 0].set_title(\"Original Image\\n(No rotation)\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Apply 5 times\n",
    "for idx in range(5):\n",
    "    row = (idx + 1) // 3\n",
    "    col = (idx + 1) % 3\n",
    "\n",
    "    rotated = random_rotation(resized_image)\n",
    "    axes[row, col].imshow(rotated)\n",
    "    axes[row, col].set_title(f\"Random Rotation #{idx+1}\")\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle(\"Transform: RandomRotation(degrees=30)\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_06_random_rotation.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n",
    "\n",
    "print(\"\ud83d\udca1 TIP: Fill parameter\")\n",
    "print(\"   - RandomRotation(degrees=30, fill=255) for white background\")\n",
    "print(\"   - RandomRotation(degrees=30, fill=0) for black background\")\n",
    "print(\"   - Default fills with black (0)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e247ebc",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 8: DATA AUGMENTATION - RANDOM RESIZED CROP\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 8: DATA AUGMENTATION - RANDOM RESIZED CROP\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Transform: transforms.RandomResizedCrop()\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Purpose: Crop a random portion, then resize to target size\n",
    "Why it's powerful:\n",
    "  - Creates scale variation (zoom in/out effect)\n",
    "  - Creates positional variation\n",
    "  - One of THE MOST EFFECTIVE augmentations!\n",
    "\n",
    "Parameters:\n",
    "  - size: Target size after crop and resize\n",
    "  - scale: Range of crop size (proportion of original)\n",
    "    - scale=(0.08, 1.0) means crop 8% to 100% of original\n",
    "  - ratio: Aspect ratio range of crop\n",
    "    - ratio=(3/4, 4/3) allows some distortion\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "random_crop = transforms.RandomResizedCrop(\n",
    "    size=(224, 224),\n",
    "    scale=(0.5, 1.0),  # Crop 50% to 100% of image\n",
    "    ratio=(0.9, 1.1)   # Nearly square crops\n",
    ")\n",
    "\n",
    "print(\"Applying RandomResizedCrop 6 times:\")\n",
    "print(\"  size=(224, 224)\")\n",
    "print(\"  scale=(0.5, 1.0)  # Crop 50%-100% of image\")\n",
    "print(\"  ratio=(0.9, 1.1)  # Aspect ratio variation\")\n",
    "print()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx in range(6):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "\n",
    "    cropped = random_crop(original_image)\n",
    "    axes[row, col].imshow(cropped)\n",
    "    axes[row, col].set_title(f\"Random Crop #{idx+1}\\nDifferent position & scale\")\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle(\"Transform: RandomResizedCrop() - Different Crops Each Time\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_07_random_resized_crop.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n",
    "\n",
    "print(\"\ud83d\udca1 This transform is why neural networks learn to focus on parts of objects!\")\n",
    "print(\"   The network sees the same object at different scales and positions.\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b16a7",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 9: DATA AUGMENTATION - RANDOM AFFINE\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1550f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 9: DATA AUGMENTATION - RANDOM AFFINE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Transform: transforms.RandomAffine()\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Purpose: Apply random affine transformations\n",
    "Affine transformations include:\n",
    "  - Rotation\n",
    "  - Translation (shifting)\n",
    "  - Scale (zoom)\n",
    "  - Shear (slanting)\n",
    "\n",
    "Parameters:\n",
    "  - degrees: Rotation range\n",
    "  - translate: Translation as fraction of image (e.g., 0.1 = 10%)\n",
    "  - scale: Scale factor range\n",
    "  - shear: Shear angle range\n",
    "\n",
    "This is like a \"swiss army knife\" of geometric augmentations!\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "random_affine = transforms.RandomAffine(\n",
    "    degrees=15,                    # Rotate \u00b115 degrees\n",
    "    translate=(0.1, 0.1),          # Translate up to 10% in each direction\n",
    "    scale=(0.9, 1.1),              # Scale 90%-110%\n",
    "    shear=10                        # Shear \u00b110 degrees\n",
    ")\n",
    "\n",
    "print(\"Applying RandomAffine with multiple transformations:\")\n",
    "print(\"  degrees=15        # Rotation \u00b115\u00b0\")\n",
    "print(\"  translate=(0.1, 0.1)  # Shift up to 10%\")\n",
    "print(\"  scale=(0.9, 1.1)      # Zoom 90%-110%\")\n",
    "print(\"  shear=10              # Shear \u00b110\u00b0\")\n",
    "print()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(resized_image)\n",
    "axes[0, 0].set_title(\"Original Image\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Apply 5 times\n",
    "for idx in range(5):\n",
    "    row = (idx + 1) // 3\n",
    "    col = (idx + 1) % 3\n",
    "\n",
    "    transformed = random_affine(resized_image)\n",
    "    axes[row, col].imshow(transformed)\n",
    "    axes[row, col].set_title(f\"Affine Transform #{idx+1}\\n(rotate+translate+scale+shear)\")\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle(\"Transform: RandomAffine() - Combined Geometric Transforms\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_08_random_affine.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed1d5e",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 10: DATA AUGMENTATION - COLOR JITTER\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c836f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 10: DATA AUGMENTATION - COLOR JITTER\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Transform: transforms.ColorJitter()\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Purpose: Randomly change brightness, contrast, saturation, and hue\n",
    "Why it's important:\n",
    "  - Images have different lighting conditions\n",
    "  - Camera settings vary\n",
    "  - Makes model robust to color variations\n",
    "\n",
    "Parameters (all values between 0 and 1):\n",
    "  - brightness: How much to jitter brightness\n",
    "    - 0.2 means brightness between 80% and 120%\n",
    "  - contrast: How much to jitter contrast\n",
    "  - saturation: How much to jitter saturation (color intensity)\n",
    "  - hue: How much to jitter hue (color shift)\n",
    "    - 0.1 means hue shift \u00b110% (\u00b136\u00b0 on color wheel)\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "color_jitter = transforms.ColorJitter(\n",
    "    brightness=0.3,  # \u00b130% brightness\n",
    "    contrast=0.3,    # \u00b130% contrast\n",
    "    saturation=0.3,  # \u00b130% saturation\n",
    "    hue=0.1          # \u00b110% hue (\u00b136\u00b0 on 360\u00b0 color wheel)\n",
    ")\n",
    "\n",
    "print(\"Applying ColorJitter 6 times:\")\n",
    "print(\"  brightness=0.3  # \u00b130%\")\n",
    "print(\"  contrast=0.3    # \u00b130%\")\n",
    "print(\"  saturation=0.3  # \u00b130%\")\n",
    "print(\"  hue=0.1         # \u00b136\u00b0 on color wheel\")\n",
    "print()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx in range(6):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "\n",
    "    jittered = color_jitter(resized_image)\n",
    "    axes[row, col].imshow(jittered)\n",
    "\n",
    "    if idx == 0:\n",
    "        axes[row, col].set_title(\"Original Image\")\n",
    "    else:\n",
    "        axes[row, col].set_title(f\"Color Jitter #{idx}\\n(random brightness/contrast/sat/hue)\")\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "# Use original image for first slot\n",
    "axes[0, 0].imshow(resized_image)\n",
    "axes[0, 0].set_title(\"Original Image\\n(No jitter)\")\n",
    "\n",
    "plt.suptitle(\"Transform: ColorJitter() - Color/Brightness Variations\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_09_color_jitter.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n",
    "\n",
    "print(\"\ud83d\udca1 ColorJitter makes your model robust to:\")\n",
    "print(\"   - Different lighting conditions (indoor/outdoor)\")\n",
    "print(\"   - Camera settings and quality\")\n",
    "print(\"   - Day/night, shadows, etc.\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a2cbc",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 11: COMPOSING TRANSFORMS\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b60d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 11: COMPOSING TRANSFORMS WITH transforms.Compose()\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Transform: transforms.Compose()\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Purpose: Chain multiple transforms together in a pipeline\n",
    "This is how you build a complete preprocessing pipeline!\n",
    "\n",
    "Transforms are applied IN ORDER:\n",
    "  1. First transform gets the original image\n",
    "  2. Second transform gets output of first\n",
    "  3. And so on...\n",
    "\n",
    "ORDER MATTERS! Always:\n",
    "  1. Do PIL operations first (Resize, Crop, ColorJitter, etc.)\n",
    "  2. ToTensor() comes after all PIL operations\n",
    "  3. Normalize() comes after ToTensor()\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Basic Preprocessing Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),           # 1. Resize\n",
    "    transforms.CenterCrop((224, 224)),        # 2. Crop\n",
    "    transforms.ToTensor(),                    # 3. PIL \u2192 Tensor\n",
    "    transforms.Normalize(                     # 4. Normalize\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"Pipeline steps:\")\n",
    "print(\"  1. Resize((256, 256))\")\n",
    "print(\"  2. CenterCrop((224, 224))\")\n",
    "print(\"  3. ToTensor()\")\n",
    "print(\"  4. Normalize(mean=ImageNet, std=ImageNet)\")\n",
    "print()\n",
    "\n",
    "print(\"Applying composed transform...\")\n",
    "processed = basic_transform(original_image)\n",
    "\n",
    "print()\n",
    "print(f\"Result:\")\n",
    "print(f\"  Type: {type(processed)}\")\n",
    "print(f\"  Shape: {processed.shape}\")\n",
    "print(f\"  Dtype: {processed.dtype}\")\n",
    "print(f\"  Range: [{processed.min():.3f}, {processed.max():.3f}]\")\n",
    "print(f\"  Mean: {processed.mean():.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Training Transform with Augmentation\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((224, 224)),    # Random crop & scale\n",
    "    transforms.RandomHorizontalFlip(p=0.5),      # 50% chance flip\n",
    "    transforms.ColorJitter(                       # Color variations\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.1\n",
    "    ),\n",
    "    transforms.RandomRotation(degrees=15),        # Small rotations\n",
    "    transforms.ToTensor(),                        # PIL \u2192 Tensor\n",
    "    transforms.Normalize(                         # Normalize\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"Training pipeline (with augmentation):\")\n",
    "print(\"  1. RandomResizedCrop((224, 224))\")\n",
    "print(\"  2. RandomHorizontalFlip(p=0.5)\")\n",
    "print(\"  3. ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\")\n",
    "print(\"  4. RandomRotation(degrees=15)\")\n",
    "print(\"  5. ToTensor()\")\n",
    "print(\"  6. Normalize(mean=ImageNet, std=ImageNet)\")\n",
    "print()\n",
    "\n",
    "# Apply the same transform 6 times to show variation\n",
    "print(\"Applying training transform 6 times to the same image:\")\n",
    "print(\"(Notice: Each result is different due to random augmentations!)\")\n",
    "print()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "results = []\n",
    "for i in range(6):\n",
    "    augmented = train_transform(original_image)\n",
    "    results.append(augmented)\n",
    "\n",
    "print(f\"\u2713 Generated 6 different augmented versions\")\n",
    "print()\n",
    "\n",
    "# Visualize the augmented results\n",
    "# Need to denormalize for visualization\n",
    "def denormalize(tensor, mean, std):\n",
    "    \"\"\"Denormalize a tensor image.\"\"\"\n",
    "    tensor = tensor.clone()\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx, augmented in enumerate(results):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "\n",
    "    # Denormalize for visualization\n",
    "    img_denorm = denormalize(\n",
    "        augmented,\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    # Clamp to [0, 1] range\n",
    "    img_denorm = torch.clamp(img_denorm, 0, 1)\n",
    "\n",
    "    # Convert to numpy and transpose to (H, W, C)\n",
    "    img_np = img_denorm.permute(1, 2, 0).numpy()\n",
    "\n",
    "    axes[row, col].imshow(img_np)\n",
    "    axes[row, col].set_title(f\"Augmentation #{idx+1}\\n(Different each time!)\")\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle(\"Training Transform: Same Image \u2192 6 Different Augmentations\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_10_composed_training.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea694af",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 12: TRAINING VS VALIDATION TRANSFORMS\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69210729",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 12: TRAINING VS VALIDATION TRANSFORMS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"KEY CONCEPT: Different Transforms for Training and Validation\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "TRAINING transforms:\n",
    "  - Use random augmentations\n",
    "  - Create variations of the data\n",
    "  - Help model generalize\n",
    "  - Each epoch sees different variations!\n",
    "\n",
    "VALIDATION/TEST transforms:\n",
    "  - Use deterministic preprocessing only\n",
    "  - NO random augmentations\n",
    "  - Consistent results for evaluation\n",
    "  - Fair comparison across models\n",
    "\n",
    "Why this matters:\n",
    "  - Training: Want model to learn from varied data\n",
    "  - Validation: Want consistent evaluation metrics\n",
    "  - Using augmentation on validation would give inconsistent results!\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING TRANSFORM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # Augmentations (random)\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    # Preprocessing (deterministic)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"\"\"\n",
    "train_transform = transforms.Compose([\n",
    "    # === AUGMENTATION (Random) ===\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "\n",
    "    # === PREPROCESSING (Deterministic) ===\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION TRANSFORM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    # NO augmentations! Only preprocessing\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"\"\"\n",
    "val_transform = transforms.Compose([\n",
    "    # === NO AUGMENTATION - Only preprocessing ===\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Comparison: Same Image, Different Transforms\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Apply training transform 3 times\n",
    "train_results = []\n",
    "for i in range(3):\n",
    "    train_results.append(train_transform(original_image))\n",
    "\n",
    "# Apply validation transform 3 times\n",
    "val_results = []\n",
    "for i in range(3):\n",
    "    val_results.append(val_transform(original_image))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Training results (different each time)\n",
    "for idx in range(3):\n",
    "    img_denorm = denormalize(\n",
    "        train_results[idx],\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    img_denorm = torch.clamp(img_denorm, 0, 1)\n",
    "    img_np = img_denorm.permute(1, 2, 0).numpy()\n",
    "\n",
    "    axes[0, idx].imshow(img_np)\n",
    "    axes[0, idx].set_title(f\"Training Transform #{idx+1}\\n(Different!)\")\n",
    "    axes[0, idx].axis('off')\n",
    "\n",
    "# Validation results (same each time)\n",
    "for idx in range(3):\n",
    "    img_denorm = denormalize(\n",
    "        val_results[idx],\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    img_denorm = torch.clamp(img_denorm, 0, 1)\n",
    "    img_np = img_denorm.permute(1, 2, 0).numpy()\n",
    "\n",
    "    axes[1, idx].imshow(img_np)\n",
    "    axes[1, idx].set_title(f\"Validation Transform #{idx+1}\\n(Identical!)\")\n",
    "    axes[1, idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"Training (Random) vs Validation (Deterministic) Transforms\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_11_train_vs_val.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n",
    "\n",
    "print(\"\ud83d\udca1 KEY TAKEAWAY:\")\n",
    "print(\"   Training: Apply transform each epoch \u2192 different augmentations\")\n",
    "print(\"   Validation: Apply once \u2192 consistent evaluation\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cbd56",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 13: DEBUGGING TRANSFORMS\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 13: DEBUGGING TRANSFORMS STEP-BY-STEP\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"How to Debug Your Transform Pipeline\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "When transforms don't work as expected:\n",
    "  1. Apply each transform individually\n",
    "  2. Print shape, dtype, and value ranges after each step\n",
    "  3. Visualize intermediate results\n",
    "  4. Check for common mistakes\n",
    "\n",
    "Common mistakes:\n",
    "  - Normalize before ToTensor (wrong order!)\n",
    "  - Wrong mean/std values\n",
    "  - Forgetting to convert back to PIL for visualization\n",
    "  - Using training transforms on validation data\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Debug Example: Step-by-Step Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Start with original image\n",
    "print(\"STEP 0: Original Image\")\n",
    "print(f\"  Type: {type(original_image)}\")\n",
    "print(f\"  Mode: {original_image.mode}\")\n",
    "print(f\"  Size: {original_image.size}\")\n",
    "print()\n",
    "\n",
    "# Step 1: Resize\n",
    "resize_t = transforms.Resize((256, 256))\n",
    "img_step1 = resize_t(original_image)\n",
    "print(\"STEP 1: After Resize((256, 256))\")\n",
    "print(f\"  Type: {type(img_step1)}\")\n",
    "print(f\"  Size: {img_step1.size}\")\n",
    "print(f\"  \u2713 Still PIL Image\")\n",
    "print()\n",
    "\n",
    "# Step 2: Center Crop\n",
    "crop_t = transforms.CenterCrop((224, 224))\n",
    "img_step2 = crop_t(img_step1)\n",
    "print(\"STEP 2: After CenterCrop((224, 224))\")\n",
    "print(f\"  Type: {type(img_step2)}\")\n",
    "print(f\"  Size: {img_step2.size}\")\n",
    "print(f\"  \u2713 Still PIL Image\")\n",
    "print()\n",
    "\n",
    "# Step 3: ToTensor\n",
    "to_tensor_t = transforms.ToTensor()\n",
    "img_step3 = to_tensor_t(img_step2)\n",
    "print(\"STEP 3: After ToTensor()\")\n",
    "print(f\"  Type: {type(img_step3)}\")\n",
    "print(f\"  Shape: {img_step3.shape}\")\n",
    "print(f\"  Dtype: {img_step3.dtype}\")\n",
    "print(f\"  Range: [{img_step3.min():.4f}, {img_step3.max():.4f}]\")\n",
    "print(f\"  Mean: {img_step3.mean():.4f}\")\n",
    "print(f\"  \u2713 Now a Tensor! Shape is (C, H, W)\")\n",
    "print()\n",
    "\n",
    "# Step 4: Normalize\n",
    "normalize_t = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "img_step4 = normalize_t(img_step3)\n",
    "print(\"STEP 4: After Normalize()\")\n",
    "print(f\"  Type: {type(img_step4)}\")\n",
    "print(f\"  Shape: {img_step4.shape}\")\n",
    "print(f\"  Dtype: {img_step4.dtype}\")\n",
    "print(f\"  Range: [{img_step4.min():.4f}, {img_step4.max():.4f}]\")\n",
    "print(f\"  Mean: {img_step4.mean():.4f}\")\n",
    "print(f\"  \u2713 Values now centered around 0\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Visualizing Each Step\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Step 0: Original\n",
    "axes[0, 0].imshow(original_image)\n",
    "axes[0, 0].set_title(f\"Step 0: Original\\nPIL Image {original_image.size}\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Step 1: Resized\n",
    "axes[0, 1].imshow(img_step1)\n",
    "axes[0, 1].set_title(f\"Step 1: Resized\\nPIL Image {img_step1.size}\")\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Step 2: Cropped\n",
    "axes[0, 2].imshow(img_step2)\n",
    "axes[0, 2].set_title(f\"Step 2: Cropped\\nPIL Image {img_step2.size}\")\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Step 3: ToTensor\n",
    "img_step3_np = img_step3.permute(1, 2, 0).numpy()\n",
    "axes[1, 0].imshow(img_step3_np)\n",
    "axes[1, 0].set_title(f\"Step 3: ToTensor\\nTensor {img_step3.shape}\\nRange: [0, 1]\")\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Step 4: Normalized (need to denormalize for visualization)\n",
    "img_step4_denorm = denormalize(img_step4, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "img_step4_denorm = torch.clamp(img_step4_denorm, 0, 1)\n",
    "img_step4_np = img_step4_denorm.permute(1, 2, 0).numpy()\n",
    "axes[1, 1].imshow(img_step4_np)\n",
    "axes[1, 1].set_title(f\"Step 4: Normalized\\nTensor {img_step4.shape}\\nRange: \u2248[-2, 2]\")\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Hide last subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Debugging Transforms: Step-by-Step Visualization\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "output_path = output_dir / \"transform_12_debugging.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\u2713 Saved visualization: {output_path}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Common Issues and Solutions\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Issue 1: \"Normalize() expects a tensor\"\n",
    "  \u274c WRONG order:\n",
    "     Compose([Normalize(), ToTensor()])\n",
    "  \u2713 CORRECT order:\n",
    "     Compose([ToTensor(), Normalize()])\n",
    "\n",
    "Issue 2: \"Images look weird after normalization\"\n",
    "  \u2192 This is normal! Normalized images have negative values\n",
    "  \u2192 Always denormalize before visualizing\n",
    "\n",
    "Issue 3: \"Getting different results on validation set\"\n",
    "  \u2192 Check: Are you using random transforms on val data?\n",
    "  \u2192 Solution: Use deterministic transforms for validation\n",
    "\n",
    "Issue 4: \"Model isn't learning\"\n",
    "  \u2192 Check: Are you normalizing with correct mean/std?\n",
    "  \u2192 Check: Is ToTensor() converting to [0, 1] range?\n",
    "  \u2192 Use debugging to verify each step!\n",
    "\"\"\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66fab2d",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 14: PRACTICAL TIPS AND BEST PRACTICES\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 14: PRACTICAL TIPS AND BEST PRACTICES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Best Practices for Data Transforms\")\n",
    "print(\"-\" * 60)\n",
    "print()\n",
    "\n",
    "print(\"1. ALWAYS follow this order:\")\n",
    "print(\"\"\"\n",
    "   \u2713 CORRECT:\n",
    "     transforms.Compose([\n",
    "         # 1. PIL transformations first\n",
    "         transforms.Resize(...),\n",
    "         transforms.RandomCrop(...),\n",
    "         transforms.ColorJitter(...),\n",
    "\n",
    "         # 2. ToTensor (PIL \u2192 Tensor)\n",
    "         transforms.ToTensor(),\n",
    "\n",
    "         # 3. Normalize (Tensor \u2192 Normalized Tensor)\n",
    "         transforms.Normalize(...)\n",
    "     ])\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "print(\"2. Standard preprocessing recipe:\")\n",
    "print(\"\"\"\n",
    "   For ImageNet-pretrained models:\n",
    "\n",
    "   Training:\n",
    "     - RandomResizedCrop(224)\n",
    "     - RandomHorizontalFlip(p=0.5)\n",
    "     - ToTensor()\n",
    "     - Normalize(mean=ImageNet, std=ImageNet)\n",
    "\n",
    "   Validation:\n",
    "     - Resize(256)\n",
    "     - CenterCrop(224)\n",
    "     - ToTensor()\n",
    "     - Normalize(mean=ImageNet, std=ImageNet)\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "print(\"3. Computing your own normalization statistics:\")\n",
    "print(\"\"\"\n",
    "   If training from scratch on your own dataset:\n",
    "\n",
    "   from torch.utils.data import DataLoader\n",
    "\n",
    "   # Load data with ToTensor() only\n",
    "   dataset = YourDataset(transform=transforms.ToTensor())\n",
    "   loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "   # Compute mean and std\n",
    "   mean = 0.0\n",
    "   std = 0.0\n",
    "   total = 0\n",
    "\n",
    "   for images, _ in loader:\n",
    "       batch_samples = images.size(0)\n",
    "       images = images.view(batch_samples, images.size(1), -1)\n",
    "       mean += images.mean(2).sum(0)\n",
    "       std += images.std(2).sum(0)\n",
    "       total += batch_samples\n",
    "\n",
    "   mean /= total\n",
    "   std /= total\n",
    "\n",
    "   print(f\"Mean: {mean}\")\n",
    "   print(f\"Std: {std}\")\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "print(\"4. When to use which augmentation:\")\n",
    "print(\"\"\"\n",
    "   Natural images (objects, animals, scenes):\n",
    "     \u2713 RandomResizedCrop\n",
    "     \u2713 RandomHorizontalFlip\n",
    "     \u2713 ColorJitter\n",
    "     \u2713 RandomRotation (small angles)\n",
    "\n",
    "   Medical images:\n",
    "     \u2713 RandomRotation\n",
    "     \u2713 RandomAffine\n",
    "     \u2717 ColorJitter (may change diagnostic features!)\n",
    "     \u2717 HorizontalFlip (if L/R matters)\n",
    "\n",
    "   Text/Documents:\n",
    "     \u2717 HorizontalFlip (would flip text!)\n",
    "     \u2717 RandomRotation (unless small angles)\n",
    "     \u2713 ColorJitter (for different paper/lighting)\n",
    "\"\"\")\n",
    "print()\n",
    "\n",
    "print(\"5. How much augmentation is too much?\")\n",
    "print(\"\"\"\n",
    "   Start conservative:\n",
    "     - brightness=0.1, contrast=0.1, saturation=0.1\n",
    "     - rotation=5-10 degrees\n",
    "     - RandomResizedCrop scale=(0.8, 1.0)\n",
    "\n",
    "   Gradually increase if:\n",
    "     - Training loss is much lower than validation loss\n",
    "     - Model overfits quickly\n",
    "\n",
    "   Decrease if:\n",
    "     - Training loss stays high\n",
    "     - Model can't learn\n",
    "     - Augmentations destroy important features\n",
    "\"\"\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcd4a2",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "SECTION 15: SUMMARY\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 15: SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"\ud83c\udfaf What we learned:\")\n",
    "print()\n",
    "\n",
    "print(\"1. Basic Transforms:\")\n",
    "print(\"   \u2022 Resize() - Make images uniform size\")\n",
    "print(\"   \u2022 CenterCrop() - Crop center portion\")\n",
    "print(\"   \u2022 ToTensor() - PIL Image \u2192 PyTorch Tensor\")\n",
    "print(\"   \u2022 Normalize() - Standardize pixel values\")\n",
    "print()\n",
    "\n",
    "print(\"2. Data Augmentation:\")\n",
    "print(\"   \u2022 RandomHorizontalFlip() - Mirror images\")\n",
    "print(\"   \u2022 RandomRotation() - Rotate by random angles\")\n",
    "print(\"   \u2022 RandomResizedCrop() - Random zoom and position\")\n",
    "print(\"   \u2022 RandomAffine() - Combined geometric transforms\")\n",
    "print(\"   \u2022 ColorJitter() - Vary colors and brightness\")\n",
    "print()\n",
    "\n",
    "print(\"3. Key Concepts:\")\n",
    "print(\"   \u2022 Compose() - Chain transforms together\")\n",
    "print(\"   \u2022 Training transforms - Use augmentation\")\n",
    "print(\"   \u2022 Validation transforms - Deterministic only\")\n",
    "print(\"   \u2022 Always: PIL ops \u2192 ToTensor() \u2192 Normalize()\")\n",
    "print()\n",
    "\n",
    "print(\"4. Debugging:\")\n",
    "print(\"   \u2022 Apply transforms step-by-step\")\n",
    "print(\"   \u2022 Print shapes and value ranges\")\n",
    "print(\"   \u2022 Visualize intermediate results\")\n",
    "print(\"   \u2022 Check transform order!\")\n",
    "print()\n",
    "\n",
    "print(\"\ud83d\udcca Files created:\")\n",
    "for i in range(1, 13):\n",
    "    filename = f\"transform_{i:02d}_*.png\"\n",
    "    files = list(output_dir.glob(f\"transform_{i:02d}_*.png\"))\n",
    "    if files:\n",
    "        print(f\"   \u2713 {files[0].name}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d586d",
   "metadata": {
    "cell_marker": "################################################################################"
   },
   "source": [
    "PRACTICE PROBLEMS\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PRACTICE PROBLEMS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"\"\"\n",
    "Practice Problem 1: Create Weak Augmentation\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "Create a transform pipeline with \"weak\" augmentation suitable for fine-tuning:\n",
    "  - RandomResizedCrop(224, scale=(0.9, 1.0))\n",
    "  - RandomHorizontalFlip(p=0.3)\n",
    "  - ColorJitter(brightness=0.05, contrast=0.05)\n",
    "  - ToTensor()\n",
    "  - Normalize with ImageNet stats\n",
    "\n",
    "Apply it to 5 images and visualize. Are the augmentations subtle?\n",
    "\n",
    "\n",
    "Practice Problem 2: Medical Image Augmentation\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "Create augmentation for medical images where:\n",
    "  - Horizontal flip is NOT allowed (L/R matters)\n",
    "  - Rotation up to \u00b120 degrees is allowed\n",
    "  - Color changes should be minimal\n",
    "  - Slight translation/shearing is allowed\n",
    "\n",
    "Design an appropriate transform pipeline.\n",
    "\n",
    "\n",
    "Practice Problem 3: Compute Dataset Statistics\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "Write code to compute mean and std for the Oxford Flowers dataset:\n",
    "  1. Load all images with ToTensor() only\n",
    "  2. Compute per-channel mean and std\n",
    "  3. Create a Normalize() transform with these values\n",
    "  4. Compare with ImageNet statistics\n",
    "\n",
    "Hint: See Best Practices section for the code template!\n",
    "\n",
    "\n",
    "Practice Problem 4: Debug a Broken Pipeline\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "This pipeline is BROKEN. Find and fix the issues:\n",
    "\n",
    "broken_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    transforms.RandomResizedCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "])\n",
    "\n",
    "Hint: Apply each transform individually and see where it fails!\n",
    "\n",
    "\n",
    "Practice Problem 5: Create Test-Time Augmentation\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "Test-Time Augmentation (TTA) applies multiple transforms at inference:\n",
    "  1. Original image (center crop)\n",
    "  2. Flipped version\n",
    "  3. 4 corner crops + center crop\n",
    "  4. Average predictions\n",
    "\n",
    "Create 5 different deterministic crops/flips of the same image.\n",
    "(Useful for better test accuracy!)\n",
    "\n",
    "\n",
    "Practice Problem 6: Visualize Transform Effects\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "Create a comprehensive visualization showing:\n",
    "  - Original image\n",
    "  - All 5 augmentation types we learned (side-by-side)\n",
    "  - Before/after comparison for each\n",
    "\n",
    "Save as one large figure with appropriate titles.\n",
    "\n",
    "\n",
    "Practice Problem 7: Strong Augmentation\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "Create \"strong\" augmentation for robust training:\n",
    "  - RandomResizedCrop(224, scale=(0.5, 1.0))\n",
    "  - RandomHorizontalFlip(p=0.5)\n",
    "  - RandomRotation(degrees=30)\n",
    "  - ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2)\n",
    "  - RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2))\n",
    "\n",
    "Apply to an image. Is it still recognizable? That's the balance!\n",
    "\n",
    "\n",
    "Practice Problem 8: Transform Probability\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "Apply RandomHorizontalFlip(p=0.5) to the same image 100 times.\n",
    "  - Count how many times it flipped\n",
    "  - Is it close to 50%?\n",
    "  - Plot a histogram of \"flipped\" vs \"not flipped\"\n",
    "\n",
    "This demonstrates the randomness in augmentation!\n",
    "\"\"\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"END OF LESSON 08: DATA TRANSFORMS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"\ud83c\udf89 Congratulations! You've mastered PyTorch data transforms!\")\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(\"  \u2022 Try the practice problems\")\n",
    "print(\"  \u2022 Experiment with different augmentation strengths\")\n",
    "print(\"  \u2022 Apply transforms to your own datasets\")\n",
    "print(\"  \u2022 Move on to Lesson 09: Data Splitting and DataLoaders\")\n",
    "print()\n",
    "print(f\"All visualizations saved to: {output_dir}\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}