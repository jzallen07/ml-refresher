{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Linear Regression - Code Exercise\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "LESSON 6: LINEAR REGRESSION IN PYTORCH\n",
    "================================================================================\n",
    "\n",
    "Source: Based on TK's article on Linear Regression with PyTorch\n",
    "Author: Educational lesson based on TK's tutorial\n",
    "Topic: Building and training linear regression models using PyTorch\n",
    "\n",
    "Linear regression is one of the fundamental machine learning algorithms.\n",
    "It models the relationship between input features and output targets using\n",
    "a linear equation: y = xw + b\n",
    "\n",
    "Where:\n",
    "- x = input feature(s)\n",
    "- w = weight(s) - the slope of the line\n",
    "- b = bias - the y-intercept\n",
    "- y = predicted output\n",
    "\n",
    "In this lesson, we'll:\n",
    "1. Understand the linear model mathematically\n",
    "2. Use PyTorch's built-in nn.Linear module\n",
    "3. Build a custom linear regression model class\n",
    "4. Train the model on distance-time data\n",
    "5. Understand the training loop: predict \u2192 loss \u2192 adjust \u2192 repeat\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e93417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PYTORCH LINEAR REGRESSION LESSON\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72347892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: INTRODUCTION - THE LINEAR MODEL\n",
    "# ============================================================================\n",
    "print(\"SECTION 1: Understanding the Linear Model\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33448e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The linear equation: y = xw + b\n",
    "\n",
    "This is the foundation of linear regression. Let's break it down:\n",
    "- 'x' is our input (independent variable)\n",
    "- 'w' is the weight/slope (what we want to learn)\n",
    "- 'b' is the bias/intercept (also what we want to learn)\n",
    "- 'y' is our output (dependent variable)\n",
    "\n",
    "Example: If we're predicting time based on distance:\n",
    "- x = distance traveled\n",
    "- y = time taken\n",
    "- w = how much time increases per unit distance\n",
    "- b = base time (starting point)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813d2418",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear equation: y = xw + b\")\n",
    "print(\"We need to LEARN the values of 'w' (weight) and 'b' (bias)\")\n",
    "print(\"PyTorch will help us find these through gradient descent!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47537ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: USING nn.Linear DIRECTLY\n",
    "# ============================================================================\n",
    "print(\"\\nSECTION 2: Using PyTorch's nn.Linear Module\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch provides nn.Linear, a ready-to-use linear transformation module.\n",
    "It implements y = xw + b automatically!\n",
    "\n",
    "Parameters:\n",
    "- in_features: number of input features (dimensions of x)\n",
    "- out_features: number of output features (dimensions of y)\n",
    "- bias: whether to include the bias term b (default=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple linear model: 1 input feature \u2192 1 output feature\n",
    "model_simple = nn.Linear(in_features=1, out_features=1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baab7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Created nn.Linear model with:\")\n",
    "print(f\"  - in_features=1 (one input)\")\n",
    "print(f\"  - out_features=1 (one output)\")\n",
    "print(f\"  - bias=True (includes bias term)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the initial random weights and bias\n",
    "print(\"Initial model parameters (randomly initialized):\")\n",
    "print(f\"  Weight: {model_simple.weight.item():.4f}\")\n",
    "print(f\"  Bias: {model_simple.bias.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317db732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with some inputs\n",
    "test_input = torch.tensor([[2.], [4.]])\n",
    "print(f\"Testing model with input: {test_input.T}\")\n",
    "predictions = model_simple(test_input)\n",
    "print(f\"Model predictions: {predictions.T}\")\n",
    "print(f\"  (These are random because the model hasn't been trained yet!)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From TK's article example:\n",
    "If weight = -0.4443 and bias = -0.5045\n",
    "Then for inputs [2., 4.]:\n",
    "  - For x=2: y = 2*(-0.4443) + (-0.5045) = -1.3930\n",
    "  - For x=4: y = 4*(-0.4443) + (-0.5045) = -2.2815\n",
    "Result: tensor([[-1.3930], [-2.2815]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: BUILDING A CUSTOM MODEL WITH nn.Module\n",
    "# ============================================================================\n",
    "print(\"\\nSECTION 3: Building a Custom Linear Regression Model\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dab6d0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "While nn.Linear works great on its own, we often want to build custom models\n",
    "using nn.Module. This is THE standard way to define models in PyTorch.\n",
    "\n",
    "All PyTorch models should:\n",
    "1. Inherit from nn.Module\n",
    "2. Define layers in __init__\n",
    "3. Implement the forward() method (defines the forward pass)\n",
    "\n",
    "This pattern scales from simple linear regression to complex neural networks!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff9dbc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Linear Regression Model\n",
    "\n",
    "    This class wraps nn.Linear in a custom nn.Module class.\n",
    "    While this is overkill for simple linear regression, it demonstrates\n",
    "    the pattern you'll use for all PyTorch models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of input features\n",
    "            output_size (int): Number of output features\n",
    "        \"\"\"\n",
    "        # Always call the parent class constructor first!\n",
    "        super(LinearRegression, self).__init__()\n",
    "\n",
    "        # Define our linear layer\n",
    "        # This creates trainable parameters (weight and bias)\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: define how data flows through the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input data\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Model predictions\n",
    "        \"\"\"\n",
    "        # Simply pass input through our linear layer\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a335478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our custom model\n",
    "model = LinearRegression(input_size=1, output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Created custom LinearRegression model!\")\n",
    "print(f\"Model architecture:\\n{model}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b635b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: shape={param.shape}, value={param.item():.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: PREPARING TRAINING DATA\n",
    "# ============================================================================\n",
    "print(\"\\nSECTION 4: Preparing Training Data\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526008ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From TK's article, we have distance-time data:\n",
    "- distances: how far traveled (input, x)\n",
    "- times: how long it took (output, y)\n",
    "\n",
    "This is a perfect problem for linear regression because time should\n",
    "increase linearly with distance (assuming constant speed).\n",
    "\n",
    "The model will learn: time = distance * w + b\n",
    "Where:\n",
    "  w \u2248 the inverse of speed (time per unit distance)\n",
    "  b \u2248 base time or startup time\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ef350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data from TK's article\n",
    "distances = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)\n",
    "times = torch.tensor([[6.96], [12.11], [16.77], [22.21]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data (distance \u2192 time):\")\n",
    "print(\"  Distance  |  Time\")\n",
    "print(\"  \" + \"-\" * 20)\n",
    "for d, t in zip(distances, times):\n",
    "    print(f\"    {d.item():.1f}     |  {t.item():.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data shapes:\")\n",
    "print(f\"  distances: {distances.shape} (4 samples, 1 feature)\")\n",
    "print(f\"  times: {times.shape} (4 samples, 1 target)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Goal: Learn the relationship between distance and time\")\n",
    "print(\"The model will adjust its weight and bias to fit this data!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4210d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: DEFINING LOSS FUNCTION AND OPTIMIZER\n",
    "# ============================================================================\n",
    "print(\"\\nSECTION 5: Defining Loss Function and Optimizer\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa422ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From TK's article: \"We need the model, a loss function, and an optimizer\n",
    "to build the entire training process.\"\n",
    "\n",
    "LOSS FUNCTION (How wrong are we?):\n",
    "  - Measures the difference between predictions and actual values\n",
    "  - MSE (Mean Squared Error) = average of (prediction - actual)\u00b2\n",
    "  - Lower loss = better model\n",
    "\n",
    "OPTIMIZER (How do we improve?):\n",
    "  - Adjusts model parameters to reduce loss\n",
    "  - SGD (Stochastic Gradient Descent) is a simple but effective optimizer\n",
    "  - Learning rate (lr) controls how big the adjustment steps are\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d308c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function: Mean Squared Error\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c461bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss Function: MSELoss (Mean Squared Error)\")\n",
    "print(\"  Formula: MSE = (1/n) * \u03a3(predicted - actual)\u00b2\")\n",
    "print(\"  Goal: Minimize this value!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc206df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer: Stochastic Gradient Descent\n",
    "# It will adjust model.parameters() (the weight and bias)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizer: SGD (Stochastic Gradient Descent)\")\n",
    "print(f\"  Learning rate: 0.01\")\n",
    "print(f\"  Parameters to optimize: {list(model.parameters())}\")\n",
    "print(\"  (These are the weight and bias that will be adjusted)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Learning rate (lr=0.01) is crucial:\n",
    "  - Too high: the model might overshoot and never converge\n",
    "  - Too low: training will be very slow\n",
    "  - 0.01 is a common starting point for simple problems\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2059b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: THE TRAINING LOOP\n",
    "# ============================================================================\n",
    "print(\"\\nSECTION 6: The Training Loop\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ece3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From TK's article: \"The training loop follows this pattern:\n",
    "  predict \u2192 calculate loss \u2192 adjust parameters \u2192 predict \u2192 calculate loss \u2192 adjust parameters\n",
    "The loop goes on until it finishes all epochs.\"\n",
    "\n",
    "Each iteration through the loop:\n",
    "  1. optimizer.zero_grad() - Clear previous gradients\n",
    "  2. outputs = model(distances) - Forward pass: make predictions\n",
    "  3. loss = loss_function(outputs, times) - Calculate loss\n",
    "  4. loss.backward() - Backward pass: compute gradients\n",
    "  5. optimizer.step() - Update parameters using gradients\n",
    "\n",
    "An EPOCH is one complete pass through all training data.\n",
    "We'll train for 500 epochs to give the model time to learn.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training for 500 epochs...\")\n",
    "print(\"Watch the loss decrease as the model learns!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21266b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store initial parameters to see how they change\n",
    "initial_weight = model.linear.weight.item()\n",
    "initial_bias = model.linear.bias.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26155355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial parameters:\")\n",
    "print(f\"  Weight: {initial_weight:.4f}\")\n",
    "print(f\"  Bias: {initial_bias:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training progress:\")\n",
    "print(\"  Epoch  |  Loss\")\n",
    "print(\"  \" + \"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a808dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (from TK's article)\n",
    "for epoch in range(500):\n",
    "    # ---- STEP 1: Zero the gradients ----\n",
    "    # Gradients accumulate by default in PyTorch, so we must clear them\n",
    "    # each iteration to avoid using stale gradient information\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # ---- STEP 2: Forward pass (predict) ----\n",
    "    # Pass our input data (distances) through the model to get predictions\n",
    "    # This calls model.forward(distances) automatically\n",
    "    outputs = model(distances)\n",
    "\n",
    "    # ---- STEP 3: Calculate loss ----\n",
    "    # Compare our predictions (outputs) with actual values (times)\n",
    "    # The loss tells us how wrong our predictions are\n",
    "    loss = loss_function(outputs, times)\n",
    "\n",
    "    # ---- STEP 4: Backward pass (compute gradients) ----\n",
    "    # Calculate gradients of the loss with respect to model parameters\n",
    "    # This is where PyTorch's autograd magic happens!\n",
    "    # It computes \u2202loss/\u2202weight and \u2202loss/\u2202bias\n",
    "    loss.backward()\n",
    "\n",
    "    # ---- STEP 5: Update parameters (learn) ----\n",
    "    # Adjust the model parameters using the gradients\n",
    "    # New_param = Old_param - learning_rate * gradient\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress every 50 epochs\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"  {epoch:4d}   |  {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final loss after training\n",
    "print(\"  \" + \"-\" * 30)\n",
    "print(f\"  Final  |  {loss.item():.6f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22cfaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check learned parameters\n",
    "final_weight = model.linear.weight.item()\n",
    "final_bias = model.linear.bias.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c163532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Learned parameters:\")\n",
    "print(f\"  Weight: {final_weight:.4f} (was {initial_weight:.4f})\")\n",
    "print(f\"  Bias: {final_bias:.4f} (was {initial_bias:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The model has learned! Weight and bias now capture the distance-time relationship.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c154ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What just happened?\n",
    "  - The loss decreased from ~100+ to ~0.0x\n",
    "  - The weight and bias were adjusted to fit the training data\n",
    "  - The model now \"understands\" the relationship between distance and time\n",
    "\n",
    "Mathematical insight:\n",
    "  - The weight tells us approximately how much time increases per distance unit\n",
    "  - The bias captures any base time or offset\n",
    "  - Together, they form: time = distance * weight + bias\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e82ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: EVALUATING THE TRAINED MODEL\n",
    "# ============================================================================\n",
    "print(\"\\nSECTION 7: Evaluating the Trained Model\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ba43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now that our model is trained, let's test it!\n",
    "We'll compare its predictions with the actual training data,\n",
    "and also make predictions for new, unseen distances.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91cff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in evaluation mode (not strictly necessary for simple models,\n",
    "# but it's good practice for when you use dropout, batch norm, etc.)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec14a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient calculation for inference (saves memory and computation)\n",
    "with torch.no_grad():\n",
    "    # Test on training data\n",
    "    print(\"Predictions on training data:\")\n",
    "    print(\"  Distance  |  Actual Time  |  Predicted Time  |  Error\")\n",
    "    print(\"  \" + \"-\" * 60)\n",
    "\n",
    "    train_predictions = model(distances)\n",
    "    for d, actual, pred in zip(distances, times, train_predictions):\n",
    "        error = abs(actual.item() - pred.item())\n",
    "        print(f\"    {d.item():.1f}     |     {actual.item():.2f}      |      {pred.item():.2f}       |  {error:.2f}\")\n",
    "    print()\n",
    "\n",
    "    # Test on new data\n",
    "    print(\"Predictions on NEW distances (not in training data):\")\n",
    "    print(\"  Distance  |  Predicted Time\")\n",
    "    print(\"  \" + \"-\" * 30)\n",
    "\n",
    "    new_distances = torch.tensor([[0.5], [1.5], [2.5], [5.0], [10.0]], dtype=torch.float32)\n",
    "    new_predictions = model(new_distances)\n",
    "\n",
    "    for d, pred in zip(new_distances, new_predictions):\n",
    "        print(f\"    {d.item():.1f}     |     {pred.item():.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The model has successfully learned the distance-time relationship!\")\n",
    "print(\"It can now make predictions for any distance, even ones it hasn't seen before.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notice how the model:\n",
    "  1. Fits the training data very closely (small errors)\n",
    "  2. Can extrapolate to new distances (like 5.0 and 10.0)\n",
    "  3. Follows a linear pattern (as expected from linear regression)\n",
    "\n",
    "The learned equation is: time = distance * {:.4f} + {:.4f}\n",
    "\"\"\".format(final_weight, final_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946bd18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: PRACTICE PROBLEMS\n",
    "# ============================================================================\n",
    "print(\"\\nSECTION 8: Practice Problems\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now it's your turn! Try these exercises to reinforce your understanding:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b688a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "PROBLEM 1: Change the Learning Rate\n",
    "-------------------------------------\n",
    "Experiment with different learning rates:\n",
    "  - Try lr=0.001 (10x smaller)\n",
    "  - Try lr=0.1 (10x larger)\n",
    "\n",
    "Questions:\n",
    "  a) How does a smaller learning rate affect training speed?\n",
    "  b) How does a larger learning rate affect loss convergence?\n",
    "  c) Can you find an optimal learning rate for this problem?\n",
    "\n",
    "Hint: Copy the training loop code and create a new optimizer with a different lr.\n",
    "\n",
    "\n",
    "PROBLEM 2: Different Training Data\n",
    "------------------------------------\n",
    "Create your own training data for a different linear relationship:\n",
    "  - Example: hours studied vs exam score\n",
    "  - Example: square footage vs house price\n",
    "  - Example: years of experience vs salary\n",
    "\n",
    "Steps:\n",
    "  1. Create new input and output tensors\n",
    "  2. Create a new model instance\n",
    "  3. Train it using the same training loop\n",
    "  4. Evaluate the learned relationship\n",
    "\n",
    "Hint: Use torch.tensor([[val1], [val2], ...], dtype=torch.float32)\n",
    "\n",
    "\n",
    "PROBLEM 3: Multiple Inputs (Challenge!)\n",
    "-----------------------------------------\n",
    "Extend the model to handle multiple input features.\n",
    "For example, predict time based on BOTH distance AND speed:\n",
    "  time = distance * w1 + speed * w2 + b\n",
    "\n",
    "Steps:\n",
    "  1. Create a model with input_size=2 instead of 1\n",
    "  2. Create training data with 2 features per sample:\n",
    "     inputs = torch.tensor([[dist1, speed1], [dist2, speed2], ...])\n",
    "  3. Train and evaluate\n",
    "\n",
    "Hint: Change model = LinearRegression(input_size=2, output_size=1)\n",
    "\n",
    "\n",
    "BONUS CHALLENGE: Visualize the Results\n",
    "----------------------------------------\n",
    "Install matplotlib and plot:\n",
    "  - Training data points (scatter plot)\n",
    "  - Model's learned line\n",
    "  - Loss over epochs\n",
    "\n",
    "This will help you visualize what the model learned!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"END OF LESSON\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Key Takeaways:\n",
    "  1. Linear regression models y = xw + b\n",
    "  2. PyTorch provides nn.Linear and nn.Module for building models\n",
    "  3. Training requires: model, loss function, optimizer\n",
    "  4. The training loop: zero_grad \u2192 forward \u2192 loss \u2192 backward \u2192 step\n",
    "  5. Gradients tell us how to adjust parameters to reduce loss\n",
    "  6. Lower loss = better fit to the data\n",
    "\n",
    "Next Steps:\n",
    "  - Try the practice problems above\n",
    "  - Experiment with different data and hyperparameters\n",
    "  - Learn about other loss functions and optimizers\n",
    "  - Move on to more complex models (multiple layers, non-linear activations)\n",
    "\n",
    "Happy learning! \ud83d\ude80\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}