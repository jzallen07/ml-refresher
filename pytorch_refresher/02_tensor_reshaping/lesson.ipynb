{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Reshaping - Code Exercise\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PYTORCH TENSOR RESHAPING OPERATIONS\n",
    "================================================================================\n",
    "\n",
    "Lesson: Understanding tensor shape manipulation in PyTorch\n",
    "Source: Based on TK's article on PyTorch tensor reshaping\n",
    "Author: PyTorch Refresher Course\n",
    "\n",
    "Topics Covered:\n",
    "- unsqueeze(): Adding dimensions to tensors\n",
    "- squeeze(): Removing dimensions from tensors\n",
    "- view(): Flexible tensor reshaping\n",
    "- Dynamic sizing with -1 placeholder\n",
    "- Real-world ML applications\n",
    "\n",
    "Why Reshaping Matters:\n",
    "In deep learning, tensor shapes must match exactly for operations to work.\n",
    "Reshaping is essential for:\n",
    "- Adding batch dimensions for model input\n",
    "- Flattening feature maps before fully connected layers\n",
    "- Preparing data for different layer types (Conv2D vs Linear)\n",
    "- Broadcasting operations between tensors\n",
    "\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93263f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58f857",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Print PyTorch version for reference\n",
    "print(\"=\" * 80)\n",
    "print(\"PYTORCH TENSOR RESHAPING LESSON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f72179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: UNSQUEEZE() - ADDING DIMENSIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 1: UNSQUEEZE() - ADDING DIMENSIONS\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"What is unsqueeze()?\")\n",
    "print(\"-\" * 80)\n",
    "print(\"unsqueeze() adds a new dimension of size 1 at the specified position.\")\n",
    "print(\"This is crucial when you need to add a batch dimension or match tensor shapes.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74260bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1.1: Basic unsqueeze - scalar to 1D tensor\n",
    "print(\"Example 1.1: Adding a dimension to a scalar\")\n",
    "print(\"-\" * 80)\n",
    "tensor = torch.tensor(1)\n",
    "print(f\"Original tensor: {tensor}\")\n",
    "print(f\"Original shape: {tensor.shape}\")\n",
    "print(f\"Number of dimensions: {tensor.ndim}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004dbd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dimension at position 0 (becomes the first dimension)\n",
    "batch = tensor.unsqueeze(0)\n",
    "print(f\"After unsqueeze(0): {batch}\")\n",
    "print(f\"New shape: {batch.shape}\")\n",
    "print(f\"Number of dimensions: {batch.ndim}\")\n",
    "print(\"\u2192 We added a dimension of size 1 at position 0\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1.2: Unsqueeze a 1D tensor\n",
    "print(\"Example 1.2: Adding dimensions to a 1D tensor\")\n",
    "print(\"-\" * 80)\n",
    "tensor_1d = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "print(f\"Original 1D tensor: {tensor_1d}\")\n",
    "print(f\"Original shape: {tensor_1d.shape}\")  # torch.Size([5])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42934b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsqueeze at dimension 0 (add batch dimension)\n",
    "unsqueezed_0 = tensor_1d.unsqueeze(0)\n",
    "print(f\"After unsqueeze(0): {unsqueezed_0}\")\n",
    "print(f\"Shape: {unsqueezed_0.shape}\")  # torch.Size([1, 5])\n",
    "print(\"\u2192 Added dimension at the front - useful for batch dimension\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsqueeze at dimension 1 (add feature dimension)\n",
    "unsqueezed_1 = tensor_1d.unsqueeze(1)\n",
    "print(f\"After unsqueeze(1): {unsqueezed_1}\")\n",
    "print(f\"Shape: {unsqueezed_1.shape}\")  # torch.Size([5, 1])\n",
    "print(\"\u2192 Added dimension at the end - converts to column vector\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e93b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1.3: Unsqueeze with negative indexing\n",
    "print(\"Example 1.3: Using negative indices with unsqueeze\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Negative indices count from the end: -1 is the last position\")\n",
    "tensor_1d = torch.tensor([10, 20, 30])\n",
    "print(f\"Original tensor: {tensor_1d}, shape: {tensor_1d.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsqueezed_neg = tensor_1d.unsqueeze(-1)\n",
    "print(f\"After unsqueeze(-1): {unsqueezed_neg}\")\n",
    "print(f\"Shape: {unsqueezed_neg.shape}\")\n",
    "print(\"\u2192 unsqueeze(-1) adds dimension at the last position\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1.4: Real ML use case - adding batch dimension\n",
    "print(\"Example 1.4: ML Use Case - Adding batch dimension for model input\")\n",
    "print(\"-\" * 80)\n",
    "# Imagine we have a single image flattened to 784 pixels (28x28 MNIST image)\n",
    "single_image = torch.randn(784)\n",
    "print(f\"Single image (flattened): shape = {single_image.shape}\")\n",
    "print(\"Problem: PyTorch models expect input with batch dimension [batch_size, features]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c672aea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Add batch dimension\n",
    "batched_image = single_image.unsqueeze(0)\n",
    "print(f\"After adding batch dimension: shape = {batched_image.shape}\")\n",
    "print(\"\u2192 Now compatible with model.forward() which expects [batch_size, 784]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadbe860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: SQUEEZE() - REMOVING DIMENSIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 2: SQUEEZE() - REMOVING DIMENSIONS\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"What is squeeze()?\")\n",
    "print(\"-\" * 80)\n",
    "print(\"squeeze() removes dimensions of size 1 from a tensor.\")\n",
    "print(\"Useful for cleaning up extra dimensions or preparing output.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e25186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2.1: Basic squeeze from TK's article\n",
    "print(\"Example 2.1: Squeezing a single dimension\")\n",
    "print(\"-\" * 80)\n",
    "tensor = torch.tensor([1.0])\n",
    "print(f\"Original tensor: {tensor}\")\n",
    "print(f\"Original shape: {tensor.shape}\")  # torch.Size([1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c780ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezed_tensor = tensor.squeeze(0)\n",
    "print(f\"After squeeze(0): {squeezed_tensor}\")\n",
    "print(f\"Squeezed shape: {squeezed_tensor.shape}\")  # torch.Size([])\n",
    "print(\"\u2192 Removed dimension of size 1 at position 0, now it's a scalar\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3116131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2.2: Squeeze without specifying dimension\n",
    "print(\"Example 2.2: Squeezing all dimensions of size 1\")\n",
    "print(\"-\" * 80)\n",
    "tensor_with_ones = torch.tensor([[[1.0, 2.0, 3.0]]])\n",
    "print(f\"Original tensor shape: {tensor_with_ones.shape}\")  # torch.Size([1, 1, 3])\n",
    "print(f\"Original tensor: {tensor_with_ones}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295341b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezed_all = tensor_with_ones.squeeze()\n",
    "print(f\"After squeeze(): {squeezed_all}\")\n",
    "print(f\"Squeezed shape: {squeezed_all.shape}\")  # torch.Size([3])\n",
    "print(\"\u2192 squeeze() without argument removes ALL dimensions of size 1\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2.3: Squeeze specific dimension\n",
    "print(\"Example 2.3: Squeezing only a specific dimension\")\n",
    "print(\"-\" * 80)\n",
    "tensor_multi = torch.randn(1, 5, 1, 3)\n",
    "print(f\"Original shape: {tensor_multi.shape}\")  # torch.Size([1, 5, 1, 3])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezed_dim0 = tensor_multi.squeeze(0)\n",
    "print(f\"After squeeze(0): shape = {squeezed_dim0.shape}\")  # torch.Size([5, 1, 3])\n",
    "print(\"\u2192 Removed dimension at position 0\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43927d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezed_dim2 = tensor_multi.squeeze(2)\n",
    "print(f\"After squeeze(2): shape = {squeezed_dim2.shape}\")  # torch.Size([1, 5, 3])\n",
    "print(\"\u2192 Removed dimension at position 2\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee8950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2.4: ML use case - removing batch dimension after inference\n",
    "print(\"Example 2.4: ML Use Case - Removing batch dimension from single prediction\")\n",
    "print(\"-\" * 80)\n",
    "# Model output for a single sample (but still has batch dimension)\n",
    "model_output = torch.tensor([[0.1, 0.7, 0.2]])\n",
    "print(f\"Model output (with batch dim): {model_output}\")\n",
    "print(f\"Shape: {model_output.shape}\")  # torch.Size([1, 3])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b946cba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "prediction = model_output.squeeze(0)\n",
    "print(f\"After squeezing batch dimension: {prediction}\")\n",
    "print(f\"Shape: {prediction.shape}\")  # torch.Size([3])\n",
    "print(\"\u2192 Clean output without extra batch dimension\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e49674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: VIEW() - FLEXIBLE RESHAPING WITH EXPLICIT DIMENSIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 3: VIEW() - FLEXIBLE RESHAPING\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"What is view()?\")\n",
    "print(\"-\" * 80)\n",
    "print(\"view() reshapes a tensor to a new shape without copying data.\")\n",
    "print(\"The total number of elements must remain the same.\")\n",
    "print(\"Key point: The tensor must be contiguous in memory for view() to work.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3.1: Basic view from TK's article\n",
    "print(\"Example 3.1: Reshaping 1D to 2D tensor\")\n",
    "print(\"-\" * 80)\n",
    "tensor = torch.tensor([0, 1, 2, 3, 4])\n",
    "print(f\"Original tensor: {tensor}\")\n",
    "print(f\"Original shape: {tensor.shape}\")  # torch.Size([5])\n",
    "print(f\"Total elements: {tensor.numel()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor = tensor.view(5, 1)\n",
    "print(f\"After view(5, 1):\")\n",
    "print(reshaped_tensor)\n",
    "print(f\"New shape: {reshaped_tensor.shape}\")  # torch.Size([5, 1])\n",
    "print(f\"Total elements: {reshaped_tensor.numel()}\")\n",
    "print(\"\u2192 Reshaped from [5] to [5, 1] - same data, different shape\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3.2: Multiple reshaping possibilities\n",
    "print(\"Example 3.2: Different ways to reshape the same tensor\")\n",
    "print(\"-\" * 80)\n",
    "tensor = torch.arange(12)  # Creates tensor [0, 1, 2, ..., 11]\n",
    "print(f\"Original tensor: {tensor}\")\n",
    "print(f\"Original shape: {tensor.shape}\")  # torch.Size([12])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6566dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to 3x4 matrix\n",
    "reshaped_3x4 = tensor.view(3, 4)\n",
    "print(\"Reshaped to 3x4:\")\n",
    "print(reshaped_3x4)\n",
    "print(f\"Shape: {reshaped_3x4.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3523a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to 4x3 matrix\n",
    "reshaped_4x3 = tensor.view(4, 3)\n",
    "print(\"Reshaped to 4x3:\")\n",
    "print(reshaped_4x3)\n",
    "print(f\"Shape: {reshaped_4x3.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to 2x2x3 tensor (3D)\n",
    "reshaped_3d = tensor.view(2, 2, 3)\n",
    "print(\"Reshaped to 2x2x3:\")\n",
    "print(reshaped_3d)\n",
    "print(f\"Shape: {reshaped_3d.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3.3: View constraint - total elements must match\n",
    "print(\"Example 3.3: Understanding view() constraints\")\n",
    "print(\"-\" * 80)\n",
    "tensor = torch.arange(10)\n",
    "print(f\"Tensor with {tensor.numel()} elements: {tensor}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fa888",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # This will fail because 10 elements can't be reshaped to 3x4 (12 elements)\n",
    "    wrong_reshape = tensor.view(3, 4)\n",
    "except RuntimeError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(\"\u2192 Can't reshape 10 elements into 3x4 shape (which needs 12 elements)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a8d55",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# This works because 10 = 2 * 5\n",
    "correct_reshape = tensor.view(2, 5)\n",
    "print(\"Correct reshape to 2x5:\")\n",
    "print(correct_reshape)\n",
    "print(f\"Shape: {correct_reshape.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: USING -1 PLACEHOLDER FOR DYNAMIC SIZING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 4: USING -1 PLACEHOLDER FOR DYNAMIC SIZING\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"What is the -1 placeholder?\")\n",
    "print(\"-\" * 80)\n",
    "print(\"-1 in view() means 'infer this dimension automatically'.\")\n",
    "print(\"PyTorch calculates the size based on the total number of elements\")\n",
    "print(\"and the other specified dimensions.\")\n",
    "print(\"You can only use -1 for ONE dimension at a time.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4.1: Basic -1 usage from TK's article\n",
    "print(\"Example 4.1: Using -1 to infer dimension size\")\n",
    "print(\"-\" * 80)\n",
    "tensor = torch.tensor([0, 1, 2, 3, 4])\n",
    "print(f\"Original tensor: {tensor}\")\n",
    "print(f\"Original shape: {tensor.shape}\")  # torch.Size([5])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fd890",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor = tensor.view(-1, 1)\n",
    "print(f\"After view(-1, 1):\")\n",
    "print(reshaped_tensor)\n",
    "print(f\"New shape: {reshaped_tensor.shape}\")  # torch.Size([5, 1])\n",
    "print(\"\u2192 PyTorch inferred the first dimension should be 5\")\n",
    "print(\"   Calculation: total_elements / 1 = 5 / 1 = 5\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4.2: -1 in different positions\n",
    "print(\"Example 4.2: Using -1 at different positions\")\n",
    "print(\"-\" * 80)\n",
    "tensor = torch.arange(24)\n",
    "print(f\"Tensor with {tensor.numel()} elements\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4aec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let PyTorch infer the first dimension\n",
    "reshaped_1 = tensor.view(-1, 6)\n",
    "print(f\"view(-1, 6) \u2192 shape: {reshaped_1.shape}\")\n",
    "print(f\"PyTorch calculated: 24 / 6 = 4 for first dimension\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let PyTorch infer the last dimension\n",
    "reshaped_2 = tensor.view(4, -1)\n",
    "print(f\"view(4, -1) \u2192 shape: {reshaped_2.shape}\")\n",
    "print(f\"PyTorch calculated: 24 / 4 = 6 for last dimension\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works with multiple dimensions\n",
    "reshaped_3 = tensor.view(2, 3, -1)\n",
    "print(f\"view(2, 3, -1) \u2192 shape: {reshaped_3.shape}\")\n",
    "print(f\"PyTorch calculated: 24 / (2 * 3) = 4 for last dimension\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e0eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4.3: Using -1 to flatten\n",
    "print(\"Example 4.3: Flattening a tensor with -1\")\n",
    "print(\"-\" * 80)\n",
    "tensor_3d = torch.randn(2, 3, 4)\n",
    "print(f\"Original 3D tensor shape: {tensor_3d.shape}\")\n",
    "print(f\"Total elements: {tensor_3d.numel()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791acd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = tensor_3d.view(-1)\n",
    "print(f\"After view(-1): {flattened.shape}\")\n",
    "print(\"\u2192 Flattened to 1D tensor with all elements\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b87c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: flatten to keep batch dimension\n",
    "batch_flattened = tensor_3d.view(2, -1)\n",
    "print(f\"After view(2, -1): {batch_flattened.shape}\")\n",
    "print(\"\u2192 Kept batch dimension (2), flattened the rest (3*4=12)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babf6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4.4: Why -1 is useful - works with variable sizes\n",
    "print(\"Example 4.4: Dynamic reshaping with -1 (works for any batch size)\")\n",
    "print(\"-\" * 80)\n",
    "print(\"This is crucial when batch size varies during training/inference:\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef1a6b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for batch_size in [1, 4, 8]:\n",
    "    # Simulate different batch sizes of 28x28 images\n",
    "    images = torch.randn(batch_size, 28, 28)\n",
    "    print(f\"Batch of {batch_size} images, shape: {images.shape}\")\n",
    "\n",
    "    # Flatten while preserving batch dimension\n",
    "    flattened_images = images.view(batch_size, -1)\n",
    "    print(f\"  After view({batch_size}, -1): {flattened_images.shape}\")\n",
    "\n",
    "    # Even better: use -1 for batch dimension too!\n",
    "    auto_flattened = images.view(-1, 28*28)\n",
    "    print(f\"  After view(-1, 784): {auto_flattened.shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: REAL-WORLD ML EXAMPLES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 5: REAL-WORLD ML EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d054887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5.1: Preparing images for CNN\n",
    "print(\"Example 5.1: Adding Batch and Channel Dimensions for CNN\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Scenario: Single grayscale image needs to be fed to a CNN\")\n",
    "print(\"CNN expects: [batch_size, channels, height, width]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single grayscale image (28x28 pixels)\n",
    "single_image = torch.randn(28, 28)\n",
    "print(f\"Original image shape: {single_image.shape}\")\n",
    "print(\"Missing: batch dimension and channel dimension\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf1568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add channel dimension first\n",
    "image_with_channel = single_image.unsqueeze(0)\n",
    "print(f\"After unsqueeze(0) [add channel]: {image_with_channel.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ae96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch dimension\n",
    "image_ready = image_with_channel.unsqueeze(0)\n",
    "print(f\"After unsqueeze(0) [add batch]: {image_ready.shape}\")\n",
    "print(\"\u2192 Now ready for CNN: [1, 1, 28, 28]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51078282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5.2: Flattening CNN output for fully connected layer\n",
    "print(\"Example 5.2: Flattening Feature Maps for Fully Connected Layer\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Scenario: CNN outputs feature maps, need to flatten for FC layer\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated output from CNN layer: [batch=32, channels=64, height=7, width=7]\n",
    "cnn_output = torch.randn(32, 64, 7, 7)\n",
    "print(f\"CNN output shape: {cnn_output.shape}\")\n",
    "print(f\"Total features per sample: {64 * 7 * 7} = 3136\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8cab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten while keeping batch dimension\n",
    "flattened = cnn_output.view(32, -1)\n",
    "print(f\"After view(32, -1): {flattened.shape}\")\n",
    "print(\"\u2192 Ready for fully connected layer: [32, 3136]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74632c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better: use -1 for batch size (works with any batch size)\n",
    "flattened_dynamic = cnn_output.view(-1, 64*7*7)\n",
    "print(f\"After view(-1, 3136): {flattened_dynamic.shape}\")\n",
    "print(\"\u2192 Works for any batch size!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5.3: Batch matrix operations\n",
    "print(\"Example 5.3: Reshaping for Batch Matrix Multiplication\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Scenario: Computing attention scores across batch\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc43ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query, Key, Value in transformer (simplified)\n",
    "batch_size = 4\n",
    "seq_length = 10\n",
    "hidden_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa149577",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.randn(batch_size, seq_length, hidden_dim)\n",
    "print(f\"Queries shape: {queries.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062eb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for multi-head attention (num_heads=8)\n",
    "num_heads = 8\n",
    "head_dim = hidden_dim // num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4000b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_reshaped = queries.view(batch_size, seq_length, num_heads, head_dim)\n",
    "print(f\"After view({batch_size}, {seq_length}, {num_heads}, {head_dim}):\")\n",
    "print(f\"Shape: {queries_reshaped.shape}\")\n",
    "print(\"\u2192 Split hidden_dim into (num_heads, head_dim)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55731b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose for batch matrix multiplication\n",
    "queries_transposed = queries_reshaped.transpose(1, 2)\n",
    "print(f\"After transpose(1, 2): {queries_transposed.shape}\")\n",
    "print(\"\u2192 Now shape is [batch, num_heads, seq_length, head_dim]\")\n",
    "print(\"   Ready for multi-head attention computation\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de017d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5.4: Preparing time series data\n",
    "print(\"Example 5.4: Reshaping Time Series Data for RNN\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Scenario: Have flat time series, need [batch, sequence, features]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af16e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat time series data: 100 time steps with 5 features each\n",
    "flat_data = torch.randn(500)  # 100 * 5 = 500 values\n",
    "print(f\"Flat data shape: {flat_data.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to [sequence_length, features]\n",
    "time_series = flat_data.view(100, 5)\n",
    "print(f\"After view(100, 5): {time_series.shape}\")\n",
    "print(\"\u2192 [100 time steps, 5 features]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af134305",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Add batch dimension for RNN\n",
    "batched_series = time_series.unsqueeze(0)\n",
    "print(f\"After unsqueeze(0): {batched_series.shape}\")\n",
    "print(\"\u2192 [1 batch, 100 time steps, 5 features]\")\n",
    "print(\"   Ready for RNN/LSTM input\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: PRACTICE PROBLEMS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 6: PRACTICE PROBLEMS\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test your understanding with these practice problems!\")\n",
    "print(\"Try to solve them before looking at the solutions below.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7534155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "print(\"PROBLEM 1: Image Batch Preparation\")\n",
    "print(\"-\" * 80)\n",
    "print(\"You have 16 RGB images of size 224x224 stored as a flat tensor.\")\n",
    "print(\"The tensor has shape [802816] (16 * 3 * 224 * 224 = 802816)\")\n",
    "print(\"Task: Reshape it to proper format [batch, channels, height, width]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3dd188",
   "metadata": {},
   "source": [
    "CREATE YOUR SOLUTION HERE:\n",
    "flat_images = torch.randn(802816)\n",
    "solution_1 = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided solution:\n",
    "flat_images = torch.randn(802816)\n",
    "solution_1 = flat_images.view(16, 3, 224, 224)\n",
    "print(f\"Solution shape: {solution_1.shape}\")\n",
    "print(\"Expected: torch.Size([16, 3, 224, 224])\")\n",
    "print(f\"Correct: {solution_1.shape == torch.Size([16, 3, 224, 224])}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abef161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "print(\"PROBLEM 2: Remove Unnecessary Dimensions\")\n",
    "print(\"-\" * 80)\n",
    "print(\"A model outputs predictions with shape [1, 1, 10] (1 batch, 1 sequence, 10 classes)\")\n",
    "print(\"Task: Remove all dimensions of size 1 to get just the class scores\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c69dc",
   "metadata": {},
   "source": [
    "CREATE YOUR SOLUTION HERE:\n",
    "model_out = torch.randn(1, 1, 10)\n",
    "solution_2 = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875836ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided solution:\n",
    "model_out = torch.randn(1, 1, 10)\n",
    "solution_2 = model_out.squeeze()\n",
    "print(f\"Original shape: {model_out.shape}\")\n",
    "print(f\"Solution shape: {solution_2.shape}\")\n",
    "print(\"Expected: torch.Size([10])\")\n",
    "print(f\"Correct: {solution_2.shape == torch.Size([10])}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3\n",
    "print(\"PROBLEM 3: Dynamic Batch Flattening\")\n",
    "print(\"-\" * 80)\n",
    "print(\"You have feature maps from a CNN with shape [B, 256, 8, 8] where B varies.\")\n",
    "print(\"Task: Flatten spatial dimensions while keeping batch and channel info.\")\n",
    "print(\"Target shape: [B, 256, 64] where 64 = 8*8\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9d7a7",
   "metadata": {},
   "source": [
    "CREATE YOUR SOLUTION HERE:\n",
    "feature_maps = torch.randn(B, 256, 8, 8)  # B can be any value\n",
    "solution_3 = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided solution (testing with different batch sizes):\n",
    "for B in [1, 4, 16, 32]:\n",
    "    feature_maps = torch.randn(B, 256, 8, 8)\n",
    "    solution_3 = feature_maps.view(B, 256, -1)\n",
    "    # Alternative: feature_maps.view(-1, 256, 64)\n",
    "    print(f\"Batch size {B}: {feature_maps.shape} \u2192 {solution_3.shape}\")\n",
    "    assert solution_3.shape == torch.Size([B, 256, 64]), f\"Wrong shape for batch size {B}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c35033",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Expected: Shape should be [B, 256, 64] for any batch size B\")\n",
    "print(\"All batch sizes correct! \u2713\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY AND KEY TAKEAWAYS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY: KEY TAKEAWAYS\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcda UNSQUEEZE() - Adding Dimensions\")\n",
    "print(\"   \u2022 Adds dimension of size 1 at specified position\")\n",
    "print(\"   \u2022 Use case: Adding batch/channel dimensions\")\n",
    "print(\"   \u2022 Syntax: tensor.unsqueeze(dim)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcda SQUEEZE() - Removing Dimensions\")\n",
    "print(\"   \u2022 Removes dimensions of size 1\")\n",
    "print(\"   \u2022 Use case: Cleaning up extra dimensions\")\n",
    "print(\"   \u2022 Syntax: tensor.squeeze(dim) or tensor.squeeze() for all\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ad839",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcda VIEW() - Flexible Reshaping\")\n",
    "print(\"   \u2022 Reshapes tensor to new shape without copying data\")\n",
    "print(\"   \u2022 Total elements must stay the same\")\n",
    "print(\"   \u2022 Tensor must be contiguous in memory\")\n",
    "print(\"   \u2022 Syntax: tensor.view(shape)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd178f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcda -1 PLACEHOLDER - Dynamic Sizing\")\n",
    "print(\"   \u2022 PyTorch infers the dimension size automatically\")\n",
    "print(\"   \u2022 Can only use -1 for ONE dimension\")\n",
    "print(\"   \u2022 Essential for dynamic batch sizes\")\n",
    "print(\"   \u2022 Syntax: tensor.view(-1, other_dims) or tensor.view(dims, -1)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83c\udfaf BEST PRACTICES\")\n",
    "print(\"   1. Use unsqueeze(0) to add batch dimension for model input\")\n",
    "print(\"   2. Use view(-1, ...) for dynamic batch-size handling\")\n",
    "print(\"   3. Use squeeze() to clean up model outputs\")\n",
    "print(\"   4. Always verify shapes with .shape after reshaping\")\n",
    "print(\"   5. Use view(-1) to flatten tensors completely\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LESSON COMPLETE! Practice these operations to master tensor manipulation.\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}